```toc
```

- 并不是所有刺激的影响是相等的，需要将注意力引向一小部分感兴趣的物体
- ==attention 就是权重，权重就是 attrntion==
- 核心思想：
    - 通过加权求和获得对上下文的全局感知

## 注意力机制

### 注意力机制

- 从心理学角度来看，动物需要在复杂环境下有效关注值得注意的点
    - 人类根据自主线索（volitional cues）和非自主线索（nonvolitional cues）选择注意点
        - 非自主线索：一堆白色物体中看到黑色物体，明显黑色物体更显眼，自然关注的更多
        - 自主线索：想读书，此时注意力在书上
- 注意力机制框架![[00 Attachments/Pasted image 20240812143543.png|400]]
    - 卷积、全连接、池化层都只考虑==非自主线索==
        - 卷积核抽取特征
        - 最大池化层：抽取最大
    - 自注意机制则显示的考虑自主线索
        - 自主线索被称为查询（query）
        - 输入为非自主线索（key）和值（value）的键值对（key 和 value 可以是一样的）
            - key：客观存在的咖啡杯和书本
            - value：感官输入
        - 通过==注意力池化层来由偏向性的选择某些输入== （权重）
    - ==通过给定查询，注意力机制通过注意力汇聚（attention pooling）将非自主性提示的 key 引导至感官输入==
        - 类似于地址选择器

#### 非参注意力汇聚（池化）层

- ==不学参数==
- ==查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚层==
    - ==注意力汇聚层有选择地聚合了值（感官输入）以生成最终的输出==
- 给定数据 $(key_i, value_i)$
  ，可表示为$$f(\text{query}) = \sum_{i=1}^n \alpha(\text{query}, \text{key}_i) \text{value}_i$$
    - $\alpha(x, x_i)$ 为==注意力权重==
- 最简单的方案就是平均池化层：$$f(query) = \frac{1}{n}\sum_{i=1}^n value_i$$
    - 无视查询（query），将所有的 value 作为均值汇聚
- Nadaraya-Watson 核回归$$f(query) = \sum_{i=1}^n \frac{K(query - key_i)}{\sum_{j=1}^n K(query - key_j)} value_i$$
    - 根据输入的位置对输出 $value_i$ 进行加权（每一个值 value 的相对重要性）
    - ==K 可以视为衡量 query 与 key 的距离的函数==（相似度）
    - 使用高斯核（zhengtaifenbu）:$K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2})$
        -
        则有$$\begin{aligned}f(\text{query}) &= \sum_{i=1}^n \alpha(\text{query}, \text{key}_i) \text{value}_i \\&= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(\text{query} - \text{key}_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(\text{query} - \text{key}_j)^2\right)} \text{value}_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(\text{query} - \text{key}_i)^2\right) \text{value}_i\end{aligned}$$
        - 类似做了一个有距离的选择，距离越近，权重越大
        - Softmax 将值映射到 \[0, 1]，然后归一化
- ==理论上，只要有足够多的数据，就可以很好的还原原始的模型==

#### 参数化注意力汇聚层

- 非参的注意力汇聚层没有需要学习的参数，如果需要学习参数，那么可以加一个可以学习的权重举证 w
    - w 可以理解为核函数 K 的参数
    - $$\begin{split}\begin{aligned}f(\text{query}) &= \sum_{i=1}^n \alpha(\text{query}, \text{key}_i) \text{value}_i \\&= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((\text{query} - \text{key}_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((\text{query} - \text{key}_j)w)^2\right)} \text{value}_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((\text{query} - \text{key}_i)w)^2\right) \text{value}_i\end{aligned}\end{split}$$

### 代码实现

## 注意力分数

- 注意力层输出：query 与 key_i 进行一个全连接层，然后 softmax 乘 value_i 后相加
- 通过 k，q 得到注意力权重，权重为 v 的权重

## 使用注意力机制的 seq2seq

### seq2seq 的局限

- 在 seq2seq 中，通过设计一个两个循环神经网络的编码器-解码器架构，用于序列到序列的学习
    - 编码器将长度可变的序列转换为固定形状的上下文变量（context）
    - 解码器根据生成的词元和上下文变量按词元生成输出（目标）序列词元
- 问题在于==并非所有输入（源）词元都对解码某个词元都有用==，seq2seq 在每个解码步骤中依然依赖于同一个上下文变量（编码器最后输出的隐状态）
    - 只考虑自己的权重，games 被翻译为游戏；考虑到北京是，被理解为比赛；综合 winter 和 2022
      时，就翻译为冬奥会![[00 Attachments/Pasted image 20240829165203.png|400]]

### 注意力机制的实现

- RNN 改变了传统的神经网络，建立了网络隐层间的时序关联![[00 Attachments/Pasted image 20240829165457.png|400]]
    - 每一时刻的 $S_t$ 不仅取决于当前的输入，还取决于上一时刻的 $S_{t-1}$
- 两个 RNN 组合，实现 Encoder-Decoder 模型
    - 对一句话进行编码，再解码，就实现了机器翻译![[00 Attachments/Pasted image 20240829172116.png|400]]
    - 但是，这种不管输入多长，都同一压缩成长度编码 C 的做法（用一个隐状态涵盖所有信息），会导致翻译精度下降
- attention 机制就通过每个时间输入不同的上下文变量 C
  来解决这个问题![[00 Attachments/Pasted image 20240829172801.png|400]]
    - $\alpha$ 表示了此刻所有输入的权重，从 C 的角度来看，就是不同输入的注意力
    - 通过神经网络训练，得到最好的 attentiion 权重矩阵
    - 让每一时刻，模型都能动态的看到全局信息，将注意力集中到对当前单词翻译最重要的信息上（本质即为提高有效信息的利用率）
    - 图中的注意力即为注意力权重矩阵 $\alpha$ 得到 C 的过程![[00 Attachments/Pasted image 20240829174829.png|400]]
- 人类的视觉系统就是一种 attention 机制，他将有限的注意力集中在重点信息上，从而节省资源，快速获得最有效的信息

### 代码实现

## 自注意力

- [自注意机制笔记](https://www.bilibili.com/h5/note-app/view?cvid=19755688&pagefrom=comment)
- 自查表，标明上下文关系（计算每个单词与其他所有单词之间的关联）

### 自注意力

- 随着 GPU 等大规模并行运算的发展，RNN 的顺序结构很不方便：难以并行运算，效率太低
- 回顾 Attention 的训练过程，通过不同权重计算 $C_i$（上下文变量）
  的过程就像是打分，至于输入的顺序好像没有什么作用![[00 Attachments/Pasted image 20240830004250.png|400]]
- 因此可以做出如下简化
    - 去掉输入的箭头，在 Encode 阶段，==计算每个单词与其他所有单词之间的关联（关联用权重表示）==
    -
    注意力汇聚函数：$$\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d$$
        - ==将 xi 同时作为 key 、value 和 query== ，以此来对序列抽取特征
        - 自注意力之所以叫做自注意力，是因为 key，value，query 都是来自于自身，xi 既作为 key ，又作为 value ，同时还作为 query
          （self-attention 中的 self 所强调的是 key，value，query 的取法）
        - $f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i$
    - 利用这些权重加权表示，放到一个所谓的前馈神经网络中（为了效果更好，可以多加几层），得到新的输出，这样就很好的嵌入了上下文信息
        - ![[00 Attachments/Pasted image 20240830011006.png|400]]
        - 多加几层（步骤重复几次）可以使效果更好![[00 Attachments/Pasted image 20240830011039.png|400]]
- 这一机制有点像照镜子，希望能给别人好的印象（得到最好的效果），需要不断照镜子，全身打量，关注突出的内容（穿搭、身材、发型），同时当前的印象不仅受制于今天的打扮，也受制于以前的印象

### 比较卷积神经网络、循环神经网络和自注意力

- 与 RNN，CNN 比较（图中，n 为序列长度， d 为特征维度，k 卷积核大小）![[00 Attachments/Pasted image 20240914100002.png|400]]
    - 每层的计算复杂度
        - self-attention：
            - 一次矩阵乘法的复杂度为 $O(n^2*d)$
            - 在这个过程中进行了三次矩阵乘法的操作![[00 Attachments/Pasted image 20240914100242.png|200]]
            - [矩阵乘法复杂度分析-CSDN博客](https://blog.csdn.net/qq_39463175/article/details/111818717)
            - 三个矩阵乘法的代码如下所示（可知，连乘的矩阵是先后处理的）
              ```python
              int A(m*n),
              int B(n*m)
              int C(m*n)
               
              int D(m*m)
              int E(m*n)
               
              //先计算D=A*B
               for(i=0;i<m;i++){ //A矩阵中的 m-行
                      for(j=0;j<m;j++){  //B矩阵中的 m-列
                          for(k=0;k<n;k++){  //A矩阵中的n 或者B矩阵中的n ,一样的
                              D[i][j]= D[i][j]+A[i][k]*B[k][j]; 
                           } 
                       } 
                   }
               
              //在计算E=D*C
               
               for(i=0;i<m;i++){ //D矩阵中的 m-行
                      for(j=0;j<n;j++){  //C矩阵中的 n-列
                          for(k=0;k<m;k++){  //D矩阵中的m 或者C矩阵中的m ,一样的
                              E[i][j]= E[i][j]+A[i][k]*B[k][j]; 
                           } 
                       } 
                   }
              ```
        - RNN：
            - 全连接层复杂度为 $O(d^2)$
            - 由于 RNN 的循环特性，沿着序列进行 n 次计算
        - CNN：
            - 在序列模型中，卷积核以（k × d）的形式存在，类似滑动窗口
            - 一次窗口的计算为 $O(d × k^2)$（因为输入输出的通道数不变，==要进行一次反卷积的操作==？？？？？？？）
            - n 长度的序列，近似重复 n 次
    - 顺序操作
        - self-attention 并行计算
        - RNN 由于只能按序列顺序进行隐状态的更新，所以是串行
        - CNN 并行
    - 感受野
        - self-attention 本身的机制可以全局考量，一次操作，信息尽收眼底
        - RNN 由于只能进行序列操作，最后的信息要 n 个时刻才能获取
        - CNN 一次卷积只考虑该卷积核内所包括的，要获取全局，需要进行多层卷积

### 位置编码

- 注意到注意力层汇聚的计算过程：query 与 key_i 进行一个全连接层得到注意力分数，然后 softmax 得到注意力权重，乘 value_i 后相加
    -
    自注意力分数，自主意权重，上下文变量![[00 Attachments/Pasted image 20240903161152.png|400]]![[00 Attachments/Pasted image 20240903161202.png|400]]![[00 Attachments/Pasted image 20240903161212.png|400]]
    - 可以发现，==自注意力因为并行计算而放弃了顺序操作==
    - CNN 的位置信息位于卷积核中
    - RNN 的输入本身就是有序的
- 为了利用序列的顺序信息，需要位置编码将位置信息注入到输入里（之所以是将位置信息注入输入而不是改变模型，以为了避免使模型更加复杂）
    - ![[00 Attachments/Pasted image 20240830020655.png|400]]
    -
    矩阵第i行、第2j列和2j+1列上的元素为$$\begin{split}\begin{aligned} p_{i, 2j} &= \sin\left(\frac{i}{10000^{2j/d}}\right),\\p_{i, 2j+1} &= \cos\left(\frac{i}{10000^{2j/d}}\right).\end{aligned}\end{split}$$
        - 对于 P 中的每一列，奇数列是一个 **cos 函数**，偶数列是一个 **sin 函数**，不同的列之间的周期是不一样的
- P 位置编码矩阵如下所示![[00 Attachments/Pasted image 20240830021825.png|400]]
    - X 轴横坐标表示 P 矩阵中的行数
    - 图中不同颜色的曲线表示 P 矩阵中不同的列
    - P 实际上是对每一个样本（row）、每一个维度（dimension）添加一点不一样的值，使得模型能够分辨这种细微的差别，作为位置信息
- 这种方式跟之前的方式的不同之处在于，之前是**将位置信息放进模型中**或者**将位置信息与数据分开然后进行拼接（concat）**
  ，位置编码是直接将位置信息加入到了数据中，这样做的好处是**不改变模型和数据的大小**，缺点是==需要模型对于 P
  中元素的细微信息进行辨认，取决于模型是否能够有效地使用 P 中的位置信息==

#### 绝对位置信息

- 每个位置的编码使唯一的![[00 Attachments/Pasted image 20240903164054.png|400]]
    - 类似二进制编码中，每个位数具有周期性，位置编码矩阵是依赖于不同频率、相位的 sin 函数，因此也具有唯一性
    - 同时 sin 的取值范围为 \[-1, 1]，所以可以编码的范围更广，可以在任意维度上进行编码

#### 相对位置信息

- 编码使用的是 **sin 函数或者是 cos 函数**
  ，使得它对于序列中两个固定距离的位置编码，不管它们处于序列中的哪个位置，他们的编码信息都能够通过一个线性变换进行转换![[00 Attachments/Pasted image 20240903164525.png|400]]
    - 编码的是一个相对位置信息，位置位于 i + σ 处的位置编码可以**线性投影**位置 i 处的位置编码来表示，也就是说位置信息和绝对位置
      i 无关，只是和相对位置 σ 有关
        - ==投影矩阵和序列中的位置 i 是无关的，但是和 j 是相关的（和 dimension
          的信息是相关的）==，意味着在一个序列中，假设一个词出现在另外一个词两个或者三个位置的时候，不管这对词出现在序列中的什么位置，对于位置信息来讲，都是可以通过一个同样的线性变换查找出来的
    - 相对来讲，这样编码的好处在于==模型能够更加关注相对的位置信息，而不是关注一个词出现在一个句子中的绝对位置==

## Transformer

- Transformer 模型是**完全基于注意力机制**，没有任何卷积层或循环神经网络
    - 与使用注意力机制的 seq2seq 不同
- Transformer 最初应用在**文本数据上的序列到序列学习**，现在已经推广到各种现代的深度学习中，如**语言**、**视觉**、**语音**和
  **强化学习**领域

### Transformer 架构

- ![[00 Attachments/Pasted image 20240830081014.png|400]]
- 基于编码器-解码器的架构来处理序列对，==Transformer
  的编码器和解码器是基于自注意力的模块叠加而成的，源（source，输入）序列和目标（target，输出）序列的嵌入（embedding）表示通过加上位置编码（positional
  encoding）加入位置信息，再分别输入到编码器和解码器中==

1. Transformer 的编码器是由多个相同的层叠加而成的，每个层都有**两个子层**（==每个子层都采用了残差连接，并且在残差连接的加法计算之后，都使用了层归一化，因此
   Transformer 编码器都将输出一个 d 维表示向量==）
    - 第一个子层是**多头自注意力汇聚**
        - Transformer 块中的多头注意力实际上就是自注意力（==自注意力同时具有并行计算和最短的最大路径长度这两个优势==）
        - 在计算编码器的自注意力时，key 、value 和 query 的值都来自**前一个编码器层的输出**
    - 第二个子层是**基于位置的前馈网络**
        - Positionwise FFN 实际上是全连接
        - 本质上和编码器-解码器的架构没有本质上的区别，将 Transformer 编码器最后一层的输出作为解码器的输入来完成信息的传递
2. Transformer 解码器也是由多个相同的层叠加而成的，每层都有**三个子层**，并且在每个子层中也使用了**残差连接**和**层归一化
   **
    - 第一个子层是**解码器自注意力（带掩码的多头自注意力）**
        - 在解码器自注意力中，key 、value 和 query 都来自**上一个解码器层的输出**
        - 解码器中的每个位置只能考虑该位置之前的所有位置
        -
        带掩码的自注意力保留了自回归的属性，确保预测仅仅依赖于已生成的输出词元（为了在解码器中保留自回归的属性，带掩码的自注意力设定了有效长度（dec_valid_lens）作为参数，以便任何查询都只会与解码器中所有已经生成的词元的位置（即直到该查询为止）进行注意力计算，而不会对当前位置之后的
        key-value 对进行注意力计算）
    - 第二个子层是**编码器-解码器注意力**
        - 除了编码器中所描述的两个子层之外，解码器还在这两个子层之间插入了编码器-解码器注意力层，作为第三个子层，它的 *
          *query 来自上一个解码器层的输出，key 和 value 来自整个编码器的输出**
    - 第三个子层是**基于位置的前馈网络**

- ![[00 Attachments/Pasted image 20240903195810.png|400]]

### 对比 seq2seq

- 与 seq2seq 不同，==Transformer 是完全基于自注意机制==![[00 Attachments/Pasted image 20240903155317.png|400]]
    - seq2seq 采用了 CNN 来提取上下文信息，顺序输入的方式导致了模型只能进行串行输入
    - Transformer 将使用自注意机制的 seq2seq 中的 RNN 全部转换成了 Transformer 块

### 多头注意力

- ![[00 Attachments/Pasted image 20240903160009.png|400]]
    1. 对**同一个 key 、value 、query** 抽取不同的信息
        - 例如短距离关系和长距离关系
    2. 多头注意力使用 h 个独立的注意力池化
        - 合并各个头（head）输出得到最终输出

    - 换个角度，可以理解为多人对同一个人进行评价（不同的人有不同的评价标准，侧重点不同），然后综合考虑这些评价得出总评价，那么这个总评价就可以更好的反应被评价人的特点
- 细节（数学）展示![[00 Attachments/Pasted image 20240903161613.png|400]]
    - key 、value 、query 都是长为 1 的向量，通过全连接层映射到一个低一点的维（减少特征维度），然后进入到注意力模块中
        - 途中的权重矩阵 W 即为全连接层 FC 的可学习参数

### 带掩码的多头注意力（Masked Multi-head attention）

- 这里指的是训练时![[00 Attachments/Pasted image 20240903170949.png|400]]
    1. 解码器对序列中一个元素输出时，不应该考虑该元素之后的元素
        - **注意力中是没有时间信息的**，在输出中间第 i
          个信息的时候，也能够看到后面的所有信息，这在编码的时候是可以的，但是在解码的时候是不行的，在解码的时候不应该考虑该元素本身或者该元素之后的元素
    2. 可以通过掩码来实现
        - 也就是计算 x_i 输出时，假装当前序列长度为 i
- 关于序列到序列模型（sequence-to-sequence
  model），==在训练阶段，其输出序列的所有位置（时间步）的词元都是已知的；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，在任何解码器时间步中，只有生成的词元才能用于解码器的自注意力计算中==。为了在解码器中保留自回归的属性

### 基于位置的前馈网络（Positionwise FFN）

- [Transformer模型中的Feed-Forward层的作用-CSDN博客](https://blog.csdn.net/weixin_42399993/article/details/121585747)
    - attention 是线性的，学习能力不如非线性的，所以加一个非线性的 MLP 来提取更深层次的特征（即强化 attention 的表达能力）
- 基于位置的前馈网络**对序列中的所有位置的表示进行变换时**使用的是**同一个多层感知机（MLP）**
  ，这就是称前馈网络是基于位置的原因![[00 Attachments/Pasted image 20240903171227.png|400]]
    - 其实就是全连接层，将输入形状由（b，n，d）变成（bn，d），然后作用两个全连接层，最后输出形状由（bn，d）变回（b，n，d），等价于两层核窗口为
      1 的一维卷积层
        - b：batchsize
        - n：序列长度
        - d：dimension（每个词元转化为固定长度的数字编码）
    - 之前在做卷积的时候是将 n（通道数） 和 d 合成一维，变成 nd ；但是现在 n 是序列的长度，会变化，要使模型能够处理任意长度的句子，就不能将
      n 作为一个特征，因此对每个序列中的每个元素（词元）作用一个全连接（将每个序列中的 xi 当作是一个样本）

### 残差连接和归一化

- ![[00 Attachments/Pasted image 20240903173412.png|400]]
    - 使用残差链接是模型在上一层输出的基础上进行学习，防止模型学偏
    - 层归一化和批量归一化的目标相同，但是层归一化是基于特征维度进行归一化的
        - 层归一化和批量归一化的区别在于：==批量归一化在 d（特征） 的维度上找出一个矩阵，将其均值变成 0 ，方差变成
          1；层归一化每次选的是一个元素，也就是每个 batch 里面的一个样本进行归一化==
        - 尽管批量归一化在计算机视觉中被广泛应用，但是在自然语言处理任务中，批量归一化通常不如层归一化的效果好，因为*
          *在自然语言处理任务中，输入序列的长度通常是变化的**
        - 虽然在做层归一化的时候，长度也是变化的，但是至少来说还是在一个**单样本**
          中，不管批量多少，都给定一个特征，这样对于变化的长度来讲，稍微稳定一点，不会因为长度变化，导致稳定性发生很大的变化

### 信息传递

- ![[00 Attachments/Pasted image 20240903180244.png|400]]
    - 编码器的输出作为键（key）值（value）供带掩码的多头注意力的输出查询（query）
    - key、value、query 再送入一个多头注意力机制中，此处的多头自主意与其他两处不同
        - 这里只是普通的注意力机制（k、v、q不是来自同一个变量），而其他两处为自主意力机制
    - 编码器的输出送入解码器中的每个 transformer 块中

### 预测

- 预测与训练时不同![[00 Attachments/Pasted image 20240903181202.png|400]]
    - 训练时的输入是已知的，而预测时的输入是由各个时间步的输出组成
    - 预测时，key、value 不断更新，预测的输出作为 query，之后又加入 key、value 中

### 总结

- ![[00 Attachments/Pasted image 20240903182119.png|400]]
- 和 seq2seq 有点类似，不同之处在于 **Transformer 是一个纯使用注意力的编码-解码器**
- 编码器和解码器都有 n 个 Transformer 块
- 每个块里使用**多头（自）注意力（multi-head attention）**，**基于位置的前馈网络（Positionwise FFN）**，**残差连接**和**层归一化
  **
    - 编码器和解码器中各有一个自注意力，但是在编码器和解码器中传递信息的是一个正常的注意力
    - 基于位置的前馈网络使用同一个多层感知机，作用是对所有序列位置的表示进行转换，实际上就是一个全连接，等价于 1×1 的卷积
    - Add & norm：Add 实际上就是 Residual block 可以帮助将网络做的更深，norm 使用的是 Layer Norm 使得训练起来更加容易；Transformer
      中的残差连接和层规范化是训练非常深度模型的重要工具
- 在 Transformer 中，多头注意力用于表示输入序列和输出序列，但是解码器必须通过**掩码机制**来保留**自回归属性**
  （用自己之前的值预测未来的值）
- ==编码器实现了对语言、语法的上下文理解，类似于将语句进行了拆解（语法分析什么的）==
- ==解码器实现了一种语言到另一种语言的映射，类似于将拆解的成分进行了重新组装==

## 来自 Transformers 的双向编码器表示（BERT）

- BERT 的训练分为预训练和微调两部分
    - 预训练时，提升语法与上下文的基础
    - 微调时，根据具体的场景（任务），对输出进行加训

### NLP 的困境

- 传统的 NLP 的迁移学习抽取的特征不足![[00 Attachments/Pasted image 20240904202237.png|400]]
    - 迁移学习在计算机视觉中比较流行，将 ImageNet 或者更大的数据集上预训练好的模型应用到其他任务中，比如小数据的预测、图片分类或者是目标检测
    - 做迁移学习的时候，一般不更新预训练好的模型
        - 有点像干细胞，根据具体场景（下游任务）进行分化
    - 使用预训练好的模型来抽取特征的时候，==一般得到的是一些比较底层的特征，很多时候只是当成一个 embedding 层（代替原来的
      embedding）来使用，处理一些复杂的任务时，还是需要设计一个比较复杂的模型（RNN，CNN 等）==
        - embedding 层，将词元表示为向量
    - 语言模型只看一个方向，而且训练的模型不是很大（RNN 处理不了很长的序列，因为它只能看到很短的一部分）
- 参照 CV 中的迁移学习![[00 Attachments/Pasted image 20240904204150.png|400]]
    - CV 中训练好的模型已经抽取了足够的特征，然后针对具体任务时，冻结底层权重，加入一个小规模的网络即可（全连接层+softmax
      做分类器等）
    - BERT 也想做类似的事情
        - 做微调的时候，特征抽取的层是可以复用的（也可以应用到别的任务上面去），只需要修改分类器就可以了
        - 预训练的模型抽取了足够多的信息，使得 feature 已经足够好能够抓住很多的语义信息，所以在做新的任务的时候，只需要增加一个输出层就可以了，将语义（lable）转化为特征空间即可

### BERT 架构

- 多个 Tramsformer
  编码器的堆叠，输入输出一一对应（输出为对应位置的词向量）![[00 Attachments/Pasted image 20240904210330.png|400]]
    - ![[00 Attachments/Pasted image 20240914120027.png|300]]
- BERT 输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的![[00 Attachments/Pasted image 20240905003419.png|400]]
    - 在 Tramsformer 训练中，label 是在解码器中输入的，但是 BERT 没有解码器
        - 因此需要将原始序列和目标序列从同一个地方输入，二者之间用 \<seq> 分隔
    - 同时，在输入的位置加入 \<cls>，用于对应分类的输出

### 预训练的两个任务

- ![[00 Attachments/Pasted image 20240905003427.png|400]]
- ![[00 Attachments/Pasted image 20240905003434.png|400]]
- ![[00 Attachments/Pasted image 20240905003440.png|400]]

### BERT 微调

- 对应词元的输出对应包含上下文信息的特征向量![[00 Attachments/Pasted image 20240914141454.png|400]]
    - 现在的一些研究表明，相同含义的词语对应的输出向量在映射到的向量空间中是接近的，就比如，吃的“苹果”，中文和英文对应的特征向量在向量空间上是接近的；但是，如果是“苹果”公司，映射到的向量空间与“苹果”食物有较远的距离
    - 这表明，Tramsformer 在学习的过程中，可以学习语义、语法、上下文一些相关的信息
- 面对不同的场景，需要对输出的词向量进行额外的加训
    - ![[00 Attachments/Pasted image 20240914142737.png|400]]
    - ![[00 Attachments/Pasted image 20240914142754.png|400]]
    - ![[00 Attachments/Pasted image 20240914142802.png|400]]
        - 找出答案在原文中的起始位置![[00 Attachments/Pasted image 20240914142823.png|400]]
