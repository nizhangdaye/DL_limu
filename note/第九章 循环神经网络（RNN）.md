```toc
```

## åºåˆ—æ¨¡å‹

- ä¸ºè§£å†³å«æœ‰åºåˆ—æ•°æ®çš„ä¸€ç±»é—®é¢˜è€Œå»ºç«‹çš„æ•°å­¦æ¨¡å‹

### åºåˆ—æ•°æ®

- ä¹‹å‰çš„æ¨¡å‹è€ƒè™‘çš„æ˜¯ç©ºé—´ä¿¡æ¯ï¼ˆå›¾ç‰‡ï¼‰ï¼Œç°åœ¨
- ![[00 Attachments/Pasted image 20240716101020.png|400]]
- ![[00 Attachments/Pasted image 20240716101103.png|400]]

### ç»Ÿè®¡å·¥å…·

-

åœ¨å›¾ç‰‡å¤„ç†ä¸­ï¼Œæ¯å¼ å›¾ç‰‡éƒ½å¯è®¤ä¸ºæ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œä½†æ˜¯åœ¨åºåˆ—æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å‡è®¾==æ ·æœ¬æ˜¯ä¸ç‹¬ç«‹çš„==![[00 Attachments/Pasted image 20240716103850.png|400]]

- è”åˆæ¦‚ç‡å¯å±•å¼€ä¸ºæ¡ä»¶æ¦‚ç‡
- æ‰€æœ‰çš„æœºå™¨å­¦ä¹ éƒ½æ˜¯å¯¹ $P(X)$ ï¼ˆäº‹ä»¶ X å‘ç”Ÿçš„æ¦‚ç‡ï¼‰è¿›è¡Œå»ºæ¨¡

- æ¦‚ç‡çš„ä¹˜æ³•å…¬å¼![[00 Attachments/Pasted image 20240716110444.png|400]]
    - $$P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}, \ldots, x_1)$$
    - æ±‚ $x_T$ å‘ç”Ÿçš„æ¦‚ç‡ï¼ˆä¸ä¹‹å‰çš„æ¡ä»¶æœ‰å…³ï¼‰ï¼Œå°±éœ€è¦çŸ¥é“ä¹‹å‰æ‰€æœ‰äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡
        - åœ¨å·²çŸ¥å‘ç”Ÿäº‹ä»¶ $(x_1,...,x_{t-1})$ åï¼Œå¯ç”±æ¡ä»¶æ¦‚ç‡é¢„æµ‹ $x_t$
          å‘ç”Ÿçš„æ¦‚ç‡$$x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1)$$
            - ä¹‹åçš„å†…å®¹å°†==å›´ç»•å¦‚ä½•æœ‰æ•ˆçš„ä¼°è®¡ $P(x_t \mid x_{t-1}, \ldots, x_1)$ å±•å¼€==
    - ä¹Ÿå¯ä»¥åè¿‡æ¥ï¼Œç”¨æ¥é¢„æµ‹ä¹‹å‰äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡
        - ä»ä¿¡å·ä¸ç³»ç»Ÿçš„è§’åº¦åˆ†æï¼Œå¦‚æœè¯¥ç³»ç»Ÿæ˜¯éå› æœçš„

### åºåˆ—æ¨¡å‹

- é—®é¢˜çš„æ ¸å¿ƒåœ¨ä¸ $x_1,...,x_{t-1}$ äº‹ä»¶å‘ç”Ÿåï¼Œ$x_t$ å‘ç”Ÿçš„æ¦‚ç‡![[00 Attachments/Pasted image 20240716152833.png|400]]
    - å¯¹ä¹‹å‰ t-1 ä¸ªäº‹ä»¶è¿›è¡Œå»ºæ¨¡ï¼Œç”¨ $f$ è¡¨ç¤º
    - å³åŸºäº t-1 ä¸ªæ•°æ®ï¼ˆäº‹ä»¶ï¼‰è®­ç»ƒæ¨¡å‹ï¼Œç„¶åç”¨æ¨¡å‹é¢„æµ‹ $x_t$ï¼ˆç”¨è‡ªå·±ä¹‹å‰çš„å€¼é¢„æµ‹ä¹‹åçš„å€¼ï¼Œ==è‡ªå›å½’==ï¼‰

#### æ–¹æ¡ˆä¸€ï¼šé©¬å°”ç§‘å¤«æ¨¡å‹

- å¦‚æœå°†ä¹‹å‰çš„æ‰€ç”¨æ•°æ®éƒ½ç”¨äºé¢„æµ‹ï¼Œå°†ä¼šå¯¼è‡´è®¡ç®—é‡å¢åŠ ![[00 Attachments/Pasted image 20240716192345.png|400]]
    - å¯¹ä¹‹å‰ $Ï„$ ä¸ªæ•°æ®è¿›è¡Œå»ºæ¨¡ï¼ˆä½¿ç”¨ $Ï„$ ä¸ªæ•°æ®å»è¿‘ä¼¼ $t$ ä¸ªæ•°æ®ï¼‰
        - ç»™å®šé•¿åº¦ä¸º $Ï„$ çš„å‘é‡ï¼ˆç‰¹å¾ï¼‰ï¼Œé¢„æµ‹ä¸€ä¸ªæ ‡é‡
        - å¯ä»¥è§†ä½œä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’é—®é¢˜

#### æ–¹æ¡ˆäºŒï¼šæ½œå˜é‡æ¨¡å‹

- ä½¿ç”¨æ½œå˜é‡æ¦‚æ‹¬å†å²ä¿¡æ¯ï¼ˆä¹‹å‰çš„æ•°æ®ï¼‰![[00 Attachments/Pasted image 20240716195520.png|400]]
    - $$h_t = f_1(x_{t},h_{t-1})ï¼Œx_{t+1} = f_2(x_t,h_{t})$$
    - è¿™æ ·å°±ç”¨ä¸¤ä¸ªæ¨¡å‹ $f_1,f_2$ ï¼Œæ¯ä¸ªæ¨¡å‹åªè·Ÿä¸¤ä¸ªå˜é‡æœ‰å…³

### æ€»ç»“

- ![[00 Attachments/Pasted image 20240716195745.png|400]]
    - RNN å³ä¸ºæ½œå˜é‡æ¨¡å‹

### ä»£ç å®ç°

- ä½¿ç”¨é©¬å°”ç§‘å¤«æ¨¡å‹æ¥è®­ç»ƒä¸€ä¸ª MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰

#### è®­ç»ƒ

- ä½¿ç”¨æ­£å¼¦å‡½æ•°åŠ ä¸Šå™ªå£°ç”Ÿæˆåºåˆ—æ•°æ®

```python
import torch
from torch import nn
from d2l import torch as d2l

T = 1000  # æ€»å…±äº§ç”Ÿ1000ä¸ªç‚¹  
time = torch.arange(1, T + 1, dtype=torch.float32)
x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))
d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))
```

- ![[00 Attachments/Pasted image 20240716213037.png|400]]
- å°†è¿™ä¸ªåºåˆ—è½¬æ¢ä¸ºæ¨¡å‹çš„ç‰¹å¾ï¼æ ‡ç­¾ï¼ˆfeature-labelï¼‰å¯¹
    - åŸºäºåµŒå…¥ç»´åº¦ $ğœ$ï¼Œæˆ‘ä»¬å°†æ•°æ®æ˜ å°„ä¸ºæ•°æ®å¯¹ $ğ‘¦_ğ‘¡=ğ‘¥_ğ‘¡$ å’Œ $\mathbf{x}_t = [x_{t-\tau}, \ldots, x_{t-1}]$

```python
tau = 4
features = torch.zeros((T - tau, tau))
# æ¯å››ä¸ªæ•°æ®ä½œä¸ºç‰¹å¾ï¼Œç¬¬äº”ä¸ªä½œä¸ºæ ‡ç­¾ï¼Œä¸æ–­æ„é€ è¿™æ ·çš„æ•°æ®å½¢æˆæ•°æ®é›†  
for i in range(tau):
    features[:, i] = x[i:T - tau + i]  # å‰4ä¸ªæ—¶åˆ»çš„æ•°å€¼ç»„æˆä¸€ä¸ªå‘é‡ä½œä¸ºç‰¹å¾  
# æ‰€ä»ç¬¬5ä¸ªæ—¶åˆ»å¼€å§‹ï¼Œæ¯ä¸ªæ—¶åˆ»çš„labelæ˜¯è¯¥æ—¶åˆ»çš„xå€¼ï¼Œè¯¥æ—¶åˆ»çš„è¾“å…¥æ˜¯å‰4ä¸ªæ—¶åˆ»çš„æ•°å€¼ç»„æˆçš„ä¸€ä¸ªå‘é‡ã€‚  
# ç»è¿‡å˜åŒ–åæ•°æ®çš„è¾“å…¥å…±æœ‰996ç»„4ä¸ªä¸€ç»„çš„æ•°æ®ï¼Œè¾“å‡ºå…±996ä¸ªå€¼  
labels = x[tau:].reshape((-1, 1))
batch_size, n_train = 16, 600  # å–å‰600ä¸ªæ ·æœ¬ä½œä¸ºè®­ç»ƒé›†  
# ä½¿ç”¨ features å’Œ labels çš„å‰ n_train ä¸ªæ ·æœ¬åˆ›å»ºä¸€ä¸ªå¯è¿­ä»£çš„è®­ç»ƒé›†  
train_iter = d2l.load_array((features[:n_train], labels[:n_train]),
                            batch_size, is_train=True)
```

- æ¨¡å‹ï¼šä¸¤ä¸ªå…¨è¿æ¥å±‚çš„é”™å±‚æ„ŸçŸ¥æœºã€RuLU æ¿€æ´»å‡½æ•°å’Œå¹³æ–¹æŸå¤±

```python
def init_weights(m):
    # å¦‚æœå½“å‰æ¨¡å—æ˜¯çº¿æ€§å±‚  
    if type(m) == nn.Linear:
        # åˆå§‹åŒ–æƒé‡å‡½æ•°  
        nn.init.xavier_uniform_(m.weight)


def get_net():
    # å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„  
    net = nn.Sequential(nn.Linear(4, 10), nn.ReLU(), nn.Linear(10, 1))
    # å¯¹ç½‘ç»œçš„æƒé‡è¿›è¡Œåˆå§‹åŒ–  
    net.apply(init_weights)
    # è¿”å›æ„å»ºå¥½çš„ç¥ç»ç½‘ç»œæ¨¡å‹  
    return net


# å®šä¹‰å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°  
loss = nn.MSELoss()
```

- è®­ç»ƒæ¨¡å‹

```python
def train(net, train_iter, loss, epochs, lr):
    # å®šä¹‰ä¼˜åŒ–å™¨  
    trainer = torch.optim.Adam(net.parameters(), lr)
    for epoch in range(epochs):
        for X, y in train_iter:
            trainer.zero_grad()  # æ¢¯åº¦æ¸…é›¶  
            l = loss(net(X), y)
            l.backward()
            trainer.step()
        print(f'epoch {epoch + 1}, '
              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')

    # è®­ç»ƒæ¨¡å‹  


net = get_net()
train(net, train_iter, loss, 5, 0.01)
# epoch 1, loss: 0.067933  
# epoch 2, loss: 0.056947  
# epoch 3, loss: 0.053831  
# epoch 4, loss: 0.055126  
# epoch 5, loss: 0.056145
```

#### é¢„æµ‹

##### å•æ­¥é¢„æµ‹ï¼ˆone-step-adead perdictionï¼‰

- ç»™å®šå‰å››ä¸ªæ•°æ®ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªæ•°æ®

```python
# é¢„æµ‹æ¨¡å‹  
onestep_preds = net(features)
# è¿›è¡Œæ•°æ®å¯è§†åŒ–ï¼Œå°†çœŸå®æ•°æ®å’Œä¸€æ­¥é¢„æµ‹ç»“æœç»˜åˆ¶åœ¨åŒä¸€ä¸ªå›¾ä¸­è¿›è¡Œæ¯”è¾ƒ  
d2l.plt.figure()
d2l.plot([time, time[tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy()], 'time', 'x',
         legend=['data', 'l-step preds'], xlim=[1, 1000], figsize=(6, 3))
```

- ![[00 Attachments/Pasted image 20240716213123.png|400]]

##### å¤šæ­¥é¢„æµ‹

- å¦‚æœæ•°æ®è§‚å¯Ÿåºåˆ—çš„æ—¶é—´æ­¥åªåˆ° 604ï¼Œ
  æˆ‘ä»¬éœ€è¦ä¸€æ­¥ä¸€æ­¥åœ°å‘å‰è¿ˆè¿›ï¼š==åœ¨é¢„æµ‹çš„åŸºç¡€ä¸Šè¿›è¡Œé¢„æµ‹==$$\begin{split}\hat{x}_{605} = f(x_{601}, x_{602}, x_{603}, x_{604}), \\
  \hat{x}_{606} = f(x_{602}, x_{603}, x_{604}, \hat{x}_{605}), \\
  \hat{x}_{607} = f(x_{603}, x_{604}, \hat{x}_{605}, \hat{x}_{606}),\\
  \hat{x}_{608} = f(x_{604}, \hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}),\\
  \hat{x}_{609} = f(\hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}, \hat{x}_{608}),\\
  \ldots\end{split}$$
    - é€šå¸¸ï¼Œ==å¯¹äºç›´åˆ° $ğ‘¥_ğ‘¡$ çš„è§‚æµ‹åºåˆ—ï¼Œå…¶åœ¨æ—¶é—´æ­¥ $ğ‘¡+ğ‘˜$ å¤„çš„é¢„æµ‹è¾“å‡º $\hat{x}_{t+k}$ç§°ä¸º $ğ‘˜$
      æ­¥é¢„æµ‹==ï¼ˆğ‘˜-step-ahead-predictionï¼‰
    - ç”±äºæˆ‘ä»¬çš„è§‚å¯Ÿå·²ç»åˆ°äº† $ğ‘¥_{604}$ï¼Œå®ƒçš„ $ğ‘˜$ æ­¥é¢„æµ‹æ˜¯ $\hat{x}_{604+k}$
        - æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„é¢„æµ‹ï¼ˆè€Œä¸æ˜¯åŸå§‹æ•°æ®ï¼‰æ¥è¿›è¡Œå¤šæ­¥é¢„æµ‹

```python
# åˆå§‹åŒ–å¤šæ­¥é¢„æµ‹ç»“æœçš„å¼ é‡  
multistep_preds = torch.zeros(T)
# å°†å·²çŸ¥çš„çœŸå®æ•°æ®èµ‹å€¼ç»™å¤šæ­¥é¢„æµ‹ç»“æœ  
multistep_preds[:n_train + tau] = x[:n_train + tau]
# å¯¹å‰©ä½™æ—¶é—´æ­¥è¿›è¡Œå¤šæ­¥é¢„æµ‹  
for i in range(n_train + tau, T):
    multistep_preds[i] = net(multistep_preds[i - tau:i].reshape((1, -1)))  # å‰4ä¸ªæ—¶åˆ»çš„æ•°å€¼é¢„æµ‹ç¬¬5ä¸ªæ—¶åˆ»çš„æ•°å€¼  

# è¿›è¡Œæ•°æ®å¯è§†åŒ–  
d2l.plt.figure()
d2l.plot(
    [time, time[tau:], time[n_train + tau:]],
    [x.detach().numpy(), onestep_preds.detach().numpy(), multistep_preds[n_train + tau:].detach().numpy()],
    'time',
    'x',
    legend=['data', '1-step preds', 'multistep preds'],
    xlim=[1, 1000],
    figsize=(6, 3))
```

- å¯ä»¥å‘ç°é¢„æµ‹å‘ç”Ÿè¾ƒå¤§çš„åç§»ï¼ˆå‰å‡ æ­¥é¢„æµ‹å¯ä»¥ï¼Œä½†æ˜¯ä¹‹åçš„ k
  æ­¥é¢„æµ‹ä¸è¡Œï¼‰ï¼š![[00 Attachments/Pasted image 20240716213207.png|400]]
    - ç´«è‰²éƒ¨åˆ†æ˜¯ç”¨å®é™…çš„æ•°æ®è¿›è¡Œé¢„æµ‹
    - ç»¿è‰²éƒ¨åˆ†ä½¿ç”¨é¢„æµ‹çš„æ•°æ®è¿›è¡Œé¢„æµ‹
        - ç»è¿‡å‡ ä¸ªé¢„æµ‹æ­¥éª¤ä¹‹åï¼Œé¢„æµ‹çš„ç»“æœå¾ˆå¿«å°±ä¼šè¡°å‡åˆ°ä¸€ä¸ªå¸¸æ•°
        - å°†è¯¯å·®è¿›è¡Œäº†ä¸æ–­çš„ç´¯è®¡
- åŸºäºğ‘˜=1,4,16,64ï¼Œé€šè¿‡å¯¹æ•´ä¸ªåºåˆ—é¢„æµ‹çš„è®¡ç®—ï¼Œ æ›´ä»”ç»†åœ°çœ‹ä¸€ä¸‹ ğ‘˜ æ­¥é¢„æµ‹çš„å›°éš¾

```python
max_steps = 64  # æœ€å¤šé¢„æµ‹æœªæ¥çš„64æ­¥  
# åˆå§‹åŒ–ç‰¹å¾å¼ é‡  
features = torch.zeros((T - tau - max_steps + 1, tau + max_steps))
for i in range(tau):
    # åˆ—iï¼ˆi<tauï¼‰æ˜¯æ¥è‡ªxçš„è§‚æµ‹ï¼Œå…¶æ—¶é—´æ­¥ä»ï¼ˆiï¼‰åˆ°ï¼ˆi+T-tau-max_steps+1ï¼‰  
    features[:, i] = x[i:i + T - tau - max_steps + 1]

# é¢„æµ‹æœªæ¥max_stepsæ­¥  
for i in range(tau, tau + max_steps):
    # åˆ—iï¼ˆi>=tauï¼‰æ˜¯æ¥è‡ªï¼ˆi-tau+1ï¼‰æ­¥çš„é¢„æµ‹ï¼Œå…¶æ—¶é—´æ­¥ä»ï¼ˆiï¼‰åˆ°ï¼ˆi+T-tau-max_steps+1ï¼‰  
    features[:, i] = net(features[:, i - tau:i]).reshape(-1)  # æ ¹æ®å‰4ä¸ªæ—¶åˆ»çš„æ•°å€¼ï¼ˆåŒ…æ‹¬å·²ç»é¢„æµ‹çš„éƒ¨åˆ†ï¼‰é¢„æµ‹ç¬¬5ä¸ªæ—¶åˆ»çš„æ•°å€¼  

# é¢„æµ‹çš„æ­¥é•¿  
steps = (1, 4, 16, 64)  # ç¬¬1æ­¥ï¼Œç¬¬4æ­¥ï¼Œç¬¬16æ­¥ï¼Œç¬¬64æ­¥çš„é¢„æµ‹ç»“æœ  
# è¿›è¡Œæ•°æ®å¯è§†åŒ–  
d2l.plt.figure()
d2l.plot([time[tau + i - 1:T - max_steps + i] for i in steps],
         [features[:, (tau + i - 1)].detach().numpy() for i in steps],
         'time',
         'x',
         legend=[f'{i}-step preds' for i in steps],
         xlim=[5, 1000],
         figsize=(6, 3))
```

- è¿™å¼ å›¾ä½“ç°äº†å¤šæ­¥é¢„æµ‹çš„å›°éš¾![[00 Attachments/Pasted image 20240716213254.png|400]]
    - å³ä½¿è¿™æ˜¯ä¸€ä¸ªå¾ˆç®€å•çš„å‡½æ•°ï¼Œä¹Ÿå¾ˆéš¾é¢„æµ‹å¾ˆè¿œçš„æœªæ¥
- ä¹‹åçš„å†…å®¹å›´ç»•å¦‚ä½•é¢„æµ‹æ›´è¿œçš„æœªæ¥

## æ–‡æœ¬é¢„å¤„ç†

- ç›¸å…³åè¯ï¼š
    - ==è¯å…ƒ==ï¼ˆTokenï¼‰ï¼šè¯å…ƒæ˜¯æ–‡æœ¬çš„åŸºæœ¬å•å…ƒã€‚å®ƒå¯ä»¥æ˜¯å•è¯ã€æ ‡ç‚¹ç¬¦å·ã€å­å•è¯æˆ–å­—ç¬¦ï¼Œå…·ä½“å–å†³äºåˆ†è¯ï¼ˆtokenizationï¼‰ç­–ç•¥
    - ==è¯­æ–™åº“==ï¼ˆCorpusï¼‰ï¼šè¯­æ–™åº“æ˜¯ç»è¿‡æ•´ç†å’Œæ ‡æ³¨çš„å¤§é‡æ–‡æœ¬é›†åˆï¼Œç”¨äºè¯­è¨€ç ”ç©¶å’ŒNLPæ¨¡å‹çš„è®­ç»ƒ
        - ä¾‹å¦‚ï¼Œå¸¸è§çš„è¯­æ–™åº“åŒ…æ‹¬æ–°é—»æ–‡ç« ã€ä¹¦ç±ã€ç¤¾äº¤åª’ä½“å¸–å­ç­‰ã€‚è¯­æ–™åº“çš„è´¨é‡å’Œè§„æ¨¡å¯¹NLPæ¨¡å‹çš„æ€§èƒ½æœ‰ç›´æ¥å½±å“
        - è¯­æ–™åº“é€šå¸¸éœ€è¦ç»è¿‡æ¸…ç†ã€æ ‡æ³¨ç­‰é¢„å¤„ç†æ­¥éª¤ï¼Œä»¥æé«˜å…¶æœ‰æ•ˆæ€§
    - ==è¯æ±‡è¡¨==ï¼ˆVocabularyï¼‰ï¼šè¯æ±‡è¡¨æ˜¯ä»è¯­æ–™åº“ä¸­æå–çš„æ‰€æœ‰ç‹¬ç‰¹è¯å…ƒçš„é›†åˆ
        - å®ƒåŒ…å«äº†è¯­æ–™åº“ä¸­å‡ºç°çš„æ‰€æœ‰è¯å…ƒåŠå…¶é¢‘ç‡æˆ–å…¶ä»–ç»Ÿè®¡ä¿¡æ¯

- æ–‡æœ¬é¢„å¤„ç†çš„å¸¸è§æ­¥éª¤
    1. å°†æ–‡æœ¬ä½œä¸ºå­—ç¬¦ä¸²åŠ è½½åˆ°å†…å­˜ä¸­
    2. å°†å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºè¯å…ƒï¼ˆå¦‚å•è¯å’Œå­—ç¬¦ï¼‰
    3. æ„å»ºè¯­æ–™åº“
    4. å»ºç«‹ä¸€ä¸ªè¯è¡¨ï¼Œå°†æ‹†åˆ†çš„è¯å…ƒæ˜ å°„åˆ°æ•°å­—ç´¢å¼•
    5. ==å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—ç´¢å¼•åºåˆ—==ï¼Œæ–¹ä¾¿æ¨¡å‹æ“ä½œ

### è¯»å–æ•°æ®é›†

- å°†æ•°æ®é›†è¯»å–åˆ°ç”±å¤šæ¡æ–‡æœ¬è¡Œç»„æˆçš„åˆ—è¡¨ä¸­ï¼Œå…¶ä¸­æ¯æ¡æ–‡æœ¬è¡Œéƒ½æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²
    - æ­¤å¤„å¿½ç•¥æ ‡ç‚¹å’Œå¤§å†™

```python
import collections
import re
from d2l import torch as d2l

# è¯»å–æ•°æ®é›†  
d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',
                                '090b5e7e70c295757f55df93cb0a180b9691891a')


def read_time_machine():
    """å°†æ—¶é—´æœºå™¨æ•°æ®é›†åŠ è½½ä¸ºæ–‡æœ¬è¡Œçš„åˆ—è¡¨"""
    with open(d2l.download('time_machine'), 'r') as f:
        lines = f.readlines()  # è¯»å–æ‰€æœ‰è¡Œ  
        # æŠŠä¸æ˜¯å¤§å†™å­—æ¯ã€å°å†™å­—æ¯çš„ä¸œè¥¿ï¼Œå…¨éƒ¨å˜æˆç©ºæ ¼  
        # å»é™¤éå­—æ¯å­—ç¬¦ï¼Œå¹¶è½¬æ¢ä¸ºå°å†™  
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]  # æ­£åˆ™è¡¨è¾¾å¼  


# è¯»å–æ—¶é—´æœºå™¨æ•°æ®é›†ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨ 'lines' å˜é‡ä¸­  
lines = read_time_machine()
print(lines[0])
print(lines[10])
# the time machine by h g wells  
# twinkled and his usually pale face was flushed and animated the
```

### è¯å…ƒåŒ–

- `tokenize`å‡½æ•°å°†æ–‡æœ¬è¡Œåˆ—è¡¨ï¼ˆ`lines`ï¼‰ä½œä¸ºè¾“å…¥ï¼Œåˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ–‡æœ¬åºåˆ—ï¼ˆå¦‚ä¸€æ¡æ–‡æœ¬è¡Œï¼‰
    - æ¯ä¸ªæ–‡æœ¬åºåˆ—åˆè¢«æ‹†åˆ†æˆä¸€ä¸ªè¯å…ƒåˆ—è¡¨ï¼Œ==è¯å…ƒ==ï¼ˆtokenï¼‰æ˜¯æ–‡æœ¬çš„åŸºæœ¬å•ä½
    - æœ€åï¼Œè¿”å›ä¸€ä¸ªç”±è¯å…ƒåˆ—è¡¨ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­çš„æ¯ä¸ªè¯å…ƒéƒ½æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆstringï¼‰

```python
def tokenize(lines, token='word'):
    """å°†æ–‡æœ¬è¡Œåˆ—è¡¨è¿›è¡Œåˆ†è¯å¤„ç†"""
    if token == 'word':
        # ä»¥ç©ºæ ¼ä¸ºåˆ†éš”ç¬¦å°†æ¯è¡Œå­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå•è¯åˆ—è¡¨  
        return [line.split() for line in lines]
    elif token == 'char':
        # å°†æ¯è¡Œå­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå­—ç¬¦åˆ—è¡¨ï¼ˆåŒ…æ‹¬ç©ºæ ¼ï¼‰
        return [list(line) for line in lines]
    else:
        print('é”™ä½ï¼šæœªçŸ¥ä»¤ç‰Œç±»å‹ï¼š' + token)


tokens = tokenize(lines)  # å¯¹ lines è¿›è¡Œåˆ†è¯å¤„ç†  
for i in range(11):
    # ç©ºåˆ—è¡¨è¡¨ç¤ºç©ºè¡Œ  
    print(tokens[i])
# ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']  
# []  
# []  
# []  
# []  
# ['i']  
# []  
# []  
# ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']  
# ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']  
# ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']
```

### è¯è¡¨ï¼ˆvocabularyï¼‰

- è¯å…ƒçš„ç±»å‹æ˜¯å­—ç¬¦ä¸²ï¼Œä½†æ˜¯æ¨¡å‹çš„è¾“å…¥æ˜¯æ•°å­—
- å› æ­¤éœ€è¦æ„å»ºä¸€ä¸ªè¯è¡¨ï¼ˆå­—å…¸ï¼‰ï¼Œå°†å­—ç¬¦ä¸²ç±»å‹çš„è¯å…ƒæ˜ å°„åˆ°ä» 0 å¼€å§‹çš„æ•°å­—ç´¢å¼•ä¸­
    - å…ˆå°†è®­ç»ƒé›†ä¸­çš„æ‰€æœ‰æ–‡æ¡£åˆå¹¶åœ¨ä¸€èµ·ï¼Œå¯¹å®ƒä»¬çš„å”¯ä¸€è¯å…ƒè¿›è¡Œç»Ÿè®¡ï¼Œå¾—åˆ°çš„ç»Ÿè®¡ç»“æœç§°ä¹‹ä¸º==è¯­æ–™==ï¼ˆcorpusï¼‰
    - ç„¶åæ ¹æ®æ¯ä¸ªå”¯ä¸€è¯å…ƒçš„å‡ºç°é¢‘ç‡ï¼Œä¸ºå…¶åˆ†é…ä¸€ä¸ªæ•°å­—ç´¢å¼•
        - å¾ˆå°‘å‡ºç°çš„è¯å…ƒé€šå¸¸è¢«ç§»é™¤ï¼Œè¿™å¯ä»¥é™ä½å¤æ‚æ€§
        - å¦å¤–ï¼Œè¯­æ–™åº“ä¸­ä¸å­˜åœ¨æˆ–å·²åˆ é™¤çš„ä»»ä½•è¯å…ƒéƒ½å°†æ˜ å°„åˆ°ä¸€ä¸ªç‰¹å®šçš„æœªçŸ¥è¯å…ƒâ€œ$<unk>$â€
        - å¯ä»¥é€‰æ‹©å¢åŠ ä¸€ä¸ªåˆ—è¡¨ï¼Œç”¨äºä¿å­˜é‚£äº›è¢«ä¿ç•™çš„è¯å…ƒ
            - ä¾‹å¦‚ï¼šå¡«å……è¯å…ƒï¼ˆâ€œ$<pad>$â€ï¼‰ï¼› åºåˆ—å¼€å§‹è¯å…ƒï¼ˆâ€œ$<bos>$â€ï¼‰ï¼› åºåˆ—ç»“æŸè¯å…ƒï¼ˆâ€œ$<eos>$â€ï¼‰

```python
class Vocab:
    """æ–‡æœ¬è¯è¡¨"""

    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        """åˆå§‹åŒ–è¯è¡¨å¯¹è±¡"""
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        counter = count_corpus(tokens)  # ç»Ÿè®¡ tokens ä¸­è¯å…ƒçš„é¢‘ç‡  
        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)
        # è®¾ç½®æœªçŸ¥æ ‡è®°ç´¢å¼•ä¸º 0ï¼Œæ„å»ºåŒ…å«æœªçŸ¥æ ‡è®°å’Œä¿ç•™ç‰¹æ®Šæ ‡è®°çš„åˆ—è¡¨ uniq_tokens
        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens
        uniq_tokens += [token for token, freq in self.token_freqs
                        if freq >= min_freq and token not in uniq_tokens]  # ä¿ç•™è¯é¢‘å¤§äºç­‰äº min_freq çš„æ ‡è®°  
        # token_to_idx è·å–è¯å…ƒåˆ°ç´¢å¼•çš„æ˜ å°„ï¼Œidx_to_token è·å–ç´¢å¼•åˆ°è¯å…ƒçš„æ˜ å°„(è¯è¡¨)  
        self.idx_to_token, self.token_to_idx = [], dict()
        # éå† uniq_tokens ä¸­çš„æ¯ä¸ªæ ‡è®°ï¼Œå°†å…¶æ·»åŠ åˆ°ç´¢å¼•åˆ°æ ‡è®°çš„åˆ—è¡¨ä¸­ï¼Œå¹¶å°†æ ‡è®°å’Œå¯¹åº”ç´¢å¼•å­˜å‚¨åˆ°æ ‡è®°åˆ°ç´¢å¼•çš„å­—å…¸ä¸­  
        # ç´¢å¼•å€¼ä» 0 å¼€å§‹é€’å¢ï¼Œå¯¹åº”äºæ ‡è®°åœ¨åˆ—è¡¨ä¸­çš„ä½ç½®  
        for token in uniq_tokens:
            self.idx_to_token.append(token)
            # å°†å½“å‰æ ‡è®° `token` å’Œå…¶å¯¹åº”çš„ç´¢å¼•å€¼å­˜å‚¨åˆ°æ ‡è®°åˆ°ç´¢å¼•çš„å­—å…¸ `self.token_to_idx` ä¸­  
            self.token_to_idx[token] = len(self.idx_to_token) - 1

    def __len__(self):
        """è·å–è¯è¡¨çš„é•¿åº¦"""
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        """æ ¹æ®æ ‡è®°è·å–å…¶å¯¹åº”çš„ç´¢å¼•æˆ–ç´¢å¼•åˆ—è¡¨"""
        # å¦‚æœ tokens ä¸æ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼Œåˆ™è¿”å›å¯¹åº”çš„ç´¢å¼•æˆ–é»˜è®¤çš„æœªçŸ¥æ ‡è®°ç´¢å¼•  
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
            # å¯¹äºè¾“å…¥çš„æ ‡è®°åˆ—è¡¨ tokensï¼Œé€ä¸ªè°ƒç”¨ self.__getitem__() æ–¹æ³•è·å–æ¯ä¸ªæ ‡è®°å¯¹åº”çš„ç´¢å¼•å€¼ï¼Œå¹¶è¿”å›ç´¢å¼•å€¼çš„åˆ—è¡¨    
            return [self.__getitem__(token) for token in tokens]


def to_tokens(self, indices):
    """æ ¹æ®ç´¢å¼•è·å–å¯¹åº”çš„æ ‡è®°æˆ–æ ‡è®°åˆ—è¡¨"""
    # å¦‚æœè¾“å…¥çš„ indices ä¸æ˜¯åˆ—è¡¨æˆ–å…ƒç»„ç±»å‹ï¼Œåˆ™è¿”å›å¯¹åº”ç´¢å¼•å€¼å¤„çš„æ ‡è®°  
    if not isinstance(indices, (list, tuple)):
        return self.idx_to_token[indices]
    return [self.idx_to_token[index] for index in indices]


def count_corpus(tokens):
    """ç»Ÿè®¡æ ‡è®°çš„é¢‘ç‡"""
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # å¦‚æœ tokens æ˜¯ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨ï¼Œåˆ™å°†å…¶å±•å¹³ä¸ºä¸€ç»´åˆ—è¡¨  
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)  # ä½¿ç”¨ Counter å¯¹è±¡ç»Ÿè®¡è¯å…ƒçš„é¢‘ç‡
```

- ä½¿ç”¨æ—¶å…‰æœºå™¨æ•°æ®é›†ä½œä¸ºè¯­æ–™åº“æ¥æ„å»ºè¯è¡¨ï¼Œç„¶åæ‰“å°å‰å‡ ä¸ªé«˜é¢‘è¯å…ƒåŠå…¶ç´¢å¼•

```python
# æ„å»ºè¯æ±‡è¡¨  
vocab = Vocab(tokens)  # åˆ›å»ºè¯è¡¨  
print(list(vocab.token_to_idx.items())[:10])
# [('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]
```

- å°†æ¯ä¸€æ¡æ–‡æœ¬è¡Œè½¬æ¢æˆä¸€ä¸ªæ•°å­—ç´¢å¼•åˆ—è¡¨

```python
# å°†æ¯ä¸€è¡Œæ–‡æœ¬è½¬æ¢æˆä¸€ä¸ªæ•°å­—ç´¢å¼•åˆ—è¡¨  
for i in [0, 10]:
    print('æ–‡æœ¬:', tokens[i])
    print('ç´¢å¼•:', vocab[tokens[i]])
# æ–‡æœ¬: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']  
# ç´¢å¼•: [1, 19, 50, 40, 2183, 2184, 400]  
# æ–‡æœ¬: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']  
# ç´¢å¼•: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]
```

### æ•´åˆæ‰€æœ‰åŠŸèƒ½

- `load_corpus_time_machine`å‡½æ•°è¿”å›`corpus`ï¼ˆè¯å…ƒç´¢å¼•åˆ—è¡¨ï¼‰å’Œ`vocab`ï¼ˆæ—¶å…‰æœºå™¨è¯­æ–™åº“çš„è¯è¡¨ï¼‰
    1. ä¸ºäº†ç®€åŒ–åé¢ç« èŠ‚ä¸­çš„è®­ç»ƒï¼Œä½¿ç”¨å­—ç¬¦ï¼ˆè€Œä¸æ˜¯å•è¯ï¼‰å®ç°æ–‡æœ¬è¯å…ƒåŒ–ï¼ˆä¸ºä»€ä¹ˆä½¿ç”¨å­—ç¬¦å¯ä»¥ç®€åŒ–ï¼Œä½¿ç”¨å­—ç¬¦çš„è¯ï¼Œè¯è¡¨ä¸è¶…è¿‡ 28ï¼‰
    2. æ—¶å…‰æœºå™¨æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ–‡æœ¬è¡Œä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªæ®µè½ï¼Œè¿˜å¯èƒ½æ˜¯ä¸€ä¸ªå•è¯ï¼Œå› æ­¤è¿”å›çš„`corpus`
       ä»…å¤„ç†ä¸ºå•ä¸ªåˆ—è¡¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å¤šè¯å…ƒåˆ—è¡¨æ„æˆçš„ä¸€ä¸ªåˆ—è¡¨ï¼ˆï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼‰

```python
def load_corpus_time_machine(max_tokens=-1):
    """è¿”å›æ—¶å…‰æœºå™¨æ•°æ®é›†çš„æ ‡è®°ç´¢å¼•åˆ—è¡¨å’Œè¯æ±‡è¡¨"""
    lines = read_time_machine()
    tokens = tokenize(lines, 'char')  # ä»¥å­—ç¬¦ä¸ºå•ä½è¿›è¡Œåˆ†è¯  
    vocab = Vocab(tokens)  # åˆ›å»ºè¯è¡¨  
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°ç´¢å¼•åˆ—è¡¨  
    corpus = [vocab[token] for line in tokens for token in line]
    # æˆªæ–­æ–‡æœ¬é•¿åº¦ï¼ˆè‹¥æœ‰é™åˆ¶ï¼‰  
    if max_tokens > 0:
        # å¯¹æ ‡è®°ç´¢å¼•åˆ—è¡¨ corpus è¿›è¡Œæˆªæ–­ï¼Œåªä¿ç•™å‰ max_tokens ä¸ªæ ‡è®°  
        corpus = corpus[:max_tokens]
    return corpus, vocab


# è½½å…¥æ—¶å…‰æœºå™¨æ•°æ®é›†çš„æ ‡è®°ç´¢å¼•åˆ—è¡¨å’Œè¯æ±‡è¡¨  
corpus, vocab = load_corpus_time_machine()
print(len(corpus), len(vocab))
# 170580 28
print(vocab.token_to_idx.keys())
# dict_keys(['<unk>', ' ', 'e', 't', 'a', 'i', 'n', 'o', 's', 'h', 'r', 'd', 'l', 'm', 'u', 'c', 'f', 'w', 'g', 'y', 'p', 'b', 'v', 'k', 'x', 'z', 'j', 'q'])
```

## è¯­è¨€æ¨¡å‹å’Œæ•°æ®é›†

- ç”¨äºé¢„æµ‹æ–‡æœ¬åºåˆ—ä¸­ä¸‹ä¸€ä¸ªè¯æˆ–å­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒçš„æ¨¡å‹![[00 Attachments/Pasted image 20240717163109.png|400]]

### ä½¿ç”¨è®¡æ•°å»ºæ¨¡

- ä¸€ä¸ªç®€å•çš„å»ºæ¨¡æ–¹å¼![[00 Attachments/Pasted image 20240717164757.png|400]]
    - è¿ç»­å‡ºç°çš„æ¦‚ç‡

### é©¬å°”å¯å¤«æ¨¡å‹ä¸ ğ‘› å…ƒè¯­æ³•

- ä½†æ˜¯åœ¨è®¡æ•°å»ºæ¨¡ä¸­ï¼Œå¦‚æœé‡‡ç”¨çš„æ–‡æœ¬åºåˆ—è¿‡å¤§ï¼Œå¾ˆå¯èƒ½å¯¼è‡´ nï¼ˆè¯¥è¿ç»­æ–‡æœ¬å‡ºç°çš„æ¬¡æ•°ï¼‰ä¸º
  0![[00 Attachments/Pasted image 20240717165406.png|400]]
    - ä¸€å…ƒè¯­æ³•ï¼šå‡è®¾æ¯ä¸ªè¯æ˜¯ç‹¬ç«‹çš„ï¼Œåªä¾èµ–è‡ªå·±ï¼ˆå³åœ¨é©¬å°”ç§‘å¤«æ¨¡å‹ä¸­ Ï„ ä¸º 0ï¼‰
    - äºŒå…ƒè¯­æ³•ï¼šå‡è®¾æ¯ä¸ªè¯åªä¾èµ–äºå‰ä¸€ä¸ªè¯ï¼ˆå³åœ¨é©¬å°”ç§‘å¤«æ¨¡å‹ä¸­ Ï„ ä¸º 1ï¼‰
        - è®¡æ•° $n(x_i, x_{x+1})$ æ—¶åªéœ€å¯»æ‰¾é•¿åº¦ä¸º 2 çš„å­åºåˆ—
- N è¶Šå¤§ï¼Œå¯¹åº”çš„ä¾èµ–å…³ç³»è¶Šé•¿ï¼Œç²¾åº¦è¶Šé«˜ï¼Œä½†æ˜¯ç©ºé—´å¤æ‚åº¦æ¯”è¾ƒå¤§

### æ€»ç»“

- è¯­è¨€æ¨¡å‹ä¼°è®¡æ–‡æœ¬åºåˆ—çš„è”åˆæ¦‚ç‡
- ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•æ—¶å¸¸é‡‡ç”¨ n å…ƒè¯­æ³•
    - æ¯æ¬¡æ‰¾ä¸€ä¸ªé•¿ä¸º n çš„å­åºåˆ—ï¼Œç„¶åå»è®¡æ•°ï¼Œç”¨äºè®¡ç®—æ¦‚ç‡

### ä»£ç å®ç°

#### è‡ªç„¶è¯­è¨€ç»Ÿè®¡

- å¯¹æ—¶å…‰æœºå™¨æ„å»ºè¯è¡¨ï¼Œå¹¶æ‰“å°å‰ 10 ä¸ªé¢‘ç‡æœ€é«˜çš„è¯

```python
import random
import torch
from d2l import torch as d2l

tokens = d2l.tokenize(d2l.read_time_machine())  # è¿›è¡Œåˆ†è¯å¤„ç†  
corpus = [token for line in tokens for token in line]  # å°†æ‰€æœ‰å•è¯æ‹¼æ¥æˆä¸€ä¸ªåˆ—è¡¨ï¼Œç”Ÿæˆè¯­æ–™åº“  
vocab = d2l.Vocab(corpus)  # ä½¿ç”¨è¯­æ–™åº“æ„å»ºè¯æ±‡è¡¨  
print(vocab.token_freqs[:10])
# [('the', 2261),  
#  ('i', 1267),  
#  ('and', 1245),  
#  ('of', 1155),  
#  ('a', 816),  
#  ('to', 695),  
#  ('was', 552),  
#  ('in', 541),  
#  ('that', 443),  
#  ('my', 440)]
```

- ä¸€äº›å¸¸è§çš„æµè¡Œè¯é€šå¸¸è¢«ç§°ä¸º==åœç”¨è¯==ï¼ˆstop wordsï¼‰ï¼Œå› æ­¤å¯ä»¥è¢«è¿‡æ»¤ï¼Œä½†æ˜¯ä¾ç„¶ä¼šåœ¨æ¨¡å‹ä¸­ä½¿ç”¨ï¼ˆå¤šä¸ªå•è¯ç»„åˆï¼‰
- å¯ä»¥çœ‹åˆ°å•è¯çš„ä½¿ç”¨é¢‘ç‡ä¸‹é™çš„å¾ˆå¿«ï¼Œç”»å‡ºè¯é¢‘å›¾

```python
# ä»è¯æ±‡è¡¨çš„token_freqsä¸­æå–é¢‘ç‡ä¿¡æ¯ï¼Œå­˜å‚¨åœ¨åˆ—è¡¨freqsä¸­  
freqs = [freq for token, freq in vocab.token_freqs]
# ä½¿ç”¨d2låº“ä¸­çš„plotå‡½æ•°ç»˜åˆ¶è¯é¢‘å›¾  
d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)', xscale='log', yscale='log')
```

- ![[00 Attachments/Pasted image 20240729194200.png|400]]
- é€šè¿‡å›¾ç‰‡å¯ä»¥å‘ç°ï¼šè¯é¢‘ä»¥è¿‘ä¼¼çº¿æ€§çš„æ–¹å¼è¿…é€Ÿè¡°å‡
- æ¥ä¸‹æ¥æŸ¥çœ‹äºŒå…ƒè¯­æ³•ã€ä¸‰å…ƒè¯­æ³•çš„æƒ…å†µ

```python
# äºŒå…ƒè¯­æ³•  
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]  # äºŒå…ƒè¯­æ³•è¯­æ–™åº“  
bigram_vocab = d2l.Vocab(bigram_tokens)  # æ„å»ºè¯æ±‡è¡¨  
print(bigram_vocab.token_freqs[:10])
# [(('of', 'the'), 309),  
#  (('in', 'the'), 169),  
#  (('i', 'had'), 130),  
#  (('i', 'was'), 112),  
#  (('and', 'the'), 109),  
#  (('the', 'time'), 102),  
#  (('it', 'was'), 99),  
#  (('to', 'the'), 85),  
#  (('as', 'i'), 78),  
#  (('of', 'a'), 73)]  
# ä¸‰å…ƒè¯­æ³•  
trigram_tokens = [triple for triple in zip(corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens)  # æ„å»ºè¯æ±‡è¡¨  
for i in trigram_vocab.token_freqs[:10]:
    print(i)
# (('the', 'time', 'traveller'), 59)  
# (('the', 'time', 'machine'), 30)  
# (('the', 'medical', 'man'), 24)  
# (('it', 'seemed', 'to'), 16)  
# (('it', 'was', 'a'), 15)  
# (('here', 'and', 'there'), 15)  
# (('seemed', 'to', 'me'), 14)  
# (('i', 'did', 'not'), 14)  
# (('i', 'saw', 'the'), 13)  
# (('i', 'began', 'to'), 13)
```

- å¯ä»¥çœ‹åˆ°å½“ä½¿ç”¨ä¸‰å…ƒè¯­æ³•çš„æ—¶å€™é«˜é¢‘è¯æ›´èƒ½ååº”æ–‡ç« çš„ä¿¡æ¯ï¼ˆä¿¡æ¯é‡æ›´å¤šï¼‰
- ç”»å‡ºè¿™ä¸‰ç§è¯å…ƒè¯­æ³•çš„è¯é¢‘å›¾

```python
# ç›´è§‚åœ°å¯¹æ¯”ä¸‰ç§æ¨¡å‹ä¸­çš„æ ‡è®°é¢‘ç‡  
bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
         ylabel='frequency: n(x)', xscale='log', yscale='log',
         legend=['unigram', 'bigram', 'trigram'])
```

- ![[00 Attachments/Pasted image 20240729194255.png|400]]
- ä½¿ç”¨ n å…ƒè¯­æ³•ï¼Œå½“ n è¶Šå¤§ï¼Œè®¡ç®—é‡ä¹Ÿè¶Šå¤§ï¼ˆæŒ‡æ•°å…³ç³»ï¼‰ï¼Œä½†æ˜¯ä»å›¾ä¸­ä¹Ÿå¯ä»¥çœ‹å‡ºï¼Œåœ¨ä¸‰å…ƒè¯­æ³•ä¸­ï¼Œå¤§å¤šæ•°è¯å…ƒçš„å‡ºç°é¢‘ç‡éå¸¸å°ï¼Œå¯ä»¥è¿›è¡Œè¿‡æ»¤æ“ä½œï¼ˆä¸€åˆ€åˆ‡ï¼‰
    - ==é•¿åºåˆ—å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼šå®ƒä»¬å¾ˆå°‘å‡ºç°æˆ–è€…ä»ä¸å‡ºç°==
    - æ‰€ä»¥åœ¨å®é™…ä¸­ï¼Œn å–è¾ƒå¤§å€¼ä¹Ÿæ˜¯å¯ä»¥æ¥å—çš„

#### è¯»å–é•¿åºåˆ—æ•°æ®ï¼ˆä¸¤ç§è¯»å–æ–¹å¼ï¼‰

- åºåˆ—æ•°æ®æœ¬è´¨ä¸Šæ˜¯è¿ç»­çš„ï¼Œå› æ­¤å½“åºåˆ—å˜å¾—å¤ªé•¿è€Œä¸èƒ½è¢«æ¨¡å‹ä¸€æ¬¡æ€§å…¨éƒ¨å¤„ç†æ—¶ï¼Œå°±å¸Œæœ›å¯¹åºåˆ—è¿›è¡Œæ‹†åˆ†ä»¥æ–¹ä¾¿æ¨¡å‹çš„è¯»å–
- ç­–ç•¥
    - å‡è®¾ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæ¨¡å‹ä¸­çš„ç½‘ç»œä¸€æ¬¡å¤„ç†å…·æœ‰é¢„å®šä¹‰é•¿åº¦ï¼ˆn ä¸ªæ—¶é—´æ­¥ï¼‰çš„ä¸€ä¸ªå°æ‰¹é‡åºåˆ—
- é‚£ä¹ˆè¯¥å¦‚ä½•éšæœºç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡æ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾ä¸€å…±è¯»å–ï¼Ÿï¼ˆç±»ä¼¼æ»‘åŠ¨çª—å£ï¼‰
    - é¦–å…ˆï¼Œæ–‡æœ¬åºåˆ—æ˜¯ä»»æ„é•¿çš„ï¼Œå› æ­¤ä»»æ„é•¿çš„åºåˆ—å¯ä»¥è¢«åˆ’åˆ†ä¸ºå…·æœ‰==ç›¸åŒï¼ˆnï¼‰æ—¶é—´æ­¥é•¿çš„å­åºåˆ—==
    - å‡è®¾ç¥ç»ç½‘ç»œä¸€æ¬¡å¤„ç†å…·æœ‰ n ä¸ªæ—¶é—´æ­¥çš„å­åºåˆ—
    - ä»åŸå§‹æ–‡æœ¬ä¸­è·å–å­åºåˆ—ï¼Œèµ·å§‹ä½ç½®ä¸åŒ![[00 Attachments/Pasted image 20240729200812.png|400]]
    - ä½¿ç”¨éšæœºåç§»é‡æ¥æŒ‡å®šå­åºåˆ—çš„èµ·å§‹ä½ç½®
        - è¦†ç›–æ€§ï¼ˆcoverageï¼‰ï¼šå–å¾—çš„å­åºåˆ—å°½å¯èƒ½è¦†ç›–åŸå§‹æ–‡æœ¬
        - éšæœºæ€§ï¼ˆrandomnessï¼‰ï¼šå­åºåˆ—çš„èµ·å§‹ä½ç½®æ˜¯éšæœºçš„

##### éšæœºé‡‡æ ·ï¼ˆrandom samplingï¼‰

- åœ¨éšæœºé‡‡æ ·ä¸­ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯åœ¨åŸå§‹çš„é•¿åºåˆ—ä¸Šä»»æ„æ•è·çš„å­åºåˆ—
- åœ¨è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œæ¥è‡ªä¸¤ä¸ªç›¸é‚»çš„ã€éšæœºçš„ã€å°æ‰¹é‡ä¸­çš„å­åºåˆ—ä¸ä¸€å®šåœ¨åŸå§‹åºåˆ—ä¸Šç›¸é‚»
- å¯¹äºè¯­è¨€å»ºæ¨¡ï¼Œç›®æ ‡æ˜¯åŸºäºåˆ°ç›®å‰ä¸ºæ­¢æˆ‘ä»¬çœ‹åˆ°çš„è¯å…ƒæ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯å…ƒï¼Œå› æ­¤æ ‡ç­¾æ˜¯ç§»ä½äº†ä¸€ä¸ªè¯å…ƒçš„åŸå§‹åºåˆ—

```python
# éšå³ç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡æ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾ä»¥ä¾›è¯»å–  
# åœ¨éšå³é‡‡æ ·ä¸­ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯åœ¨åŸå§‹çš„é•¿åºåˆ—ä¸Šä»»æ„æ•è·çš„å­åºåˆ—  

# ç»™ä¸€æ®µå¾ˆé•¿çš„åºåˆ—ï¼Œè¿ç»­åˆ‡æˆå¾ˆå¤šæ®µé•¿ä¸ºTçš„å­åºåˆ—  
# ä¸€å¼€å§‹åŠ äº†ä¸€ç‚¹éšæœºï¼Œä½¿å¾—æ¯æ¬¡åˆ‡çš„èµ·å§‹ä½ç½®ä¸åŒ  
def seq_data_iter_random(corpus, batch_size, num_steps):
    """ä½¿ç”¨éšå³æŠ½æ ·ç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡å­åºåˆ—  
    :param corpus: è¯æ±‡è¡¨  
    :param batch_size: æ‰¹é‡å¤§å°  
    :param num_steps: å­åºåˆ—é•¿åº¦  
    :return: ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€æ‰¹å­åºåˆ—çš„ç‰¹å¾å’Œæ ‡ç­¾  
    """  # ä»éšæœºåç§»é‡å¼€å§‹å¯¹åºåˆ—è¿›è¡Œåˆ†åŒºï¼ŒéšæœºèŒƒå›´åŒ…æ‹¬num_steps-1 ç¡®å®šèµ·å§‹ä½ç½®  
    corpus = corpus[random.randint(0, num_steps - 1):]
    # è®¡ç®—èƒ½å¤Ÿç”Ÿæˆçš„å­åºåˆ—æ•°é‡  
    num_subseqs = (len(corpus) - 1) // num_steps
    # åˆ›å»ºåˆå§‹ç´¢å¼•åˆ—è¡¨  
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # è¿›è¡Œéšæœºæ‰“ä¹±  
    random.shuffle(initial_indices)

    # è¿”å›ä»æŒ‡å®šä½ç½®å¼€å§‹çš„é•¿åº¦ä¸ºnum_stepsçš„å­åºåˆ—  
    def data(pos):
        return corpus[pos:pos + num_steps]

        # è®¡ç®—æ‰¹æ¬¡çš„æ•°é‡  

    num_batches = num_subseqs // batch_size
    # å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œè¿­ä»£  
    for i in range(0, batch_size * num_batches, batch_size):
        # è·å–å½“å‰æ‰¹æ¬¡çš„åˆå§‹ç´¢å¼•åˆ—è¡¨  
        initial_indices_per_batch = initial_indices[i:i + batch_size]
        # æ ¹æ®åˆå§‹ç´¢å¼•åˆ—è¡¨è·å–å¯¹åº”çš„ç‰¹å¾åºåˆ—X  
        X = [data(j) for j in initial_indices_per_batch]
        # æ ¹æ®åˆå§‹ç´¢å¼•åˆ—è¡¨è·å–å¯¹åº”çš„æ ‡ç­¾åºåˆ—Y  
        Y = [data(j + 1) for j in initial_indices_per_batch]
        # ä½¿ç”¨torch.tensorå°†Xå’ŒYè½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶é€šè¿‡yieldè¯­å¥è¿”å›  
        yield torch.tensor(X), torch.tensor(Y)
```

- ç”Ÿæˆä¸€ä¸ªä» 0 åˆ° 34 çš„åºåˆ—
    - å‡è®¾æ‰¹é‡å¤§å°ä¸º2ï¼Œæ—¶é—´æ­¥æ•°ä¸º5ï¼Œè¿™æ„å‘³ç€å¯ä»¥ç”Ÿæˆ$\lfloor (35 - 1) / 5 \rfloor= 6$ ä¸ªâ€œç‰¹å¾ï¼æ ‡ç­¾â€å­åºåˆ—å¯¹
    - å¦‚æœè®¾ç½®å°æ‰¹é‡å¤§å°ä¸º2ï¼Œæˆ‘ä»¬åªèƒ½å¾—åˆ°3ä¸ªå°æ‰¹é‡

```python
my_seq = list(range(35))
for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
# X:  tensor([[ 5,  6,  7,  8,  9],  
#         [15, 16, 17, 18, 19]])
# Y: tensor([[ 6,  7,  8,  9, 10],  
#         [16, 17, 18, 19, 20]])  
# X:  tensor([[25, 26, 27, 28, 29],  
#         [ 0,  1,  2,  3,  4]])
# Y: tensor([[26, 27, 28, 29, 30],  
#         [ 1,  2,  3,  4,  5]])  
# X:  tensor([[10, 11, 12, 13, 14],  
#         [20, 21, 22, 23, 24]])
# Y: tensor([[11, 12, 13, 14, 15],  
#         [21, 22, 23, 24, 25]])
```

##### é¡ºåºåˆ†åŒºï¼ˆsequential partitioningï¼‰

- åœ¨è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œé™¤äº†å¯¹åŸå§‹åºåˆ—å¯ä»¥éšæœºæŠ½æ ·å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä¿è¯==ä¸¤ä¸ªç›¸é‚»çš„å°æ‰¹é‡ä¸­çš„å­åºåˆ—åœ¨åŸå§‹åºåˆ—ä¸Šä¹Ÿæ˜¯ç›¸é‚»çš„==ï¼ˆä¸æ˜¯å°æ‰¹é‡ä¸­çš„å­åºåˆ—ç›¸é‚»ï¼‰
- è¿™ç§ç­–ç•¥åœ¨åŸºäºå°æ‰¹é‡çš„è¿­ä»£è¿‡ç¨‹ä¸­==ä¿ç•™äº†æ‹†åˆ†çš„å­åºåˆ—çš„é¡ºåº==ï¼Œå› æ­¤ç§°ä¸ºé¡ºåºåˆ†åŒº

```python
# ä¿è¯ä¸¤ä¸ªç›¸é‚»çš„å°æ‰¹é‡ä¸­çš„å­åºåˆ—åœ¨åŸå§‹åºåˆ—ä¸Šä¹Ÿæ˜¯ç›¸é‚»çš„  
def seq_data_iter_sequential(corpus, batch_size, num_steps):
    """  
    ä½¿ç”¨é¡ºåºåˆ†åŒºç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡å­åºåˆ—  
    :param corpus: è¯æ±‡è¡¨  
    :param batch_size: æ‰¹é‡å¤§å°  
    :param num_steps: å­åºåˆ—é•¿åº¦  
    :return: ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€æ‰¹å­åºåˆ—çš„ç‰¹å¾å’Œæ ‡ç­¾  
    """  # éšæœºé€‰æ‹©ä¸€ä¸ªåç§»é‡ä½œä¸ºèµ·å§‹ä½ç½®  
    offset = random.randint(0, num_steps)
    # è®¡ç®—å¯ä»¥ç”Ÿæˆçš„å­åºåˆ—çš„æ€»é•¿åº¦  
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    # åˆ›å»ºç‰¹å¾åºåˆ—Xçš„å¼ é‡  
    Xs = torch.tensor(corpus[offset:offset + num_tokens])
    # åˆ›å»ºæ ‡ç­¾åºåˆ—Yçš„å¼ é‡  
    Ys = torch.tensor(corpus[offset + 1:offset + 1 + num_tokens])
    # é‡æ–°è°ƒæ•´Xså’ŒYsçš„å½¢çŠ¶ï¼Œä½¿å…¶æˆä¸º(batch_size, -1)çš„äºŒç»´å¼ é‡  
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    # è®¡ç®—å¯ä»¥ç”Ÿæˆçš„æ‰¹æ¬¡æ•°é‡  
    num_batches = Xs.shape[1] // num_steps
    # å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œè¿­ä»£  
    for i in range(0, num_steps * num_batches, num_steps):
        # è·å–å½“å‰æ‰¹æ¬¡çš„ç‰¹å¾åºåˆ—X  
        X = Xs[:, i:i + num_steps]
        # è·å–å½“å‰æ‰¹æ¬¡çš„æ ‡ç­¾åºåˆ—Y  
        Y = Ys[:, i:i + num_steps]
        # ä½¿ç”¨yieldè¯­å¥è¿”å›Xå’ŒYä½œä¸ºç”Ÿæˆå™¨çš„è¾“å‡º  
        yield X, Y


for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
# X:  tensor([[ 5,  6,  7,  8,  9],  
#         [19, 20, 21, 22, 23]]) 
# Y: tensor([[ 6,  7,  8,  9, 10],  
#         [20, 21, 22, 23, 24]])  
# X:  tensor([[10, 11, 12, 13, 14],  
#         [24, 25, 26, 27, 28]])
# Y: tensor([[11, 12, 13, 14, 15],  
#         [25, 26, 27, 28, 29]])
```

- è¿­ä»£æœŸé—´æ¥è‡ªä¸¤ä¸ªç›¸é‚»çš„å°æ‰¹é‡ä¸­çš„å­åºåˆ—åœ¨åŸå§‹åºåˆ—ä¸­ç¡®å®æ˜¯ç›¸é‚»çš„

##### æ‰“åŒ…

```python
class SeDataLoader:
    """åŠ è½½åºåˆ—æ•°æ®çš„è¿­ä»£å™¨"""

    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        # æ ¹æ®use_random_iteré€‰æ‹©æ•°æ®è¿­ä»£å‡½æ•°  
        if use_random_iter:
            # ä½¿ç”¨éšæœºåˆ†åŒºè¿­ä»£å™¨  
            self.data_iter_fn = d2l.seq_data_iter_random
        else:
            # ä½¿ç”¨é¡ºåºåˆ†åŒºè¿­ä»£å™¨  
            self.data_iter_fn = d2l.seq_data_iter_sequential
            # åŠ è½½æ•°æ®é›†å’Œè¯æ±‡è¡¨  
        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)
        # è®¾ç½®æ‰¹é‡å¤§å°å’Œæ­¥é•¿  
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        # è¿”å›æ•°æ®è¿­ä»£å™¨  
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)


def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):
    """è¿”å›æ—¶å…‰æœºå™¨æ•°æ®é›†çš„è¿­ä»£å™¨å’Œè¯æ±‡è¡¨"""
    # è¿™ä¸ªå¯¹è±¡å°†ä½œä¸ºæ•°æ®çš„è¿­ä»£å™¨ï¼Œç”¨äºäº§ç”Ÿå°æ‰¹é‡çš„æ ·æœ¬å’Œæ ‡ç­¾ã€‚  
    data_iter = SeDataLoader(batch_size, num_steps, use_random_iter, max_tokens)
    # è¿”å›æ•°æ®è¿­ä»£å™¨å’Œå¯¹åº”çš„è¯æ±‡è¡¨  
    return data_iter, data_iter.vocab
```

## å¾ªç¯ç¥ç»ç½‘ç»œ ï¼ˆrecurrent neural networksï¼‰

### å¾ªç¯ç¥ç»ç½‘ç»œ

- æ ‡å‡†ç¥ç»ç½‘ç»œä¸­çš„æ‰€æœ‰è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œä½†æ˜¯åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¾‹å¦‚åœ¨é¢„æµ‹çŸ­è¯­çš„ä¸‹ä¸€ä¸ªå•è¯æ—¶ï¼Œå‰é¢çš„å•è¯æ˜¯å¿…è¦çš„ï¼Œå› æ­¤å¿…é¡»è®°ä½å‰é¢çš„å•è¯
    - ç»“æœï¼ŒRNN åº”è¿è€Œç”Ÿï¼Œå®ƒä½¿ç”¨éšè—å±‚æ¥å…‹æœè¿™ä¸ªé—®é¢˜ã€‚ RNN æœ€é‡è¦çš„ç»„æˆéƒ¨åˆ†æ˜¯éšè—çŠ¶æ€ï¼Œå®ƒè®°ä½æœ‰å…³åºåˆ—çš„ç‰¹å®šä¿¡æ¯

#### å¸¦éšçŠ¶æ€çš„å¾ªç¯ç¥ç»ç½‘ç»œ

- å›é¡¾æ½œå˜é‡æ¨¡å‹![[00 Attachments/Pasted image 20240726004026.png|400]]
- åœ¨å¤šå±‚æ„ŸçŸ¥æœºä¸­åŠ å…¥æ½œå˜é‡ï¼Œ==æ•è·å¹¶ä¿ç•™äº†åºåˆ—ç›´åˆ°å…¶å½“å‰æ—¶é—´æ­¥çš„å†å²ä¿¡æ¯==ï¼Œ
  å°±å¦‚å½“å‰æ—¶é—´æ­¥ä¸‹ç¥ç»ç½‘ç»œçš„çŠ¶æ€æˆ–è®°å¿†![[00 Attachments/Pasted image 20240726010827.png|400]]
    - $h_t$ ç”±ä¹‹å‰çš„ x ç¡®å®šï¼Œç”¨äºé¢„æµ‹å½“å‰çš„ $x_t$ï¼ˆå³ $o_t$ï¼‰
    - å®é™…çš„æ•ˆæœï¼ˆæ‰“å­—é¢„æµ‹ï¼‰![[00 Attachments/Pasted image 20240726010227.png|400]]
    - æŸå¤±æ˜¯é€šè¿‡ $o_t$ å’Œ $x_t$ çš„å…³ç³»æ¥è®¡ç®— $loss = f(x_t, o_t)$
    - ==ä¿å­˜äº†å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—å˜é‡ $\mathbf{H}_{t-1}$ï¼Œ
      å¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æƒé‡å‚æ•° $\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$ï¼Œ æ¥æè¿°å¦‚ä½•åœ¨å½“å‰æ—¶é—´æ­¥ä¸­ä½¿ç”¨å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—å˜é‡==
- éšçŠ¶æ€ä½¿ç”¨çš„å®šä¹‰ä¸å‰ä¸€ä¸ªæ—¶é—´æ­¥ä¸­ä½¿ç”¨çš„å®šä¹‰ç›¸åŒï¼Œ å› æ­¤æ½œå˜é‡çš„è®¡ç®—æ˜¯å¾ªç¯çš„ï¼ˆrecurrentï¼‰
    - äºæ˜¯åŸºäºå¾ªç¯è®¡ç®—çš„éšçŠ¶æ€ç¥ç»ç½‘ç»œè¢«å‘½åä¸ºå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆrecurrent neural networkï¼‰
- ä¸‰ä¸ªç›¸é‚»æ—¶é—´æ­¥çš„è®¡ç®—é€»è¾‘![[00 Attachments/Pasted image 20240726013627.png|400]]
    1. æ‹¼æ¥å½“å‰æ—¶é—´æ­¥ ğ‘¡ çš„è¾“å…¥ ğ‘‹ğ‘¡ å’Œå‰ä¸€æ—¶é—´æ­¥ ğ‘¡âˆ’1 çš„éšçŠ¶æ€ ğ»ğ‘¡âˆ’1
    2. å°†æ‹¼æ¥çš„ç»“æœé€å…¥å¸¦æœ‰æ¿€æ´»å‡½æ•° ğœ™ çš„å…¨è¿æ¥å±‚ã€‚ å…¨è¿æ¥å±‚çš„è¾“å‡ºæ˜¯å½“å‰æ—¶é—´æ­¥ ğ‘¡ çš„éšçŠ¶æ€ ğ»ğ‘¡

#### å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰

- è¡¡é‡æ¨¡å‹çš„å¥½å![[00 Attachments/Pasted image 20240726013828.png|400]]
    - è¾“å‡ºå³ä¸ºåˆ¤æ–­ä¸‹ä¸€ä¸ªè¯ï¼Œå‡è®¾è¯è¡¨ä¸­æœ‰ m ä¸ªè¯ï¼Œ==ä¸€ä¸ªè¯å°±å¯ä»¥è§†ä¸ºä¸€ä¸ªç±»åˆ«ï¼Œå³ç±»ä¼¼äºåšä¸€ä¸ªåˆ†ç±»é—®é¢˜==ï¼Œåˆ¤æ–­å„ä¸ªè¯å‡ºç°çš„æ¦‚ç‡ï¼Œé‚£ä¹ˆå°±å¯ä»¥ä½¿ç”¨äº¤å‰ç†µæŸå¤±
        - é€šè¿‡ä¸€ä¸ªåºåˆ—ä¸­æ‰€æœ‰çš„ ğ‘› ä¸ªè¯å…ƒçš„äº¤å‰ç†µæŸå¤±çš„å¹³å‡å€¼æ¥è¡¡é‡
        - ğ‘ƒ ç”±è¯­è¨€æ¨¡å‹ç»™å‡ºï¼Œ ğ‘¥ğ‘¡ æ˜¯åœ¨æ—¶é—´æ­¥ ğ‘¡ ä»è¯¥åºåˆ—ä¸­è§‚å¯Ÿåˆ°çš„å®é™…è¯å…ƒ
- å›°æƒ‘åº¦çš„æœ€å¥½çš„ç†è§£æ˜¯â€œä¸‹ä¸€ä¸ªè¯å…ƒçš„å®é™…é€‰æ‹©æ•°çš„è°ƒå’Œå¹³å‡æ•°â€
    - åœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€»æ˜¯å®Œç¾åœ°ä¼°è®¡æ ‡ç­¾è¯å…ƒçš„æ¦‚ç‡ä¸º 1
        - åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„å›°æƒ‘åº¦ä¸º 1
    - åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€»æ˜¯é¢„æµ‹æ ‡ç­¾è¯å…ƒçš„æ¦‚ç‡ä¸º 0
        - åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›°æƒ‘åº¦æ˜¯æ­£æ— ç©·å¤§
    - åœ¨åŸºçº¿ä¸Šï¼Œè¯¥æ¨¡å‹çš„é¢„æµ‹æ˜¯è¯è¡¨çš„æ‰€æœ‰å¯ç”¨è¯å…ƒä¸Šçš„å‡åŒ€åˆ†å¸ƒ
        - åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›°æƒ‘åº¦ç­‰äºè¯è¡¨ä¸­å”¯ä¸€è¯å…ƒçš„æ•°é‡
        - äº‹å®ä¸Šï¼Œå¦‚æœæˆ‘ä»¬åœ¨æ²¡æœ‰ä»»ä½•å‹ç¼©çš„æƒ…å†µä¸‹å­˜å‚¨åºåˆ—ï¼Œ è¿™å°†æ˜¯æˆ‘ä»¬èƒ½åšçš„æœ€å¥½çš„ç¼–ç æ–¹å¼
        - å› æ­¤ï¼Œè¿™ç§æ–¹å¼æä¾›äº†ä¸€ä¸ªé‡è¦çš„ä¸Šé™ï¼Œ è€Œä»»ä½•å®é™…æ¨¡å‹éƒ½å¿…é¡»è¶…è¶Šè¿™ä¸ªä¸Šé™

#### æ¢¯åº¦è£å‰ª

- å¯¹äºé•¿åº¦ä¸º T çš„åºåˆ—ï¼Œåœ¨è¿­ä»£ä¸­è®¡ç®—è¿™Tä¸ªæ—¶é—´æ­¥ä¸Šçš„æ¢¯åº¦ï¼Œ å°†ä¼šåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­äº§ç”Ÿé•¿åº¦ä¸ºO(T)çš„çŸ©é˜µä¹˜æ³•é“¾
    - å½“ T è¾ƒå¤§æ—¶ï¼Œå®ƒå¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®šï¼Œ ä¾‹å¦‚å¯èƒ½å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸æˆ–æ¢¯åº¦æ¶ˆå¤±
- æ§åˆ¶æ¢¯åº¦çš„æ•°å€¼![[00 Attachments/Pasted image 20240726020146.png|400]]
    - æ¢¯åº¦é“¾ï¼šé€šè¿‡å¹³å‡äº¤å‰ç†µæŸå¤±æ±‚æ¢¯åº¦ï¼Œæ²¿ç€æ­¥é•¿åå‘ä¼ æ’­ï¼š$tâ€”â€”>t-1â€”â€”>t-2â€”â€”>...$

#### RNN çš„åº”ç”¨

- ![[00 Attachments/Pasted image 20240726020645.png|400]]
    - æ‰€è°“çš„ä¸€å¯¹ä¸€å°±æ˜¯ MLPï¼Œç»™ä¸€ä¸ªæ ·æœ¬ï¼Œè¾“å‡ºä¸€ä¸ªé¢„æµ‹ç±»åˆ«
    - æ–‡æœ¬ç”Ÿæˆï¼šç»™å®šå¼€å§‹çš„è¯ï¼ˆå›¾ç‰‡æˆ–è€…éŸ³ä¹ï¼‰ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œå°†é¢„æµ‹çš„è¯ä½œä¸ºè¾“å…¥ï¼Œç”¨äºç»§ç»­é¢„æµ‹ï¼ˆçœ‹å›¾è¯´è¯ï¼‰
    - æ–‡æœ¬åˆ†ç±»ï¼šç»™å®šå¥å­åºåˆ—ï¼Œéšç€åºåˆ—çš„è¾“å…¥æ›´æ–°éšå˜é‡å±‚ï¼Œæœ€åå°†åˆ†æœ¬åˆ†ç±»ï¼ˆæƒ…æ„Ÿåˆ†ç±»ï¼‰
    - Tag ç”Ÿæˆï¼šè¯æ€§é¢„æµ‹

#### æ€»ç»“

- å¾ªç¯ç¥ç»ç½‘ç»œçš„è¾“å‡ºå–å†³äºå½“ä¸‹è¾“å…¥å’Œå‰ä¸€æ—¶é—´çš„éšå˜é‡
    - éšå˜é‡ç”¨äº==å­˜å‚¨å†å²æ—¶åˆ»çš„ä¿¡æ¯==
- åº”ç”¨åˆ°è¯­è¨€æ¨¡å‹ä¸­æ—¶ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œæ ¹æ®å½“å‰è¯é¢„æµ‹ä¸‹ä¸€æ¬¡æ—¶åˆ»è¯
- é€šå¸¸ä½¿ç”¨å›°æƒ‘åº¦æ¥è¡¡é‡è¯­è¨€æ¨¡å‹çš„å¥½å
    - æ¯ä¸€æ­¥é¢„æµ‹çš„å¹³å‡
- ![[00 Attachments/Pasted image 20240726081005.png|500]]

### RNN ä»é›¶å¼€å§‹å®ç°

- è¯»å–æ•°æ®é›†

```python
%matplotlib
inline
import math
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

# å®šä¹‰æ‰¹é‡å¤§å°å’Œæ—¶é—´æ­¥æ•°
batch_size, num_steps = 32, 35
# åŠ è½½æ—¶é—´æœºå™¨æ•°æ®å¹¶åˆ›å»ºè¯æ±‡è¡¨
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
```

#### ç‹¬çƒ­ç¼–ç 

- ==é€šè¿‡ç‹¬çƒ­ç¼–ç å°†æ¯ä¸ªè¯å…ƒï¼ˆå·²ç»å®Œæˆåˆ°æ•°å­—çš„æ˜ å°„ï¼‰è¡¨ç¤ºä¸ºæ›´å…·è¡¨ç°åŠ›çš„ç‰¹å¾å‘é‡==
    - ä¹‹åä¼šä½¿ç”¨åµŒå…¥å±‚æ¥è¡¨ç¤ºè¯å…ƒä¹‹é—´çš„å…³ç³»ï¼Œæ„æ€ç›¸è¿‘çš„è¯å…ƒé€šè¿‡åµŒå…¥å±‚åçš„ç©ºé—´ä½ç½®ä¼šæ›´åŠ ç›¸è¿‘
- ä¸¾ä¾‹

```python
# æ‰“å°è¯æ±‡è¡¨çš„å¤§å°  
print(len(vocab))
# ä½¿ç”¨ç‹¬çƒ­ç¼–ç å°† [0, 2] è¡¨ç¤ºçš„ç‰©ä½“ä¸‹æ ‡è½¬æ¢ä¸ºç‹¬çƒ­å‘é‡ï¼Œå…¶ä¸­0è¡¨ç¤ºç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œ2è¡¨ç¤ºç¬¬3ä¸ªå…ƒç´   
F.one_hot(torch.tensor([0, 2]), len(vocab))
# 28  
# tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
#          0, 0, 0, 0],  
#         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
#          0, 0, 0, 0]])
```

- é‡‡æ ·çš„å°æ‰¹é‡æ•°æ®çš„å½¢çŠ¶æ˜¯ ï¼ˆæ‰¹é‡å¤§å°ã€æ—¶é—´æ­¥æ•°ï¼‰ï¼Œä¸ºäºŒç»´å¼ é‡
    - one_hot å°†è¿™ä¸ªæ‰¹é‡æ•°æ®è½¬åŒ–ä¸ºä¸‰ç»´å¼ é‡ï¼ˆæ‰¹é‡å¤§å°ã€æ—¶é—´æ­¥æ•°ã€è¯æ±‡æ ‡å¤§å°ï¼‰
    - èƒ½å¤Ÿæ›´æ–¹ä¾¿åœ°é€šè¿‡æœ€å¤–å±‚çš„ç»´åº¦ï¼Œ ä¸€æ­¥ä¸€æ­¥åœ°æ›´æ–°å°æ‰¹é‡æ•°æ®çš„éšçŠ¶æ€

```python
# å°æ‰¹é‡å½¢çŠ¶æ˜¯(æ‰¹é‡å¤§å°ï¼Œæ—¶é—´æ­¥æ•°)  
X = torch.arange(10).reshape((2, 5))  # å¯¹Xçš„è½¬ç½®è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼Œå…¶ä¸­28è¡¨ç¤ºç¼–ç é•¿åº¦ï¼Œè¿”å›ç‹¬çƒ­ç¼–ç åçš„å½¢çŠ¶  
F.one_hot(X.T, 28).shape
# torch.Size([5, 2, 28])
```

- è¿™é‡Œçš„è½¬ç½®æ˜¯ä¸ºäº†å°†æ—¶é—´æ­¥ä¸ºè¡Œï¼Œæ‰¹é‡ä¸ºåˆ—ï¼Œæ–¹ä¾¿æŒ‰æ—¶é—´æ­¥è¿›è¡Œç´¢å¼•ï¼ˆå„ä¸ªæ‰¹é‡æ˜¯æœ‰åºçš„ï¼‰

#### åˆå§‹åŒ–æ¨¡å‹å‚æ•°

- éšè—å•å…ƒæ•°`num_hiddens`æ˜¯ä¸€ä¸ªå¯è°ƒçš„è¶…å‚æ•°ã€‚ å½“è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶ï¼Œè¾“å…¥å’Œè¾“å‡ºæ¥è‡ªç›¸åŒçš„è¯è¡¨ã€‚ å› æ­¤ï¼Œå®ƒä»¬å…·æœ‰ç›¸åŒçš„ç»´åº¦ï¼Œå³è¯è¡¨çš„å¤§å°

```python
def get_params(vocab_size, num_hiddens, device):
    """  
    åˆå§‹åŒ–å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ¨¡å‹å‚æ•°  
    :param vocab_size: è¯æ±‡è¡¨å¤§å°  
    :param num_hiddens: éšè—å±‚å¤§å°  
    :return: è¿”å›æ¨¡å‹å‚æ•°ï¼ˆæƒé‡çŸ©é˜µå’Œåç½®å‘é‡ï¼‰  
    """
    num_inputs = num_outputs = vocab_size  # è¾“å…¥è¾“å‡ºä¸ºç» one-hot ç¼–ç åçš„è¯æ±‡å‘é‡ 

    def normal(shape):
        """ç”ŸæˆæŒ‡å®šå½¢çŠ¶çš„éšæœºå¼ é‡"""
        return torch.randn(size=shape, device=device) * 0.01
        # åˆå§‹åŒ–æ¨¡å‹å‚æ•°  

    W_xh = normal((num_inputs, num_hiddens))
    W_hh = normal((num_hiddens, num_hiddens))
    b_h = torch.zeros(num_hiddens, device=device)
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    params = [W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)  # è®¾ç½®å‚æ•°ä¸ºéœ€è¦æ¢¯åº¦æ›´æ–°  

    return params
```

#### å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹

- åœ¨å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹ä¹‹å‰ï¼Œé¦–å…ˆéœ€è¦ä¸€ä¸ª `init_rnn_state` åˆå§‹åŒ–éšçŠ¶æ€

```python
def init_rnn_state(batch_size, num_hiddens, device):
    """åˆå§‹åŒ–ä¸ºé›¶çš„éšè—çŠ¶æ€"""
    return (torch.zeros((batch_size, num_hiddens), device=device),)
```

- ä¸‹é¢çš„`rnn`å‡½æ•°å®šä¹‰äº†å¦‚ä½•åœ¨ä¸€ä¸ªæ—¶é—´æ­¥å†…è®¡ç®—éšçŠ¶æ€å’Œè¾“å‡º
    - å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹é€šè¿‡`inputs`æœ€å¤–å±‚çš„ç»´åº¦ï¼ˆæ—¶é—´æ­¥ï¼‰å®ç°å¾ªç¯ï¼Œ ä»¥ä¾¿é€æ—¶é—´æ­¥æ›´æ–°å°æ‰¹é‡æ•°æ®çš„éšçŠ¶æ€`H`
- ==ç±»ä¼¼äºå‰å‘ä¼ æ’­==ï¼Œä½†åˆæœ‰äº›ä¸åŒ
    - `input`ä¸å†æ˜¯ï¼ˆæ‰¹é‡ï¼Œå¤§å°ï¼‰ï¼Œè€Œæ˜¯ï¼ˆæ—¶é—´æ­¥æ•°ï¼Œæ‰¹é‡ï¼Œå¤§å°ï¼‰
    - åŠ å…¥äº†éšçŠ¶æ€ä½œä¸ºå¾ªç¯çš„å®ç°

```python
def rnn(inputs, state, params):
    """  
    å®šä¹‰å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ŒåŒ…æ‹¬äº†æ‰€æœ‰æ—¶é—´æ­¥çš„è®¡ç®—ï¼Œç±»ä¼¼äºå‰å‘ä¼ æ’­  
    :param inputs: è¾“å…¥åºåˆ—å½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•°ï¼Œæ‰¹é‡å¤§å°, è¯æ±‡è¡¨å¤§å°)  
    :param state: éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º (æ‰¹é‡å¤§å°, éšè—å±‚å¤§å°)  
    :param params: æ¨¡å‹å‚æ•°ï¼ŒåŒ…å«æƒé‡çŸ©é˜µå’Œåç½®å‘é‡  
    :return: è¿”å›è¾“å‡ºåºåˆ—å’Œæ–°çš„éšè—çŠ¶æ€ï¼Œå½¢çŠ¶å‡ä¸º (æ—¶é—´æ­¥æ•° * æ‰¹é‡å¤§å°, è¾“å‡ºå¤§å°)  
    """
    W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    for X in inputs:  # å¯¹äºæ¯ä¸ªæ—¶é—´æ­¥  
        # æ ¹æ®å½“å‰è¾“å…¥ Xã€ä¸Šä¸€æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ Hã€ä»¥åŠæƒé‡çŸ©é˜µå’Œåç½®å‘é‡æ¥è®¡ç®—  
        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)  # æ›´æ–°éšçŠ¶æ€  
        # è®¡ç®—è¾“å‡º Yï¼Œé€šè¿‡éšè—çŠ¶æ€ H ä¸æƒé‡çŸ©é˜µ W_hq ç›¸ä¹˜å¹¶åŠ ä¸Šåç½®å‘é‡ b_q å¾—åˆ°  
        Y = torch.mm(H, W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)  # å½¢çŠ¶ï¼š(æ—¶é—´æ­¥æ•° * æ‰¹é‡å¤§å°, è¾“å‡ºå¤§å°)
```

- åˆ›å»ºä¸€ä¸ªç±»æ¥åŒ…è£…è¿™äº›å‡½æ•°ï¼Œå¹¶å­˜å‚¨ä»é›¶å¼€å§‹å®ç°çš„å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹çš„å‚æ•°

```python
# åˆ›å»ºä¸€ä¸ªç±»æ¥åŒ…è£…è¿™äº›å‡½æ•°  
class RNNModelScratch:
    def __init__(self, vocab_size, num_hiddens, device, get_params,
                 init_state, forward_fn):
        """åˆå§‹åŒ–æ¨¡å‹"""
        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens  # è¯æ±‡è¡¨å¤§å°ï¼Œéšè—å±‚å¤§å°  
        self.params = get_params(vocab_size, num_hiddens, device)
        self.init_state, self.forward_fn = init_state, forward_fn  # éšè—çŠ¶æ€åˆå§‹åŒ–å‡½æ•°ï¼Œå‰å‘ä¼ æ’­å‡½æ•°ï¼ˆrnnï¼‰  

        def __call__(self, X, state):

        # å°†è¾“å…¥åºåˆ— X è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼Œå½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°, è¯æ±‡è¡¨å¤§å°)  
        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
        # è°ƒç”¨å‰å‘ä¼ æ’­å‡½æ•°è¿›è¡Œæ¨¡å‹è®¡ç®—ï¼Œå¹¶è¿”å›è¾“å‡ºï¼Œå½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•° * æ‰¹é‡å¤§å°, è¾“å‡ºå¤§å°)  
        return self.forward_fn(X, state, self.params)

    def begin_state(self, batch_size, device):
        # è¿”å›åˆå§‹åŒ–çš„éšè—çŠ¶æ€ï¼Œç”¨äºæ¨¡å‹çš„åˆå§‹æ—¶é—´æ­¥  
        return self.init_state(batch_size, self.num_hiddens, device)
```

- ä¸¾ä¾‹ï¼ŒæŸ¥çœ‹è¾“å‡º

```python
num_hiddens = 512
net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,
                      init_rnn_state, rnn)
state = net.begin_state(X.shape[0], d2l.try_gpu())  # è¾“å…¥è¾“å‡ºéƒ½ä¸ºï¼ˆæ‰¹é‡å¤§å°ï¼Œéšè—å±‚å¤§å°ï¼‰  
Y, new_state = net(X.to(d2l.try_gpu()), state)  # å‰å‘ä¼ æ’­
Y.shape, type(new_state), len(new_state), new_state[0].shape
# (torch.Size([10, 28]), tuple, 1, torch.Size([2, 512]))
```

- å¯ä»¥çœ‹åˆ°è¾“å‡ºå½¢çŠ¶æ˜¯ï¼ˆæ—¶é—´æ­¥æ•°Ã—æ‰¹é‡å¤§å°ï¼Œè¯æ±‡è¡¨å¤§å°ï¼‰ï¼Œè€ŒéšçŠ¶æ€å½¢çŠ¶ä¿æŒä¸å˜ï¼Œå³ï¼ˆæ‰¹é‡å¤§å°ï¼Œéšè—å•å…ƒæ•°ï¼‰

#### é¢„æµ‹

- å®šä¹‰é¢„æµ‹å‡½æ•°æ¥ç”Ÿæˆ`prefix`ä¹‹åçš„æ–°å­—ç¬¦
    - åœ¨å¾ªç¯éå†`prefix`ä¸­çš„å¼€å§‹å­—ç¬¦æ—¶ï¼Œä¸æ–­åœ°å°†éšçŠ¶æ€ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œä½†æ˜¯ä¸ç”Ÿæˆä»»ä½•è¾“å‡º
    - è¿™è¢«ç§°ä¸ºé¢„çƒ­ï¼ˆwarm-upï¼‰æœŸï¼Œå› ä¸ºåœ¨æ­¤æœŸé—´æ¨¡å‹ä¼šè‡ªæˆ‘æ›´æ–°ï¼ˆä¾‹å¦‚ï¼Œæ›´æ–°éšçŠ¶æ€ï¼‰ï¼Œä½†ä¸ä¼šè¿›è¡Œé¢„æµ‹
    - é¢„çƒ­æœŸç»“æŸåï¼ŒéšçŠ¶æ€çš„å€¼é€šå¸¸æ¯”åˆšå¼€å§‹çš„åˆå§‹å€¼æ›´é€‚åˆé¢„æµ‹ï¼Œä»è€Œé¢„æµ‹å­—ç¬¦å¹¶è¾“å‡ºå®ƒä»¬

```python
def predict_ch8(prefix, num_preds, net, vocab, device):
    """åœ¨prefixåé¢ç”Ÿæˆæ–°å­—ç¬¦  
    :param prefix: å­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºè¦ç”Ÿæˆæ–°å­—ç¬¦çš„å‰ç¼€  
    :param num_preds: è¦ç”Ÿæˆçš„æ–°å­—ç¬¦çš„æ•°é‡  
    :param net: å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹  
    :param vocab: è¯æ±‡è¡¨  
    """
    state = net.begin_state(batch_size=1, device=device)  # éšè—çŠ¶æ€åˆå§‹åŒ–  
    outputs = [vocab[prefix[0]]]  # ç¬¬ä¸€ä¸ªè¾“å‡ºï¼Œæ–¹ä¾¿ get_input å‡½æ•°è·å–  
    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape(
        (1, 1))  # å¼ é‡åŒ–å‡½æ•°ï¼Œå°†æœ€è¿‘é¢„æµ‹çš„å­—ç¬¦ä½œä¸ºè¾“å…¥ï¼Œå½¢çŠ¶ä¸º (æ—¶é—´æ­¥æ•°, æ‰¹é‡å¤§å°)  
    for y in prefix[1:]:  # é¢„çƒ­æœŸé—´çš„è¾“å…¥ï¼Œæ­¤æ—¶åœ¨æ›´æ–°éšè—çŠ¶æ€
        _, state = net(get_input(), state)
        outputs.append(vocab[y])
    for _ in range(num_preds):  # é¢„æµ‹num_predsæ­¥  
        y, state = net(get_input(), state)
        outputs.append(int(y.argmax(dim=1).reshape(1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])  
```

- å°è¯•é¢„æµ‹ï¼ˆè¿˜æ²¡æœ‰è®­ç»ƒï¼‰

```python
predict_ch8('time traveller ', 10, net, vocab, d2l.try_gpu())
# 'time traveller lkxdcccccc'
```

#### æ¢¯åº¦è£å‰ª

- $$\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}$$

```python
def grad_clipping(net, theta):
    """  
    è£å‰ªæ¢¯åº¦  
    :param net: å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹  
    :param theta: è£å‰ªé˜ˆå€¼  
    """
    if isinstance(net, nn.Module):  # å¦‚æœ net æ˜¯ nn.Module çš„å®ä¾‹  
        # è·å–æ‰€æœ‰éœ€è¦è®¡ç®—æ¢¯åº¦çš„å‚æ•°åˆ—è¡¨  
        params = [p for p in net.parameters() if p.requires_grad]
    else:  # å¦‚æœ net æ˜¯è‡ªå®šä¹‰çš„æ¨¡å‹  
        params = net.params
    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))  # è®¡ç®—æ¢¯åº¦èŒƒæ•°  
    if norm > theta:  # å¦‚æœæ¢¯åº¦èŒƒæ•°å¤§äºè£å‰ªé˜ˆå€¼  
        for param in params:
            param.grad[:] *= theta / norm  # ç¼©æ”¾æ¢¯åº¦
```

#### è®­ç»ƒ

- åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°åœ¨ä¸€ä¸ªè¿­ä»£å‘¨æœŸå†…è®­ç»ƒæ¨¡å‹ã€‚å®ƒä¸ä¹‹å‰çš„æ–¹å¼æœ‰ä¸‰ä¸ªä¸åŒä¹‹å¤„
    1. åºåˆ—æ•°æ®çš„ä¸åŒé‡‡æ ·æ–¹æ³•ï¼ˆéšæœºé‡‡æ ·å’Œé¡ºåºåˆ†åŒºï¼‰å°†å¯¼è‡´éšçŠ¶æ€åˆå§‹åŒ–çš„å·®å¼‚
    2. åœ¨æ›´æ–°æ¨¡å‹å‚æ•°ä¹‹å‰è£å‰ªæ¢¯åº¦
        - è¿™æ ·çš„æ“ä½œçš„ç›®çš„æ˜¯ï¼Œå³ä½¿è®­ç»ƒè¿‡ç¨‹ä¸­æŸä¸ªç‚¹ä¸Šå‘ç”Ÿäº†æ¢¯åº¦çˆ†ç‚¸ï¼Œä¹Ÿèƒ½ä¿è¯æ¨¡å‹ä¸ä¼šå‘æ•£
    3. é‡‡ç”¨å›°æƒ‘åº¦æ¥è¯„ä»·æ¨¡å‹
        - è¿™æ ·çš„åº¦é‡ç¡®ä¿äº†ä¸åŒé•¿åº¦çš„åºåˆ—å…·æœ‰å¯æ¯”æ€§
- ä¸¤ç§ä¸åŒåˆ†åŒºçš„å¤„ç†æ–¹å¼
    - å½“ä½¿ç”¨é¡ºåºåˆ†åŒºæ—¶ï¼Œ==åªåœ¨æ¯ä¸ªè¿­ä»£å‘¨æœŸçš„å¼€å§‹ä½ç½®åˆå§‹åŒ–éšçŠ¶æ€==
        - ç”±äºä¸‹ä¸€ä¸ªå°æ‰¹é‡æ•°æ®ä¸­çš„ç¬¬ i ä¸ªå­åºåˆ—æ ·æœ¬ä¸å½“å‰ç¬¬ i
          ä¸ªå­åºåˆ—æ ·æœ¬ç›¸é‚»ï¼ˆæ—¶åºä¸Šè¿ç»­ï¼‰ï¼Œå› æ­¤==å½“å‰å°æ‰¹é‡æ•°æ®æœ€åä¸€ä¸ªæ ·æœ¬çš„éšçŠ¶æ€ï¼Œå°†ç”¨äºåˆå§‹åŒ–ä¸‹ä¸€ä¸ªå°æ‰¹é‡æ•°æ®ç¬¬ä¸€ä¸ªæ ·æœ¬çš„éšçŠ¶æ€==ï¼ˆï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼‰
          ```python
          # X:  tensor([[ 5,  6,  7,  8,  9],  
          #         [19, 20, 21, 22, 23]]) 
          # Y: tensor([[ 6,  7,  8,  9, 10],  
          #         [20, 21, 22, 23, 24]])  
          # X:  tensor([[10, 11, 12, 13, 14],  
          #         [24, 25, 26, 27, 28]])
          # Y: tensor([[11, 12, 13, 14, 15],  
          #         [25, 26, 27, 28, 29]])
          ```
            - è¿™æ ·ï¼Œå­˜å‚¨åœ¨éšçŠ¶æ€ä¸­çš„åºåˆ—çš„å†å²ä¿¡æ¯å¯ä»¥åœ¨ä¸€ä¸ªè¿­ä»£å‘¨æœŸå†…æµç»ç›¸é‚»çš„å­åºåˆ—
        - ç„¶è€Œï¼Œåœ¨ä»»ä½•ä¸€ç‚¹éšçŠ¶æ€çš„è®¡ç®—ï¼Œéƒ½ä¾èµ–äºåŒä¸€è¿­ä»£å‘¨æœŸä¸­å‰é¢æ‰€æœ‰çš„å°æ‰¹é‡æ•°æ®ï¼Œ è¿™ä½¿å¾—æ¢¯åº¦è®¡ç®—å˜å¾—å¤æ‚
            - ä¸ºäº†é™ä½è®¡ç®—é‡ï¼Œåœ¨å¤„ç†ä»»ä½•ä¸€ä¸ªå°æ‰¹é‡æ•°æ®ä¹‹å‰ï¼Œ å…ˆåˆ†ç¦»æ¢¯åº¦ï¼Œä½¿å¾—éšçŠ¶æ€çš„æ¢¯åº¦è®¡ç®—æ€»æ˜¯é™åˆ¶åœ¨ä¸€ä¸ªå°æ‰¹é‡æ•°æ®çš„æ—¶é—´æ­¥å†…
    - å½“ä½¿ç”¨éšæœºæŠ½æ ·æ—¶ï¼Œå› ä¸ºæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯åœ¨ä¸€ä¸ªéšæœºä½ç½®æŠ½æ ·çš„ï¼Œå› æ­¤==éœ€è¦ä¸ºæ¯ä¸ªè¿­ä»£å‘¨æœŸé‡æ–°åˆå§‹åŒ–éšçŠ¶æ€==
        - ä¸[3.6èŠ‚](https://zh-v2.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html#sec-softmax-scratch)ä¸­çš„
          `train_epoch_ch3`å‡½æ•°ç›¸åŒï¼Œ`updater`æ˜¯æ›´æ–°æ¨¡å‹å‚æ•°çš„å¸¸ç”¨å‡½æ•°ã€‚ å®ƒæ—¢å¯ä»¥æ˜¯ä»å¤´å¼€å§‹å®ç°çš„`d2l.sgd`å‡½æ•°ï¼Œ
          ä¹Ÿå¯ä»¥æ˜¯æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å†…ç½®çš„ä¼˜åŒ–å‡½æ•°

```python
def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):
    """  
    è®­ç»ƒæ¨¡å‹ä¸€ä¸ªè¿­ä»£å‘¨æœŸ  
    :param net: å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹  
    :param train_iter: è®­ç»ƒæ•°æ®é›†  
    :param loss: æŸå¤±å‡½æ•°  
    :param updater: è‡ªå®šä¹‰çš„æ›´æ–°å‡½æ•°æˆ– PyTorch å†…ç½®çš„ä¼˜åŒ–å™¨  
    :param use_random_iter: æ˜¯å¦ä½¿ç”¨éšæœºè¿­ä»£å™¨  
    """
    state, timer = None, d2l.Timer()
    metric = d2l.Accumulator(2)  # åˆå§‹åŒ–åº¦é‡æŒ‡æ ‡çš„ç´¯åŠ å™¨ï¼Œç”¨äºè®¡ç®—æŸå¤±å’Œæ ·æœ¬æ•°é‡  
    for X, Y in train_iter:
        if state is None or use_random_iter:  # å¦‚æœéšè—çŠ¶æ€ä¸ºç©ºæˆ–ä½¿ç”¨éšæœºè¿­ä»£å™¨  
            state = net.begin_state(batch_size=X.shape[0], device=device)
        else:  # ä½¿ç”¨ä¸Šä¸€æ—¶é—´æ­¥çš„éšè—çŠ¶æ€  
            if isinstance(net, nn.Module) and not isinstance(state, tuple):  # å¦‚æœ net æ˜¯ nn.Module çš„å®ä¾‹ä¸” state ä¸æ˜¯ tuple
                state.detach_()  # å¯¹äº nn.Module çš„å®ä¾‹ï¼Œåˆ†ç¦»éšè—çŠ¶æ€çš„è®¡ç®—å›¾ï¼Œç”¨äºå‡å°‘å†…å­˜å ç”¨å’ŒåŠ é€Ÿè®¡ç®—  
            else:  # å¦‚æœ net æ˜¯è‡ªå®šä¹‰çš„æ¨¡å‹æˆ– state æ˜¯ tuple
                for s in state:
                    s.detach_()  # å¯¹äºè‡ªå®šä¹‰çš„æ¨¡å‹æˆ– state æ˜¯ tupleï¼Œåˆ†ç¦»éšè—çŠ¶æ€çš„è®¡ç®—å›¾  
    y = Y.T.reshape(-1)
    X, y = X.to(device), y.to(device)
    y_hat, state = net(X, state)  # å‰å‘ä¼ æ’­è®¡ç®—é¢„æµ‹å€¼å’Œæ–°çš„éšè—çŠ¶æ€  
    l = loss(y_hat, y.long()).mean()  # è®¡ç®—æŸå¤±  
    if isinstance(updater, torch.optim.Optimizer):  # å¦‚æœ updater æ˜¯ torch å†…ç½®çš„ä¼˜åŒ–å™¨  
        updater.zero_grad()  # æ¢¯åº¦æ¸…é›¶  
        l.backward()  # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦  
        grad_clipping(net, 1)  # è£å‰ªæ¢¯åº¦  
        updater.step()  # ä½¿ç”¨ä¼˜åŒ–å™¨æ›´æ–°å‚æ•°  
    else:
        l.backward()  # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦  
        grad_clipping(net, 1)  # è£å‰ªæ¢¯åº¦  
        updater(batch_size=1)  # æ›´æ–°å‚æ•°  
    metric.add(l * y.numel(), y.numel())  # ç´¯åŠ æŸå¤±å’Œæ ·æœ¬æ•°é‡  


return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()
```

- ä»æŸå¤±å‡½æ•°å¯ä»¥çœ‹å‡ºï¼Œä¹‹æ‰€ä»¥å¯¹è¾“å‡ºè¿›è¡Œ `torch.cat(outputs, dim=0)`ï¼Œæ˜¯å› ä¸º==è™½ç„¶è¿™æ˜¯ä¸ªè¯­è¨€æ¨¡å‹ï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªæ ‡å‡†çš„å¤šåˆ†ç±»é—®é¢˜==
    - å¤šåˆ†ç±»ï¼Œå¯¹ä¸€ä¸ªæ‰¹é‡çš„è¾“å…¥åšé¢„æµ‹
    - è¯­è¨€æ¨¡å‹ï¼Œå¯¹ä¸€ä¸ªæ‰¹é‡ä¸åŒæ—¶é—´æ­¥çš„è¾“å…¥åšé¢„æµ‹
- è®­ç»ƒå‡½æ•°ï¼ŒåŒæ—¶æ»¡è¶³ä»é›¶å¼€å§‹å®ç°æˆ–æ˜¯ç®€æ´å®ç°

```python
def train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):
    """  
    è®­ç»ƒæ¨¡å‹  
    :param net: å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹  
    :param train_iter: è®­ç»ƒæ•°æ®é›†  
    :param vocab: è¯æ±‡è¡¨  
    :param use_random_iter: æ˜¯å¦ä½¿ç”¨éšæœºè¿­ä»£å™¨  
    """
    loss = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±å‡½æ•°  
    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity', legend=['train'], xlim=[10, num_epochs])
    if isinstance(net, nn.Module):  # å¦‚æœ net æ˜¯ nn.Module çš„å®ä¾‹  
        updater = torch.optim.SGD(net.parameters(), lr)  # ä½¿ç”¨ SGD ä¼˜åŒ–å™¨  
    else:  # å¦åˆ™ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„æ¢¯åº¦ä¸‹é™å‡½æ•°è¿›è¡Œå‚æ•°æ›´æ–°  
        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)
    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)  # å®šä¹‰é¢„æµ‹å‡½æ•°  
    for epoch in range(num_epochs):
        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)
        if (epoch + 1) % 10 == 0:
            print(predict('time traveller'))
            animator.add(epoch + 1, [ppl])
    print(f'å›°æƒ‘åº¦ {ppl:.1f}, {speed:.1f} æ ‡è®°/ç§’ {str(device)}')
    print(predict('time traveller'))
    print(predict('traveller'))
```

- ä½¿ç”¨é¡ºåºåˆ†åŒºè¿›è¡Œé¢„æµ‹

```python
num_epochs, lr = 500, 1
train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())  # å¾ªåºåˆ†åŒº  
# å›°æƒ‘åº¦ 1.0, 45872.6 æ ‡è®°/ç§’ cuda:0
# time travelleryou can show black is white by argument said filby  
# travelleryou can show black is white by argument said filby
```

- ![[00 Attachments/Pasted image 20240806221957.png|400]]
    - å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„å›°æƒ‘éƒ½ä¸º 1ï¼Œè¯´æ˜å·²ç»å®Œç¾çš„å°†æ–‡æœ¬è¿›è¡Œäº†è®°å¿†
        - å› ä¸ºè¿™å°±æ˜¯ä¸€æœ¬ä¹¦ï¼Œè¿­ä»£ 500 ä¸ªå‘¨æœŸï¼Œé‚£ä¹ˆå¾ˆå®¹æ˜“å°†è¿™æœ¬ä¹¦ç»™èƒŒä¸‹æ¥äº†
    - è§‚å¯Ÿé¢„æµ‹çš„ç»“æœï¼Œå‘ç°ä»å•è¯ã€è¯ç»„ã€å¥å­çš„è§’åº¦æ¥çœ‹ï¼Œæ˜¯æ²¡æœ‰é—®é¢˜çš„ï¼Œä½†æ˜¯åˆ°å¥å­çš„ç»„åˆï¼Œå¯ä»¥çœ‹åˆ°å…³è”æ€§ä¸å¤§
        - è¿™ä¹Ÿæ˜¯è¯­è¨€æ¨¡å‹æœ€å¸¸è§çš„é—®é¢˜ï¼Œ==ç¦ä¸èµ·ç»†çœ‹==
        - åŸæ–‡ï¼š
          ```
          'What reason?' said the Time Traveller.  
            
          'You can show black is white by argument,' said Filby, 'but you will  
          never convince me.'
          ```
- ä½¿ç”¨éšæœºæŠ½æ ·åˆ†åŒºè¿›è¡Œé¢„æµ‹

```python
train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(), use_random_iter=True)  # éšæœºæŠ½æ ·åˆ†åŒº  
# å›°æƒ‘åº¦ 1.5, 45742.9 æ ‡è®°/ç§’ cuda:0
# time travellerit s against reason said the medical man there are  
# travellerit s against reason said the medical man there are
```

- ![[00 Attachments/Pasted image 20240806223057.png|400]]
    - å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„å›°æƒ‘åº¦æ²¡æœ‰ä¸Šä¸€ä¸ªå¥½ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºè¿™é‡Œå¼•å…¥äº†éšæœºæ€§ï¼Œè®­ç»ƒèµ·æ¥å°±æ¯”è¾ƒéš¾ï¼Œæ²¡æœ‰å¾ˆå¥½å¾—å°†æ•´æœ¬ä¹¦è®°ä¸‹æ¥
    - åŸæ–‡
      ```
      'It's against reason,' said Filby.  
        
      'What reason?' said the Time Traveller.
      ```
- æ€»çš„æ¥è¯´ï¼Œå°±å•ä¸ªè¯æ¥è¯´ï¼Œæ•ˆæœè¿˜æ˜¯ä¸å·®çš„

### ç®€æ´å®ç°

#### å®šä¹‰æ¨¡å‹

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

num_hiddens = 256  # 256 ä¸ªéšè—å•å…ƒ
rnn_layer = nn.RNN(len(vocab), num_hiddens)  # éšå±‚ï¼Œè¿”å›ä¸‹ä¸€å±‚çš„è¾“å‡ºå’Œæ–°éšæ€

state = torch.zeros((1, batch_size, num_hiddens))
print(state.shape)
# torch.Size([1, 32, 256])
```

- é€šè¿‡ä¸€ä¸ªéšçŠ¶æ€å’Œä¸€ä¸ªè¾“å…¥ï¼Œå°±å¯ä»¥ç”¨æ›´æ–°åçš„éšçŠ¶æ€è®¡ç®—è¾“å‡º
- rnn_layer çš„è¾“å‡º Y ä¸æ¶‰åŠè¾“å‡ºå±‚çš„è®¡ç®—
    - å®ƒæ˜¯æŒ‡æ¯ä¸ªæ—¶é—´æ­¥çš„éšçŠ¶æ€ï¼Œè¿™äº›éšçŠ¶æ€å¯ä»¥ç”¨ä½œåç»­è¾“å‡ºå±‚çš„è¾“å…¥

```python
X = torch.rand(size=(num_steps, batch_size, len(vocab)))
Y, state_new = rnn_layer(X, state)
print(Y.shape, state_new.shape)  # Y æ—¶é—´ï¼Œæ‰¹é‡ï¼Œè¾“å‡º
# (torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))
```

- å®šä¹‰ä¸€ä¸ª RNNModel ç±»
    - å› ä¸º rnn_layer åªåŒ…å«éšè—çš„å¾ªç¯å±‚ï¼Œæ‰€ä»¥è¿˜éœ€è¦åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„è¾“å‡ºå±‚

```python
class RNNModel(nn.Module):
    """å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹"""

    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.num_hiddens = self.rnn.hidden_size
        # å¦‚æœRNNæ˜¯åŒå‘çš„ï¼Œnum_directionsåº”è¯¥æ˜¯2ï¼Œå¦åˆ™åº”è¯¥æ˜¯1
        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

    def forward(self, inputs, state):
        X = F.one_hot(inputs.T.long(), self.vocab_size)  # å°†æ¯ä¸€ä¸ªè¯è½¬æ¢æˆç‹¬çƒ­ç¼–ç 
        X = X.to(torch.float32)
        Y, state = self.rnn(X, state)  # éšè—å±‚
        # å…¨è¿æ¥å±‚é¦–å…ˆå°†Yçš„å½¢çŠ¶æ”¹ä¸º(æ—¶é—´æ­¥æ•°*æ‰¹é‡å¤§å°,éšè—å•å…ƒæ•°)
        # å®ƒçš„è¾“å‡ºå½¢çŠ¶æ˜¯(æ—¶é—´æ­¥æ•°*æ‰¹é‡å¤§å°,è¯è¡¨å¤§å°)ã€‚
        output = self.linear(Y.reshape((-1, Y.shape[-1])))
        return output, state

    def begin_state(self, device, batch_size=1):
        """
        çŠ¶æ€åˆå§‹åŒ–
        """

    if not isinstance(self.rnn, nn.LSTM):
        # nn.GRUä»¥å¼ é‡ä½œä¸ºéšçŠ¶æ€
        return torch.zeros((self.num_directions * self.rnn.num_layers,
                            batch_size, self.num_hiddens),
                           device=device)
    else:
        # nn.LSTMä»¥å…ƒç»„ä½œä¸ºéšçŠ¶æ€
        return (torch.zeros((self.num_directions * self.rnn.num_layers,
                             batch_size, self.num_hiddens),
                            device=device),
                torch.zeros((self.num_directions * self.rnn.num_layers,
                             batch_size, self.num_hiddens),
                            device=device))
```

#### é¢„æµ‹ä¸è®­ç»ƒ

- è®­ç»ƒä¹‹å‰çš„é¢„æµ‹

```python
device = d2l.try_gpu()
net = RNNModel(rnn_layer, vocab_size=len(vocab))
net = net.to(device)
d2l.predict_ch8('time traveller', 10, net, vocab, device)
# 'time travellerbbabbkabyg'
```

- å¯ä»¥çœ‹åˆ°ç”±äºæ²¡æœ‰è®­ç»ƒï¼Œé¢„æµ‹çš„æ•ˆæœå¾ˆä¸å¥½

```python
num_epochs, lr = 500, 1
d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)

```

- 
