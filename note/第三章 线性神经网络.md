```toc
```

# 3.1ã€çº¿æ€§ç¥ç»ç½‘ç»œ

## 3.1.1 çº¿æ€§å›å½’

- å›å½’ï¼ˆregressionï¼‰æ˜¯èƒ½ä¸ºä¸€ä¸ªæˆ–å¤šä¸ªè‡ªå˜é‡ä¸å› å˜é‡ä¹‹é—´å…³ç³»å»ºæ¨¡çš„ä¸€ç±»æ–¹æ³•
- å½“æˆ‘ä»¬æƒ³é¢„æµ‹ä¸€ä¸ªæ•°å€¼æ—¶ï¼Œå°±ä¼šæ¶‰åŠåˆ°å›å½’é—®é¢˜ã€‚ä½†ä¸æ˜¯æ‰€æœ‰çš„é¢„æµ‹éƒ½æ˜¯å›å½’é—®é¢˜

### 3.1.1.1 ç®€åŒ–æ¨¡å‹

- ä¸¾ä¾‹![[00 Attachments/Pasted image 20240510151211.png|500]]
- çº¿æ€§å›å½’æ¨¡å‹![[00 Attachments/Pasted image 20240510151446.png|500]]

### 3.1.1.2 ç¥ç»ç½‘ç»œ

- çº¿æ€§æ¨¡å‹å¯è§†ä½œå•å±‚ç¥ç»ç½‘ç»œ![[00 Attachments/Pasted image 20240510151706.png|500]]
    - è¾“å…¥çš„ç»´åº¦ä¸º dï¼Œè¾“å‡ºçš„ç»´åº¦ä¸º 1ï¼Œæ¯ä¸€ä¸ªç®­å¤´ä»£è¡¨æƒé‡
    - å¸¦æƒé‡çš„å±‚åªæœ‰ä¸€å±‚ï¼Œæ‰€ä»¥æ˜¯å•å±‚ç¥ç»ç½‘ç»œã€‚ï¼ˆè¾“å‡ºå±‚ä¸ç®—ï¼Œå› ä¸ºä¸å¸¦æƒé‡ï¼‰
- å¯¹äºçº¿æ€§å›å½’ï¼Œæ¯ä¸ªè¾“å…¥éƒ½ä¸æ¯ä¸ªè¾“å‡ºï¼ˆåœ¨æœ¬ä¾‹ä¸­åªæœ‰ä¸€ä¸ªè¾“å‡ºï¼‰ç›¸è¿ï¼Œ æˆ‘ä»¬å°†è¿™ç§å˜æ¢ï¼ˆä¸Šå›¾ä¸­çš„è¾“å‡ºå±‚ï¼‰ ç§°ä¸º==å…¨è¿æ¥å±‚==ï¼ˆfully-connected
  layerï¼‰æˆ–ç§°ä¸º_ç¨ å¯†å±‚_ï¼ˆdense layerï¼‰
- ![[00 Attachments/Pasted image 20240510152256.png|500]]
    - éšç€ä¸æ–­å‘å±•ï¼Œç¥ç»ç½‘ç»œå·²ç»è¶…å‡ºäº†ç¥ç»ç§‘å­¦çš„èŒƒç•´

### 3.1.1.3 æ±‚æœ€ä¼˜è§£

- ![[00 Attachments/Pasted image 20240510154956.png|500]]
    - æŸå¤±å‡½æ•°
        - è€ƒè™‘å¦‚ä½•ç”¨æ¨¡å‹æ‹Ÿåˆï¼ˆfitï¼‰æ•°æ®ä¹‹å‰ï¼Œéœ€è¦ç¡®å®šä¸€ä¸ªæ‹Ÿåˆç¨‹åº¦çš„åº¦é‡
        - ==æŸå¤±å‡½æ•°==ï¼ˆloss functionï¼‰èƒ½å¤Ÿé‡åŒ–ç›®æ ‡çš„å®é™…å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„å·®è·
        - å¹³æ–¹è¯¯å·®å‡½æ•°
            - $$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2$$
            - äºŒåˆ†ä¹‹ä¸€æ˜¯ä¸ºäº†æ±‚å¯¼åæ¶ˆå»ç³»æ•°
            - ä¸¾ä¾‹![[00 Attachments/Pasted image 20240510153706.png|300]]
                - ç”±äºå¹³æ–¹è¯¯å·®å‡½æ•°ä¸­çš„äºŒæ¬¡æ–¹é¡¹ï¼Œ ä¼°è®¡å€¼ $\hat ğ‘¦^{(ğ‘–)}$ å’Œè§‚æµ‹å€¼ $ğ‘¦^{(ğ‘–)}$
                  ä¹‹é—´è¾ƒå¤§çš„å·®å¼‚å°†å¯¼è‡´æ›´å¤§çš„æŸå¤±$$L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2$$
                - è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå¸Œæœ›æ‰¾åˆ°ä¸€ç»„å‚æ•° $(ğ‘¤^âˆ—,ğ‘^âˆ—)$ï¼Œ
                  è¿™ç»„å‚æ•°èƒ½æœ€å°åŒ–åœ¨æ‰€æœ‰è®­ç»ƒæ ·æœ¬ä¸Šçš„æ€»æŸå¤±ï¼ˆæœ€ä¼˜è§£ï¼‰$$\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\ L(\mathbf{w}, b)$$
- ![[00 Attachments/Pasted image 20240510154852.png|500]]
    - X ä¸ºè¾“å…¥ï¼Œy ä¸ºå®é™…æ•°æ®ã€‚ï¼ˆä¸€ä¸€å¯¹åº”ï¼‰
- æ±‚æœ€ä¼˜è§£![[00 Attachments/Pasted image 20240510155800.png|500]]
    - é™¤ n æ˜¯ä¸ºäº†å½’ä¸€åŒ–
- è§£æè§£![[00 Attachments/Pasted image 20240510160332.png|500]]
    - ![[00 Attachments/a83f4c3e256df61aaa0debe916393c6.jpg|200]]
    - å°†åç½® ğ‘ åˆå¹¶åˆ°å‚æ•° ğ‘¤
      ä¸­$$\mathbf{X} = [\mathbf x_1,\mathbf x_2,...,\mathbf x_n,\mathbf 1]^Tï¼Œ\mathbf{w} = [w_1,w_2,...,w_n,b]^T$$
        - $$\mathbf {\hat y} = \mathbf X \mathbf w=
          \begin{bmatrix}  
          x_{00} & \cdots & x_{0m} & 1\\  
          \vdots & \ddots & \vdots & 1\\  
          x_{n0} & \cdots & x_{nm} & 1
          \end{bmatrix}
          \begin{bmatrix}  
          w_{0} \\  
          \vdots \\  
          w_{m} \\
          b
          \end{bmatrix} $$
        - å…±æœ‰ n ç»„æ•°æ®ï¼Œæ¯ç»„æ•°æ®æœ‰ m ä¸ªå½±å“å› ç´ ï¼ˆè¾“å…¥ï¼‰
    - çº¿æ€§å›å½’æ˜¯ä¸€ä¸ªå¾ˆç®€å•çš„ä¼˜åŒ–é—®é¢˜ï¼Œä»–çš„è§£å¯ä»¥ç”¨ä¸€ä¸ªå…¬å¼ç®€å•åœ°è¡¨è¾¾å‡ºæ¥

### æ€»ç»“

- çº¿æ€§å›å½’æ˜¯å¯¹ n ç»´è¾“å…¥çš„åŠ æƒï¼Œå¤–åŠ åå·®
- ä½¿ç”¨å¹³æ–¹æŸå¤±æ¥è¡¡é‡é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å·®å¼‚
- çº¿æ€§å›å½’æœ‰è§£æ
- çº¿æ€§å›å½’å¯ä»¥çœ‹åšæ˜¯å•å±‚ç¥ç»ç½‘ç»œ

## 3.1.2 åŸºç¡€ä¼˜åŒ–ç®—æ³•

### 3.1.2.1 æ¢¯åº¦ä¸‹é™

- æ¢¯åº¦ä¸‹é™ï¼ˆgradient descentï¼‰ï¼šé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°å…³äºå‚æ•°çš„æ¢¯åº¦ï¼ˆå¯¼æ•°ï¼‰ï¼Œæ‰¾åˆ°æŸå¤±å‡½æ•°å‡å°çš„æ–¹å‘ï¼Œå¹¶æ²¿ç€è¯¥æ–¹å‘æ›´æ–°å‚æ•°ä»¥å‡å°æŸå¤±å‡½æ•°
- ä¸¾ä¾‹![[00 Attachments/Pasted image 20240510201555.png|500]]
    - ==è¶…å‚æ•°==ï¼ˆhyperparameterï¼‰ï¼šå¯ä»¥è°ƒæ•´ä½†ä¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°çš„å‚æ•°
    - è°ƒå‚ï¼ˆhyperparameter tuningï¼‰æ˜¯é€‰æ‹©è¶…å‚æ•°çš„è¿‡ç¨‹
    - è¶…å‚æ•°é€šå¸¸æ˜¯æˆ‘ä»¬æ ¹æ®è®­ç»ƒè¿­ä»£ç»“æœæ¥è°ƒæ•´çš„ï¼Œ è€Œè®­ç»ƒè¿­ä»£ç»“æœæ˜¯åœ¨ç‹¬ç«‹çš„éªŒè¯æ•°æ®é›†ï¼ˆvalidation datasetï¼‰ä¸Šè¯„ä¼°å¾—åˆ°çš„
    - ![[00 Attachments/Pasted image 20240510204637.png|500]]
- å­¦ä¹ ç‡çš„é€‰æ‹©![[00 Attachments/Pasted image 20240510205154.png|500]]
    - ä¸èƒ½å¤ªå°ï¼Œè´¹æ—¶ã€‚ä¸”æ¢¯åº¦è®¡ç®—æ˜¯æ¨¡å‹ä¸­æœ€è€—èµ„æºçš„éƒ¨åˆ†ï¼ˆæ¯æ¬¡è®¡ç®—æ¢¯åº¦ï¼Œéœ€è¦å¯¹æŸå¤±å‡½æ•°æ±‚å¯¼ï¼Œè€ŒæŸå¤±å‡½æ•°æ˜¯å¯¹æ‰€æœ‰æ ·æœ¬çš„ä¸€ä¸ªå¹³å‡æŸå¤±ï¼Œè¿™æ„å‘³ç€æ±‚ä¸€æ¬¡æ¢¯åº¦éœ€è¦å°†æ ·æœ¬é‡æ–°è®¡ç®—ä¸€éï¼‰
    - ä¸èƒ½å¤ªå¤§ã€‚è¶…è°ƒæŒ¯è¡

### 3.1.2.2 å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™

- ![[00 Attachments/Pasted image 20240510205521.png|500]]
    - $$\begin{split}\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}\end{split}$$
- æ‰¹é‡ï¼ˆbatch sizeï¼‰ï¼šéšæœºæ ·æœ¬ä¸ªæ•°
- æ‰¹é‡çš„é€‰æ‹©
    - ä¸èƒ½å¤ªå°ï¼šæ¯æ¬¡è®¡ç®—é‡å¤ªå°ï¼Œä¸åˆé€‚å¹¶è¡Œæ¥æœ€å¤§åˆ©ç”¨è®¡ç®—èµ„æº
    - ä¸èƒ½å¤ªå¤§ï¼šå†…å­˜æ¶ˆè€—å¢åŠ ï¼Œæµªè´¹è®¡ç®—ã€‚ä¾‹å¦‚å¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½æ˜¯ç›¸åŒçš„ï¼ˆå­˜åœ¨å¤§é‡è¿‘ä¼¼çš„æ ·æœ¬ï¼‰

### 3.1.2.3 æ€»ç»“

- æ¢¯åº¦ä¸‹é™é€šè¿‡ä¸æ–­æ²¿ç€åæ¢¯åº¦æ–¹å‘æ›´æ–°å‚æ•°æ±‚è§£
- å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™æ˜¯æ·±åº¦å­¦ä¹ é»˜è®¤çš„æ±‚è§£ç®—æ³•
- ä¸¤ä¸ªé‡è¦çš„è¶…å‚æ•°æ˜¯æ‰¹é‡å¤§å°å’Œå­¦ä¹ ç‡

## 3.1.3 çº¿æ€§å›å½’ä»é›¶å¼€å§‹

- ä¸ä½¿ç”¨ä»»ä½•æ·±åº¦å­¦ä¹ æ¡†æ¶æä¾›çš„è®¡ç®—ï¼Œ ä»é›¶å¼€å§‹å®ç°æ•´ä¸ªæ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°æ®æµæ°´çº¿ã€æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œå°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨

### 3.1.3.1 ç”Ÿæˆæ•°æ®é›†

- æ ¹æ®å¸¦æœ‰å™ªå£°çš„çº¿æ€§æ¨¡å‹æ„é€ ä¸€ä¸ªäººé€ æ•°æ®é›†ã€‚ä½¿ç”¨çº¿æ€§æ¨¡å‹å‚æ•° $ğ‘¤=[2,âˆ’3.4]âŠ¤$ã€$ğ‘=4.2$ å’Œå™ªå£°é¡¹ Ïµ
  ç”Ÿæˆæ•°æ®é›†åŠå…¶æ ‡ç­¾ï¼š$$ğ‘¦=ğ‘‹ğ‘¤+ğ‘+ğœ–$$

```python
def synthetic_data(w, b, num_examples):  # ç”Ÿæˆæ•°æ®é›†  
    """ç”Ÿæˆy = Xw + b + å™ªå£°"""
    X = torch.normal(0, 1, (num_examples, len(w)))  # éšæœºç”Ÿæˆ num_examples è¡Œ len(w) åˆ—çš„çŸ©é˜µ  
    y = torch.matmul(X, w) + b  # è®¡ç®— y = Xw + b    y += torch.normal(0, 0.01, y.shape)  # åŠ ä¸Šå™ªå£°  
    return X, y.reshape((-1, 1))  # è¿”å› X å’Œ è½¬ç½®åçš„ y -1è‡ªåŠ¨å°†yçš„å½¢çŠ¶è½¬ä¸º(num_examples,1)  


true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)  # featuresä¸­çš„æ¯ä¸€è¡Œéƒ½åŒ…å«ä¸€ä¸ªäºŒç»´æ•°æ®æ ·æœ¬ï¼Œ labelsä¸­çš„æ¯ä¸€è¡Œéƒ½åŒ…å«ä¸€ç»´æ ‡ç­¾å€¼ï¼ˆä¸€ä¸ªæ ‡é‡ï¼‰
```

- `features`ä¸­çš„æ¯ä¸€è¡Œéƒ½åŒ…å«ä¸€ä¸ªäºŒç»´æ•°æ®æ ·æœ¬ï¼Œ`labels`ä¸­çš„æ¯ä¸€è¡Œéƒ½åŒ…å«ä¸€ç»´æ ‡ç­¾å€¼ï¼ˆä¸€ä¸ªæ ‡é‡ï¼‰

```python
print('features:', features[0], '\nlabel:', labels[0])  # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç‰¹å¾å’Œæ ‡ç­¾  
# features: tensor([-1.0186, -0.1225])  
# label: tensor([2.5836])
```

- é€šè¿‡ç”Ÿæˆç¬¬äºŒä¸ªç‰¹å¾`features[:,Â 1]`å’Œ`labels`çš„æ•£ç‚¹å›¾ï¼Œ å¯ä»¥ç›´è§‚è§‚å¯Ÿåˆ°ä¸¤è€…ä¹‹é—´çš„çº¿æ€§å…³ç³»

```python
d2l.set_figsize()
d2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1)  # åªæœ‰ detach åæ‰èƒ½è½¬åˆ° numpy é‡Œé¢å»
```

- `set_figsize()`ï¼šSet the figure size for matplotlib
- ![[00 Attachments/Pasted image 20240510221414.png|500]]

### 3.1.2.2 è¯»å–æ•°æ®é›†

- `data_iter()`æ¥æ”¶æ‰¹é‡å¤§å°ã€ç‰¹å¾çŸ©é˜µå’Œæ ‡ç­¾å‘é‡ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆå¤§å°ä¸º batch_size çš„å°æ‰¹é‡

```python
def data_iter(batch_size, features, labels):  # ç”Ÿæˆå°æ‰¹é‡æ•°æ®é›†  
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)  # å¯¹æ ·æœ¬ç´¢å¼•è¿›è¡Œéšæœºé‡æ’åºï¼Œä»¥ä¾¿ä»¥æ¯æ¬¡è¿­ä»£å–å‡ºä¸åŒçš„æ•°æ®å­é›†  
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])  # æœ€åä¸€æ¬¡å¯èƒ½ä¸è¶³ä¸€ä¸ª batch_sizeï¼Œå› æ­¤ç”¨ min å‡½æ•°é™åˆ¶èŒƒå›´  
        yield features[batch_indices], labels[batch_indices]  # yield å…³é”®å­—ç”¨äºç”Ÿæˆè¿­ä»£å™¨ï¼Œè¿”å›ä¸€ä¸ªç”Ÿæˆå™¨å¯¹è±¡ï¼Œå¯ä»¥ç”¨ next() å‡½æ•°è·å–ä¸‹ä¸€ä¸ªå€¼
```

- `yield`å…³é”®å­—åœ¨ Python ä¸­ç”¨äºå®šä¹‰ç”Ÿæˆå™¨å‡½æ•°ï¼Œå®ƒå¯ä»¥æš‚åœå‡½æ•°çš„æ‰§è¡Œå¹¶è¿”å›ä¸€ä¸ªä¸­é—´å€¼ï¼Œç„¶ååœ¨éœ€è¦æ—¶ä»ä¸Šæ¬¡æš‚åœçš„åœ°æ–¹ç»§ç»­æ‰§è¡Œï¼Œè€Œä¸ä¼šä¸¢å¤±ä»»ä½•çŠ¶æ€ä¿¡æ¯
  ```python
  def my_generator():
      yield 1
      yield 2
      yield 3
  
  gen = my_generator()
  
  print(next(gen))  # è¾“å‡º 1
  print(next(gen))  # è¾“å‡º 2
  print(next(gen))  # è¾“å‡º 3
  ```
- å¦‚æœ`batch_indices`çš„å€¼æ˜¯`tensor([2, 5, 7, 9, 10])`ï¼Œé‚£ä¹ˆ`features[batch_indices]`å’Œ`labels[batch_indices]`
  å°†ä¼šåŒ…å«æ•°æ®é›†ä¸­ç´¢å¼•ä¸º2ã€5ã€7ã€9å’Œ10çš„ç‰¹å¾å’Œæ ‡ç­¾æ•°æ®
- åˆ©ç”¨GPUå¹¶è¡Œè¿ç®—çš„ä¼˜åŠ¿ï¼Œå¤„ç†åˆç†å¤§å°çš„â€œå°æ‰¹é‡â€ã€‚æ¯ä¸ªæ ·æœ¬éƒ½å¯ä»¥å¹¶è¡Œåœ°è¿›è¡Œæ¨¡å‹è®¡ç®—ï¼Œä¸”æ¯ä¸ªæ ·æœ¬æŸå¤±å‡½æ•°çš„æ¢¯åº¦ä¹Ÿå¯ä»¥è¢«å¹¶è¡Œè®¡ç®—ã€‚
  GPUå¯ä»¥åœ¨å¤„ç†å‡ ç™¾ä¸ªæ ·æœ¬æ—¶ï¼Œæ‰€èŠ±è´¹çš„æ—¶é—´ä¸æ¯”å¤„ç†ä¸€ä¸ªæ ·æœ¬æ—¶å¤šå¤ªå¤š
- ç›´è§‚æ„Ÿå—ä¸€ä¸‹å°æ‰¹é‡è¿ç®—ï¼šè¯»å–ç¬¬ä¸€ä¸ªå°æ‰¹é‡æ•°æ®æ ·æœ¬å¹¶æ‰“å°

```python
batch_size = 10
for X, y in data_iter(batch_size, features, labels):
    """  
    è¿è¡Œè¿­ä»£æ—¶ï¼Œä¼šè¿ç»­åœ°è·å¾—ä¸åŒçš„å°æ‰¹é‡ï¼Œç›´è‡³éå†å®Œæ•´ä¸ªæ•°æ®é›†  
    å¾ªç¯ä¸€æ¬¡ï¼Œè°ƒç”¨ä¸€æ¬¡data_iter()  
    æ¯æ¬¡è¿­ä»£è¿”å›ä¸€ä¸ªå°æ‰¹é‡çš„ç‰¹å¾å’Œæ ‡ç­¾  
    å†æ¬¡è°ƒç”¨æ—¶ï¼ˆç”±äºyieldå…³é”®å­—ï¼‰ä»ä¸Šæ¬¡çš„ä½ç½®ç»§ç»­  
    """
    print(X, '\n', y)
    break
# tensor([[-0.1774,  0.9301],  
#         [ 2.0726, -0.8996],  
#         [-0.7972, -0.3161],  
#         [ 0.0052, -1.3159],  
#         [-0.1677,  1.6213],  
#         [-0.7590,  1.0103],  
#         [-0.9327,  1.1352],  
#         [ 0.4481, -1.5346],  
#         [ 0.4171,  0.0842],  
#         [-0.5942, -1.3337]])  
#  tensor([[ 0.6886],  
#         [11.4139],  
#         [ 3.6750],  
#         [ 8.6796],  
#         [-1.6448],  
#         [-0.7698],  
#         [-1.5164],  
#         [10.3102],  
#         [ 4.7424],  
#         [ 7.5442]])
```

### 3.1.2.3 åˆå§‹åŒ–æ¨¡å‹å‚æ•°

- åœ¨å¼€å§‹ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ¨¡å‹å‚æ•°ä¹‹å‰ï¼Œ éœ€è¦å…ˆæœ‰ä¸€äº›å‚æ•°ã€‚ åœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œé€šè¿‡ä»å‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º0.01çš„æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·éšæœºæ•°æ¥åˆå§‹åŒ–æƒé‡ï¼Œ
  å¹¶å°†åç½®åˆå§‹åŒ–ä¸º0

```python
w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)  # éšæœºåˆå§‹åŒ–æƒé‡ requires_grad=True è¡¨ç¤ºéœ€è¦å¯¹ w è¿›è¡Œæ±‚å¯¼  
b = torch.zeros(1, requires_grad=True)  # åç½®åˆå§‹åŒ–ä¸º0
```

- åœ¨åˆå§‹åŒ–å‚æ•°ä¹‹åï¼Œæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯æ›´æ–°è¿™äº›å‚æ•°ï¼Œç›´åˆ°è¿™äº›å‚æ•°è¶³å¤Ÿæ‹Ÿåˆæˆ‘ä»¬çš„æ•°æ®ã€‚ æ¯æ¬¡æ›´æ–°éƒ½éœ€è¦è®¡ç®—æŸå¤±å‡½æ•°å…³äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚
  æœ‰äº†è¿™ä¸ªæ¢¯åº¦ï¼Œæˆ‘ä»¬å°±å¯ä»¥å‘å‡å°æŸå¤±çš„æ–¹å‘æ›´æ–°æ¯ä¸ªå‚æ•°ã€‚

### 3.1.2.4 å®šä¹‰æ¨¡å‹

- å°†æ¨¡å‹çš„è¾“å…¥å’Œå‚æ•°åŒæ¨¡å‹çš„è¾“å‡ºå…³è”èµ·æ¥$$\hat y = \mathbf{Xw} + b$$

```python
def linreg(X, w, b):  # çº¿æ€§å›å½’æ¨¡å‹  
    return torch.matmul(X, w) + b
```

### 3.1.2.5 å®šä¹‰æŸå¤±å‚æ•°

- ç”¨äºæ¢¯åº¦è®¡ç®—

```python
def squared_loss(y_hat, y):  # å‡æ–¹æŸå¤±å‡½æ•°  
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2  # å¯¹åº”å…ƒç´ çš„å¹³æ–¹ é™¤ä»¥ 2 æœªæ±‚å‡å€¼
```

### 3.1.2.6 å®šä¹‰ä¼˜åŒ–ç®—æ³•

- å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™
    - åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œä½¿ç”¨ä»æ•°æ®é›†ä¸­éšæœºæŠ½å–çš„ä¸€ä¸ªå°æ‰¹é‡ï¼Œç„¶åæ ¹æ®å‚æ•°è®¡ç®—æŸå¤±çš„æ¢¯åº¦
    - æ¥ä¸‹æ¥ï¼Œæœç€å‡å°‘æŸå¤±çš„æ–¹å‘æ›´æ–°æˆ‘ä»¬çš„å‚æ•°

```python
def sgd(params, lr, batch_size):  # å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•  
    with torch.no_grad():  # åœ¨ torch.no_grad() èŒƒå›´å†…ï¼Œæ¢¯åº¦ä¸ä¼šè¢«è‡ªåŠ¨è®¡ç®—å’Œç´¯åŠ  æ›´æ–°å‚æ•°æ—¶ä¸éœ€è¦æ¢¯åº¦è®¡ç®—  
        for param in params:
            param -= lr * param.grad / batch_size  # ç”±äºæŸå¤±å‡½æ•°æœªè®¡ç®—å‡å€¼ï¼Œæ‰€ä»¥è¿™é‡Œé™¤ä»¥ batch_size
            param.grad.zero_()  # æ¢¯åº¦æ¸…é›¶
```

### 3.1.2.7 è®­ç»ƒ

- åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œè¯»å–ä¸€å°æ‰¹é‡è®­ç»ƒæ ·æœ¬ï¼Œå¹¶é€šæ¨¡å‹æ¥è·å¾—ä¸€ç»„é¢„æµ‹ã€‚
- è®¡ç®—æŸå¤±
- åå‘ä¼ æ’­ï¼Œå­˜å‚¨æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
- è°ƒç”¨ä¼˜åŒ–ç®—æ³•`sgd`æ¥æ›´æ–°æ¨¡å‹å‚æ•°

```python
lr = 0.03  # å­¦ä¹ ç‡  
num_epochs = 3  # è¿­ä»£æ¬¡æ•°  
# æ–¹ä¾¿æ›¿æ¢  
net = linreg  # é€‰æ‹©æ¨¡å‹  
loss = squared_loss  # é€‰æ‹©æŸå¤±å‡½æ•°  

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)  # æ¨¡å‹é¢„æµ‹ è®¡ç®—æŸå¤±  
        # å› ä¸ºlå½¢çŠ¶æ˜¯(batch_size,1)ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ ‡é‡  
        # éœ€è¦è°ƒç”¨.sum()å‡½æ•°æ¥å¾—åˆ°ä¸€ä¸ªæ ‡é‡ï¼Œå¹¶ä»¥æ­¤è®¡ç®—å…³äº[w,b]çš„æ¢¯åº¦  
        l.sum().backward()  # æ±‚æ¢¯åº¦  
        sgd([w, b], lr, batch_size)  # ä½¿ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™è¿­ä»£æ¨¡å‹å‚æ•°  
    with torch.no_grad():  # åœ¨ torch.no_grad() èŒƒå›´å†…ï¼Œæ¢¯åº¦ä¸ä¼šè¢«è‡ªåŠ¨è®¡ç®—å’Œç´¯åŠ   
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')  # æ‰“å°è®­ç»ƒæŸå¤± mean()æ±‚å‡å€¼  
# epoch 1, loss 0.037162  
# epoch 2, loss 0.000134  
# epoch 3, loss 0.000051
```

- é€šè¿‡æ¯”è¾ƒçœŸå®å‚æ•°å’Œé€šè¿‡è®­ç»ƒå­¦åˆ°çš„å‚æ•°æ¥è¯„ä¼°è®­ç»ƒçš„æˆåŠŸç¨‹åº¦

```python
print(f'w: {true_w}, b: {true_b}')  # æ‰“å°çœŸå®å‚æ•°  
print(f'w: {w.reshape((1, -1)), b}')  # æ‰“å°è®­ç»ƒå¾—åˆ°çš„å‚æ•°  
print(f'wçš„ä¼°è®¡è¯¯å·®: {true_w - w.reshape(true_w.shape)}')
print(f'bçš„ä¼°è®¡è¯¯å·®: {true_b - b}')
# w: tensor([ 2.0000, -3.4000]), b: 4.2  
# w: (tensor([[ 1.9995, -3.3994]], grad_fn=<ViewBackward0>), tensor([4.1999], requires_grad=True))
```

- é€šå¸¸ä¸å¤ªå…³å¿ƒæ¢å¤çœŸæ­£çš„å‚æ•°ï¼Œè€Œæ›´å…³å¿ƒå¦‚ä½•é«˜åº¦å‡†ç¡®é¢„æµ‹å‚æ•°

## 3.1.4 çº¿æ€§å›å½’çš„ç®€æ´å®ç°

- ä½¿ç”¨æˆç†Ÿçš„å¼€æºæ¡†æ¶

### ç”Ÿæˆæ•°æ®é›†

```python
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l

# ç”Ÿæˆæ•°æ®é›†  
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)  # 1000ä¸ªæ ·æœ¬
```

### è¯»å–æ•°æ®é›†

```python
# è¯»å–æ•°æ®é›†  
def load_array(data_arrays, batch_size, is_train=True):  # å¸ƒå°”å€¼is_trainè¡¨ç¤ºæ˜¯å¦å¸Œæœ›æ•°æ®è¿­ä»£å™¨å¯¹è±¡åœ¨æ¯ä¸ªè¿­ä»£å‘¨æœŸå†…æ‰“ä¹±æ•°æ®  
    """æ„é€ ä¸€ä¸ªPyTorchæ•°æ®è¿­ä»£å™¨"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)


batch_size = 10
data_iter = load_array((features, labels), batch_size)  # è¿”å›çš„æ•°æ®çš„è¿­ä»£å™¨  
print(next(iter(data_iter)))  # iter(data_iter) æ˜¯ä¸€ä¸ªè¿­ä»£å™¨å¯¹è±¡ï¼Œnextæ˜¯å–è¿­ä»£å™¨é‡Œé¢çš„å…ƒç´   
# [tensor([[ 0.8482,  1.9892],  
#         [-0.5166, -1.1064],  
#         [ 2.2390,  0.2100],  
#         [ 0.5310,  1.0847],  
#         [-1.2754, -0.4711],  
#         [ 1.4253, -0.9882],  
#         [-2.3966,  0.4667],  
#         [ 0.6666, -2.1511],  
#         [-0.6097, -0.6963],  
#         [-0.3293,  0.1273]]),
# tensor([[-0.8530],  
#         [ 6.9264],  
#         [ 7.9642],  
#         [ 1.5826],  
#         [ 3.2568],  
#         [10.4177],  
#         [-2.1703],  
#         [12.8406],  
#         [ 5.3467],  
#         [ 3.1034]])]
```

### å®šä¹‰æ¨¡å‹

- å¦‚æœæ¨¡å‹å˜å¾—æ›´åŠ å¤æ‚ï¼Œä¸”å½“å‡ ä¹æ¯å¤©éƒ½éœ€è¦å®ç°æ¨¡å‹æ—¶ï¼Œè‡ªç„¶ä¼šæƒ³ç®€åŒ–è¿™ä¸ªè¿‡ç¨‹
- å¯¹äºæ ‡å‡†æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨æ¡†æ¶çš„é¢„å®šä¹‰å¥½çš„å±‚ã€‚è¿™ä½¿åªéœ€å…³æ³¨ä½¿ç”¨å“ªäº›å±‚æ¥æ„é€ æ¨¡å‹ï¼Œè€Œä¸å¿…å…³æ³¨å±‚çš„å®ç°ç»†èŠ‚
- é¦–å…ˆå®šä¹‰ä¸€ä¸ªæ¨¡å‹å˜é‡`net`ï¼Œå®ƒæ˜¯ä¸€ä¸ª`Sequential`ç±»çš„å®ä¾‹ã€‚`Sequential`ç±»å°†å¤šä¸ªå±‚ä¸²è”åœ¨ä¸€èµ·ã€‚ å½“ç»™å®šè¾“å…¥æ•°æ®æ—¶ï¼Œ`Sequential`
  å®ä¾‹å°†æ•°æ®ä¼ å…¥åˆ°ç¬¬ä¸€å±‚ï¼Œ ç„¶åå°†ç¬¬ä¸€å±‚çš„è¾“å‡ºä½œä¸ºç¬¬äºŒå±‚çš„è¾“å…¥ï¼Œä»¥æ­¤ç±»æ¨
- åœ¨PyTorchä¸­ï¼Œå…¨è¿æ¥å±‚åœ¨`Linear`ç±»ä¸­å®šä¹‰
- `Sequential`ä¸€ä¸ªæœ‰åºçš„å®¹å™¨ï¼Œç¥ç»ç½‘ç»œæ¨¡å—å°†æŒ‰ç…§åœ¨ä¼ å…¥æ„é€ å™¨çš„é¡ºåºä¾æ¬¡è¢«æ·»åŠ åˆ°è®¡ç®—å›¾ä¸­æ‰§è¡Œï¼Œ åŒæ—¶ä»¥ç¥ç»ç½‘ç»œæ¨¡å—ä¸ºå…ƒç´ çš„æœ‰åºå­—å…¸ä¹Ÿå¯ä»¥ä½œä¸ºä¼ å…¥å‚æ•°ã€‚

```python
# nnæ˜¯ç¥ç»ç½‘ç»œçš„ç¼©å†™  
from torch import nn

net = nn.Sequential(nn.Linear(2, 1))  # è¾“å…¥ç‰¹å¾ç»´åº¦ä¸º2ï¼Œè¾“å‡ºç»´åº¦ä¸º1
```

### åˆå§‹åŒ–æ¨¡å‹å‚æ•°

- åœ¨ä½¿ç”¨`net`ä¹‹å‰ï¼Œéœ€è¦åˆå§‹åŒ–æ¨¡å‹å‚æ•°ã€‚ å¦‚åœ¨çº¿æ€§å›å½’æ¨¡å‹ä¸­çš„æƒé‡å’Œåç½®
- æ­£å¦‚æˆ‘ä»¬åœ¨æ„é€ `nn.Linear`æ—¶æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºå°ºå¯¸ä¸€æ ·ï¼Œ èƒ½ç›´æ¥è®¿é—®å‚æ•°ä»¥è®¾å®šå®ƒä»¬çš„åˆå§‹å€¼ã€‚ é€šè¿‡`net[0]`é€‰æ‹©ç½‘ç»œä¸­çš„ç¬¬ä¸€ä¸ªå›¾å±‚ï¼Œ
  ç„¶åä½¿ç”¨`weight.data`å’Œ`bias.data`æ–¹æ³•è®¿é—®å‚æ•°ã€‚ è¿˜å¯ä»¥ä½¿ç”¨æ›¿æ¢æ–¹æ³•`normal_`å’Œ`fill_`æ¥é‡å†™å‚æ•°å€¼

```python
net[0].weight.data.normal_(0, 0.01)  # æƒé‡å‚æ•°åˆå§‹åŒ– 0å‡å€¼ï¼Œ0.01æ ‡å‡†å·®  
print(net[0].bias.data.fill_(0))  # åç½®å‚æ•°åˆå§‹åŒ–ä¸º0  
# tensor([0.])
```

### å®šä¹‰æŸå¤±å‡½æ•°

- è®¡ç®—å‡æ–¹è¯¯å·®ä½¿ç”¨çš„æ˜¯MSELossç±»ï¼Œä¹Ÿç§°ä¸ºå¹³æ–¹L2èŒƒæ•°
- é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒè¿”å›æ‰€æœ‰æ ·æœ¬æŸå¤±çš„å¹³å‡å€¼

```python
loss = nn.MSELoss()
```

### å®šä¹‰ä¼˜åŒ–ç®—æ³•

- PyTorchåœ¨`optim`æ¨¡å—ä¸­å®ç°äº†å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•çš„è®¸å¤šå˜ç§
- å½“æˆ‘ä»¬å®ä¾‹åŒ–ä¸€ä¸ª`SGD`å®ä¾‹æ—¶ï¼Œè¦æŒ‡å®šä¼˜åŒ–çš„å‚æ•° ï¼ˆå¯é€šè¿‡`net.parameters()`ä»æˆ‘ä»¬çš„æ¨¡å‹ä¸­è·å¾—ï¼‰ä»¥åŠä¼˜åŒ–ç®—æ³•æ‰€éœ€çš„è¶…å‚æ•°å­—å…¸
- å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™åªéœ€è¦è®¾ç½®`lr`å€¼ï¼Œè¿™é‡Œè®¾ç½®ä¸º0.03

```python
trainer = torch.optim.SGD(net.parameters(), lr=0.03)  # éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•
```

### è®­ç»ƒ

- åœ¨æ¯ä¸ªè¿­ä»£å‘¨æœŸé‡Œï¼Œæˆ‘ä»¬å°†å®Œæ•´éå†ä¸€æ¬¡æ•°æ®é›†ï¼ˆ`train_data`ï¼‰ï¼Œ ä¸åœåœ°ä»ä¸­è·å–ä¸€ä¸ªå°æ‰¹é‡çš„è¾“å…¥å’Œç›¸åº”çš„æ ‡ç­¾
- å¯¹äºæ¯ä¸€ä¸ªå°æ‰¹é‡ï¼Œä¼šè¿›è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
    - é€šè¿‡è°ƒç”¨`net(X)`ç”Ÿæˆé¢„æµ‹å¹¶è®¡ç®—æŸå¤±`loss`ï¼ˆå‰å‘ä¼ æ’­ï¼‰
    - é€šè¿‡è¿›è¡Œåå‘ä¼ æ’­æ¥è®¡ç®—æ¢¯åº¦
    - é€šè¿‡è°ƒç”¨ä¼˜åŒ–å™¨æ¥æ›´æ–°æ¨¡å‹å‚æ•°
- ä¸ºäº†æ›´å¥½çš„è¡¡é‡è®­ç»ƒæ•ˆæœï¼Œè®¡ç®—æ¯ä¸ªè¿­ä»£å‘¨æœŸåçš„æŸå¤±ï¼Œå¹¶æ‰“å°å®ƒæ¥ç›‘æ§è®­ç»ƒè¿‡ç¨‹

```python
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X), y)  # è®¡ç®—æŸå¤±  
        trainer.zero_grad()  # æ¢¯åº¦æ¸…é›¶  
        l.backward()  # åå‘ä¼ æ’­  
        trainer.step()  # æ›´æ–°å‚æ•°  
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
# epoch 1, loss 0.000208  
# epoch 2, loss 0.000100  
# epoch 3, loss 0.000101
```

- æ¯”è¾ƒç”Ÿæˆæ•°æ®é›†çš„çœŸå®å‚æ•°å’Œé€šè¿‡æœ‰é™æ•°æ®è®­ç»ƒè·å¾—çš„æ¨¡å‹å‚æ•°
- è¦è®¿é—®å‚æ•°ï¼Œé¦–å…ˆä»`net`è®¿é—®æ‰€éœ€çš„å±‚ï¼Œç„¶åè¯»å–è¯¥å±‚çš„æƒé‡å’Œåç½®

```python
w = net[0].weight.data
b = net[0].bias.data
print(f'w: {w.numpy()}, b: {b.numpy()}')  # numpy()æ–¹æ³•ç”¨äºè½¬æ¢ä¸ºnumpyæ•°ç»„  
# w: [[ 2.0004933 -3.399767 ]], b: [4.1998973]
```

# 3.2ã€Softmax å›å½’

- è™½ç„¶åå­—å¸¦æœ‰å›å½’ï¼Œä½†å…¶å®æ˜¯ä¸€ä¸ªåˆ†ç±»é—®é¢˜
- å›å½’ä¼°è®¡ä¸€ä¸ªè¿ç»­å€¼ï¼šæˆ¿ä»·
- åˆ†ç±»é¢„æµ‹ä¸€ä¸ªç¦»æ•£ç±»åˆ«ï¼šçŒ«ç‹—ï¼Ÿï¼ˆåˆ†ä¸¤ç±»ï¼‰
- é€šè¿‡å•ä¸ªä»¿å°„å˜æ¢å°†æˆ‘ä»¬çš„è¾“å…¥ç›´æ¥æ˜ å°„åˆ°è¾“å‡ºï¼Œç„¶åè¿›è¡Œsoftmaxæ“ä½œ

## 3.2.1 ä»å›å½’åˆ°å¤šç±»åˆ†ç±»

- ![[00 Attachments/Pasted image 20240514191639.png|500]]
    - $$\begin{split}\begin{aligned}
      o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\
      o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\
      o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.
      \end{aligned}\end{split}$$
    - $$\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$$
- æ­¤æ—¶çš„ o æœªè¿›è¡Œä»»ä½•è§„èŒƒåŒ–

### å‡æ–¹æŸå¤±

- ![[00 Attachments/Pasted image 20240514192936.png|500]]
    - ä¼˜åŒ–å‚æ•°ä»¥æœ€å¤§åŒ–è§‚æµ‹æ•°æ® $O_i$ çš„æ¦‚ç‡ï¼š
        - å³è¾“å…¥ç±»åˆ«ä¸º i åˆ™ï¼Œå¯¹åº”çš„ $O_i$ åº”ä¸ºæœ€å¤§
        - å¸Œæœ›æ¨¡å‹çš„è¾“å‡º $ğ‘¦^ğ‘—$ å¯ä»¥è§†ä¸ºå±äºç±» $ğ‘—$ çš„æ¦‚ç‡ï¼Œ ç„¶åé€‰æ‹©å…·æœ‰æœ€å¤§è¾“å‡ºå€¼çš„ç±»åˆ« $argma x_ğ‘—ğ‘¦_ğ‘—$ ä½œä¸ºæˆ‘ä»¬çš„é¢„æµ‹
- å¯¹ç±»åˆ«è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼ˆone-hotï¼‰
    - ç‹¬çƒ­ç¼–ç æ˜¯ä¸€ä¸ªå‘é‡ï¼Œå®ƒçš„åˆ†é‡å’Œç±»åˆ«ä¸€æ ·å¤šã€‚ ç±»åˆ«å¯¹åº”çš„åˆ†é‡è®¾ç½®ä¸º1ï¼Œå…¶ä»–æ‰€æœ‰åˆ†é‡è®¾ç½®ä¸º0
    - $$y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}$$
- æ•´ä¸ªæ•°æ®é›† $\{\mathbf{X}, \mathbf{Y}\}$ å…·æœ‰ $ğ‘›$ ä¸ªæ ·æœ¬ï¼Œ å…¶ä¸­ç´¢å¼• $ğ‘–$ çš„æ ·æœ¬ç”±ç‰¹å¾å‘é‡ $ğ‘¥(ğ‘–)$ å’Œç‹¬çƒ­æ ‡ç­¾å‘é‡ $ğ‘¦(ğ‘–)$ ç»„æˆ

### æ— æ ¡éªŒæ¯”ä¾‹

- ä½œä¸ºåˆ†ç±»è€Œè¨€å¹¶ä¸å…³å¿ƒæ•°å€¼ï¼Œè€Œæ˜¯æ˜¯å¦èƒ½å°½å¯èƒ½æé«˜æ­£ç¡®ç±»åˆ«çš„ç½®ä¿¡åº¦
- è¦ç¡®ä¿æ­£ç¡®è¾“å…¥ç±» y å®ƒå¯¹åº”çš„ç½®ä¿¡åº¦ $O_y$ è¦è¿œè¿œå¤§äºå…¶ä»–é y çš„å¯¹åº”ç½®ä¿¡åº¦ï¼ˆå¤§äºç­‰äºæŸä¸ªé˜ˆå€¼ï¼‰
    - $$O_y - O_i \ge \Delta (y, i)$$
    - è¿™é‡Œå…³æ³¨çš„æ˜¯ç›¸å¯¹å€¼ï¼ˆè€Œéå…·ä½“çš„å€¼ï¼‰ï¼Œä½†å¦‚æœè¿›è¡Œå½’ä¸€åŒ–ï¼ˆæ”¾äºåˆé€‚çš„åŒºé—´ï¼‰å¤„ç†ä¼šæœ‰åŠ©äºåˆ¤æ–­
- ![[00 Attachments/Pasted image 20240514193848.png|500]]

### æ ¡éªŒæ¯”ä¾‹

- æœªè§„èŒƒåŒ–çš„é¢„æµ‹ ğ‘œ ä¸èƒ½ç›´æ¥è§†ä½œæ„Ÿå…´è¶£çš„è¾“å‡º
- å¸Œæœ›è¾“å‡ºæ˜¯ä¸€ä¸ªæ¦‚ç‡ï¼ˆéè´Ÿï¼Œå’Œä¸ºä¸€ï¼‰
- ä»è€Œå¼•å…¥
  softmaxå‡½æ•°ï¼šå¤Ÿå°†æœªè§„èŒƒåŒ–çš„é¢„æµ‹å˜æ¢ä¸ºéè´Ÿæ•°å¹¶ä¸”æ€»å’Œä¸º1ï¼ŒåŒæ—¶è®©æ¨¡å‹ä¿æŒå¯å¯¼çš„æ€§è´¨![[00 Attachments/Pasted image 20240514195002.png|500]]
    - å¯¹ $O_i$ åšæŒ‡æ•°è¿ç®—ï¼Œç¡®ä¿éè´Ÿï¼›é™¤ä»¥æŒ‡æ•°ä¹‹å’Œï¼Œå½’ä¸€åŒ–ï¼ˆå’Œä¸º1ï¼‰
    - softmax å‡½æ•°ç»™å‡ºäº†ä¸€ä¸ªå‘é‡ $\hat ğ‘¦$ï¼Œ å¯ä»¥å°†å…¶è§†ä¸ºâ€œå¯¹ç»™å®šä»»æ„è¾“å…¥ ğ‘¥ çš„æ¯ä¸ªç±»çš„æ¡ä»¶æ¦‚ç‡â€
        - $$\hat y_1 = P(y=\text{çŒ«} \mid \mathbf{x})$$

### äº¤å‰ç†µæŸå¤±

- CrossEntropyLoss
  æ˜¯ä¸€ç§å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼Œä¸»è¦ç”¨äºåˆ†ç±»é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤šç±»åˆ†ç±»ä»»åŠ¡ä¸­ã€‚å®ƒè¡¡é‡äº†æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ï¼Œé€šè¿‡è®¡ç®—é¢„æµ‹æ¦‚ç‡çš„å¯¹æ•°ä¸çœŸå®æ ‡ç­¾çš„äº¤å‰ç†µæ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚è¯¥æŸå¤±å‡½æ•°é¼“åŠ±æ¨¡å‹å°†çœŸå®ç±»åˆ«çš„æ¦‚ç‡æ¨å‘
  1ï¼Œè€Œå°†å…¶ä»–ç±»åˆ«çš„æ¦‚ç‡æ¨å‘ 0ï¼Œå› æ­¤åœ¨è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼ŒCrossEntropyLoss èƒ½æœ‰æ•ˆæé«˜åˆ†ç±»å‡†ç¡®æ€§ã€‚
- [â€œäº¤å‰ç†µâ€å¦‚ä½•åšæŸå¤±å‡½æ•°ï¼Ÿæ‰“åŒ…ç†è§£â€œä¿¡æ¯é‡â€ã€â€œæ¯”ç‰¹â€ã€â€œç†µâ€ã€â€œKLæ•£åº¦â€ã€â€œäº¤å‰ç†µâ€_å“”å“©å“”å“©_bilibili](https://www.bilibili.com/video/BV15V411W7VB/?spm_id_from=333.337.search-card.all.click&vd_source=6fde3ed6da8858e6f7b2f1cc620c6173)
- ä½¿ç”¨äº¤å‰ç†µè¡¡é‡ $\hat y$ ä¸ $y$ ï¼ˆä¸¤ä¸ªæ¦‚ç‡ï¼‰çš„åŒºåˆ«![[00 Attachments/Pasted image 20240514195744.png|500]]
    - å°† softmax å‡½æ•°ä»£å…¥$$\begin{split}\begin{aligned}
      l(\mathbf{y}, \hat{\mathbf{y}}) &= - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
      &= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
      &= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j
      \end{aligned}\end{split}$$
    - å…³äº $o_j$
      æ±‚åå¯¼$$\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j$$
    - ç”±äº $ğ‘¦$ æ˜¯ç‹¬çƒ­ç¼–ç å‘é‡ï¼ˆåªæœ‰ä¸€ä¸ªä¸º 1ï¼Œå…¶ä»–ä¸º 0ï¼‰ï¼Œåšä¹˜ç§¯å’Œåï¼Œé™¤äº†ä¸€ä¸ªé¡¹ä»¥å¤–çš„æ‰€æœ‰é¡¹ i éƒ½æ¶ˆå¤±äº†

### ä¿¡æ¯è®ºåŸºç¡€

- ä»æ–°å®¡è§†äº¤å‰ç†µ

## æŸå¤±å‡½æ•°

- æ‹Ÿåˆè¿‡ç¨‹ä¸­ï¼Œæœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œå³ä¸ºæœ€å¤§åŒ–ä¼¼ç„¶å‡½æ•°

### å‡æ–¹æŸå¤±

- å‡è®¾ $y = 0$![[00 Attachments/Pasted image 20240514215400.png|400]]
    - è“çº¿ä¸º $l$ï¼Œç»¿çº¿ä¸º å¯¹åº”çš„ä¼¼ç„¶å‡½æ•°ï¼ˆè¡¨ç¤º $\hat y = y$ çš„å¯è¡Œæ€§ï¼‰ï¼Œæ©™çº¿ä¸ºæ¢¯åº¦
    - å½“ $\hat y$  ä¸ $y = 0$ ç¦»å¾—è¾ƒè¿œæ—¶ï¼Œæ¢¯åº¦å¤§ï¼Œå‚æ•°æ›´æ–°è·¨åº¦è¾ƒå¤§ï¼ˆå¯èƒ½ä¼šå¸¦æ¥ä¸ç¨³å®šï¼‰ï¼Œ$\hat y$ ä¼šæœç®­å¤´æ–¹å‘ç§»åŠ¨
    - éšç€é¢„æµ‹å€¼é€æ¸é è¿‘å®é™…å€¼ï¼Œæ¢¯åº¦å‡å°ï¼Œå‚æ•°æ›´æ–°çš„è·¨åº¦ä¹Ÿå˜å°

### ç»å¯¹å€¼æŸå¤±

- ä¸Šè¿°ï¼Œå½“ä¼°è®¡å€¼ç¦»å¾—é¢„æµ‹å€¼è¾ƒè¿œæ—¶ï¼Œå‚æ•°æ›´æ–°è·¨åº¦è¿‡å¤§ï¼Œä¸ºäº†ç¼“è§£ï¼Œå¯ä»¥é‡‡ç”¨ç»å¯¹å€¼æŸå¤±
- å‡è®¾ $y = 0$![[00 Attachments/Pasted image 20240514221745.png|400]]
    - å¯ä»¥çŸ¥é“ï¼Œæ— è®ºä¼°è®¡å€¼ç¦»å®é™…å€¼æœ‰å¤šè¿œï¼Œå‚æ•°æ›´æ–°è·¨åº¦ç›¸åŒï¼Œå¯èƒ½ä¼šå¸¦æ¥ç¨³å®šæ€§ä¸Šçš„å¥½å¤„
    - åå¤„ï¼Œå½“ $\hat y$ ç¦» $y$ å¾ˆè¿‘æ—¶ï¼ˆä¼˜åŒ–åˆ°é»˜å¥‘ï¼‰ï¼Œæ¢¯åº¦ä¸ç¨³å®šï¼Œå¯èƒ½ä¼šå¯¼è‡´ç³»ç»Ÿä¸ç¨³å®š

### Huber's Robust Loss

- ç»“åˆå‡æ–¹æŸå¤±ä¸ç»å¯¹å€¼æŸå¤±çš„ä¼˜ç‚¹
- ![[00 Attachments/Pasted image 20240514222122.png|400]]
    - å½“é¢„æµ‹å€¼ä¸å®é™…å€¼å·®çš„è¾ƒå¤§æ˜¯ï¼Œä¸ºç»å¯¹å€¼æŸå¤±ï¼›å·®çš„è¾ƒå°æ—¶ï¼Œä¸ºå‡æ–¹æŸå¤±ï¼ˆä¿è¯å‚æ•°æ›´æ–°çš„å¹³æ»‘æ€§ï¼‰

## å›¾ç‰‡åˆ†ç±»æ•°æ®é›†

- MNISTæ•°æ®é›†æ˜¯å›¾åƒåˆ†ç±»ä¸­å¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¹‹ä¸€ï¼Œä½†ä½œä¸ºåŸºå‡†æ•°æ®é›†è¿‡äºç®€å•ã€‚ æˆ‘ä»¬å°†ä½¿ç”¨ç±»ä¼¼ä½†æ›´å¤æ‚çš„Fashion-MNISTæ•°æ®é›†

```python
import torch
import torchvision
from torch.utils import data  # å¯¼å…¥æ•°æ®é›†  
from torchvision import transforms  # å¯¼å…¥æ•°æ®é¢„å¤„ç†  
from d2l import torch as d2l
```

### è¯»å–æ•°æ®é›†

- é€šè¿‡æ¡†æ¶ä¸­çš„å†…ç½®å‡½æ•°å°†Fashion-MNISTæ•°æ®é›†ä¸‹è½½å¹¶è¯»å–åˆ°å†…å­˜ä¸­
- é€šè¿‡ToTensorå®ä¾‹å°†å›¾åƒæ•°æ®ä»PILç±»å‹å˜æ¢æˆ32ä½æµ®ç‚¹æ•°æ ¼å¼ï¼Œå¹¶é™¤ä»¥255ä½¿å¾—æ‰€æœ‰åƒç´ çš„æ•°å€¼å‡åœ¨0ï½1ä¹‹é—´

```python
trans = transforms.ToTensor()  # è½¬æ¢ä¸ºå¼ é‡  
# è®­ç»ƒé›† train=Trueä¸‹è½½è®­ç»ƒé›†ï¼Œtransform=transå¾—åˆ°çš„æ˜¯pytorchçš„tensoræ ¼å¼ï¼Œdownload=Trueé»˜è®¤ä»ç½‘ä¸Šä¸‹è½½  
# å¦‚æœæœ¬åœ°å·²ç»æœ‰äº†å°±ä¸ç”¨ä¸‹è½½äº†  
mnist_train = torchvision.datasets.MNIST(
    root='./data', train=True, transform=trans, download=False)  # è®­ç»ƒé›†  
mnist_test = torchvision.datasets.MNIST(
    root='./data', train=False, transform=trans, download=False)  # æµ‹è¯•é›† ç”¨äºéªŒè¯æ¨¡å‹çš„å¥½å
```

- Fashion-MNIST ç”±10ä¸ªç±»åˆ«çš„å›¾åƒç»„æˆï¼Œ æ¯ä¸ªç±»åˆ«ç”±è®­ç»ƒæ•°æ®é›†ï¼ˆtrain datasetï¼‰ä¸­çš„6000å¼ å›¾åƒ å’Œæµ‹è¯•æ•°æ®é›†ï¼ˆtest
  datasetï¼‰ä¸­çš„1000å¼ å›¾åƒç»„æˆã€‚ å› æ­¤ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†åˆ«åŒ…å«60000å’Œ10000å¼ å›¾åƒã€‚ æµ‹è¯•æ•°æ®é›†ä¸ä¼šç”¨äºè®­ç»ƒï¼Œåªç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

```python
print(len(mnist_train), len(mnist_test))
# 60000 10000
```

- æ¯ä¸ªè¾“å…¥å›¾åƒçš„é«˜åº¦å’Œå®½åº¦å‡ä¸º 28 åƒç´ ã€‚ æ•°æ®é›†ç”±ç°åº¦å›¾åƒç»„æˆï¼Œå…¶é€šé“æ•°ä¸º 1ã€‚ ä¸ºäº†ç®€æ´èµ·è§ï¼Œå°†é«˜åº¦ â„ åƒç´ ã€å®½åº¦ ğ‘¤
  åƒç´ å›¾åƒçš„å½¢çŠ¶è®°ä¸º $â„Ã—ğ‘¤$ æˆ– $(â„,ğ‘¤)$

```python
print(len(mnist_train), len(mnist_test))
# 60000 10000  
print(mnist_train.data.shape, mnist_test.data.shape)
# torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])  
print(mnist_train[0][0].shape, mnist_train[0][1])  # ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å½¢çŠ¶å’Œæ ‡ç­¾  
# torch.Size([1, 28, 28]) 5
```

-

Fashion-MNISTä¸­åŒ…å«çš„10ä¸ªç±»åˆ«ï¼Œåˆ†åˆ«ä¸ºt-shirtï¼ˆTæ¤ï¼‰ã€trouserï¼ˆè£¤å­ï¼‰ã€pulloverï¼ˆå¥—è¡«ï¼‰ã€dressï¼ˆè¿è¡£è£™ï¼‰ã€coatï¼ˆå¤–å¥—ï¼‰ã€sandalï¼ˆå‡‰é‹ï¼‰ã€shirtï¼ˆè¡¬è¡«ï¼‰ã€sneakerï¼ˆè¿åŠ¨é‹ï¼‰ã€bagï¼ˆåŒ…ï¼‰å’Œankle
bootï¼ˆçŸ­é´ï¼‰ã€‚ ä»¥ä¸‹å‡½æ•°ç”¨äºåœ¨æ•°å­—æ ‡ç­¾ç´¢å¼•åŠå…¶æ–‡æœ¬åç§°ä¹‹é—´è¿›è¡Œè½¬æ¢

```python
def get_fashion_mnist_labels(labels):  # @save  
    """è¿”å›Fashion-MNISTæ•°æ®é›†çš„æ–‡æœ¬æ ‡ç­¾"""
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]


def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  # @save  
    """ç»˜åˆ¶å›¾åƒåˆ—è¡¨"""
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten()
    for i, (ax, img) in enumerate(zip(axes, imgs)):
        if torch.is_tensor(img):
            # å›¾ç‰‡å¼ é‡  
            ax.imshow(img.numpy())
        else:
            # PILå›¾ç‰‡  
            ax.imshow(img)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    return axes


X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))  # å–å‡ºä¸€ç»„æ ·æœ¬  
show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y))  # ç»˜åˆ¶å›¾åƒåˆ—è¡¨
```

- ç”»å‡ºçš„å›¾åƒ ![[00 Attachments/Figure_2.png|500]]

### è¯»å–å°æ‰¹é‡

- ä¸ºäº†ä½¿æˆ‘ä»¬åœ¨è¯»å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ—¶æ›´å®¹æ˜“ï¼Œæˆ‘ä»¬ä½¿ç”¨å†…ç½®çš„æ•°æ®è¿­ä»£å™¨ï¼Œè€Œä¸æ˜¯ä»é›¶å¼€å§‹åˆ›å»ºã€‚ å›é¡¾ä¸€ä¸‹ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ•°æ®åŠ è½½å™¨æ¯æ¬¡éƒ½ä¼šè¯»å–ä¸€å°æ‰¹é‡æ•°æ®ï¼Œå¤§å°ä¸º
  `batch_size`ã€‚ é€šè¿‡å†…ç½®æ•°æ®è¿­ä»£å™¨ï¼Œæˆ‘ä»¬å¯ä»¥éšæœºæ‰“ä¹±äº†æ‰€æœ‰æ ·æœ¬ï¼Œä»è€Œæ— åè§åœ°è¯»å–å°æ‰¹é‡

```python
batch_size = 256


def get_dataloader_workers():  # @save  
    """ä½¿ç”¨4ä¸ªè¿›ç¨‹æ¥è¯»å–æ•°æ®"""
    return 4


train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,  # Trueè¡¨ç¤ºæ¯ä¸ªepochæ‰“ä¹±æ•°æ®  
                             num_workers=get_dataloader_workers()
                             if sys.platform.startswith('win64') else 0)  # windowsä¸‹num_workers>0å¯èƒ½ä¼šæŠ¥é”™
```

- æŸ¥çœ‹çœ‹è¯»å–è®­ç»ƒæ•°æ®æ‰€éœ€çš„æ—¶é—´

```python
timer = d2l.Timer()
for X, y in train_iter:
    continue
print(f'{timer.stop():.2f} sec')
# 4.53 sec
```

### æ•´åˆæ‰€æœ‰ç»„ä»¶

- ç°åœ¨æˆ‘ä»¬å®šä¹‰`load_data_fashion_mnist`å‡½æ•°ï¼Œç”¨äºè·å–å’Œè¯»å– Fashion-MNIST æ•°æ®é›†
    - è¿™ä¸ªå‡½æ•°è¿”å›è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ•°æ®è¿­ä»£å™¨
    - æ­¤å¤–ï¼Œè¿™ä¸ªå‡½æ•°è¿˜æ¥å—ä¸€ä¸ªå¯é€‰å‚æ•°`resize`ï¼Œç”¨æ¥å°†å›¾åƒå¤§å°è°ƒæ•´ä¸ºå¦ä¸€ç§å½¢çŠ¶ã€‚

```python
def load_data_fashion_mnist(batch_size, resize=None):  # @save  # resize=Noneè¡¨ç¤ºä¸æ”¹å˜å›¾åƒå¤§å°  
    """ä¸‹è½½Fashion-MNISTæ•°æ®é›†ï¼Œç„¶åå°†å…¶åŠ è½½åˆ°å†…å­˜ä¸­"""
    trans = [transforms.ToTensor()]  # å®šä¹‰å›¾åƒè½¬æ¢æ“ä½œ  
    if resize:
        trans.insert(0, transforms.Resize(resize))  # å¦‚æœresizeä¸ä¸ºNoneï¼Œåˆ™åœ¨å›¾åƒè½¬æ¢æ“ä½œä¸­æ’å…¥Resizeæ“ä½œ  
    trans = transforms.Compose(trans)  # ç»„åˆå›¾åƒè½¬æ¢æ“ä½œ  
    mnist_train = torchvision.datasets.FashionMNIST(  # ä¸‹è½½Fashion-MNISTè®­ç»ƒæ•°æ®é›†  
        root="../data", train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(  # ä¸‹è½½Fashion-MNISTæµ‹è¯•æ•°æ®é›†  
        root="../data", train=False, transform=trans, download=True)
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,  # è¿”å›è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†çš„æ•°æ®åŠ è½½å™¨  
                            num_workers=get_dataloader_workers()
                            if sys.platform.startswith('win64') else 0),
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers()
                            if sys.platform.startswith('win64') else 0))
```

- é€šè¿‡æŒ‡å®š`resize`å‚æ•°æ¥æµ‹è¯•`load_data_fashion_mnist`å‡½æ•°çš„å›¾åƒå¤§å°è°ƒæ•´åŠŸèƒ½

```python
train_iter, test_iter = load_data_fashion_mnist(32, resize=64)
for X, y in train_iter:
    print(X.shape, X.dtype, y.shape, y.dtype)
    break
# torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64
```

## Softmax ä»é›¶å¼€å§‹

```python
import torch
from IPython import display
from d2l import torch as d2l

bath_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=bath_size)  # åœ¨æºä»£ç å¤„åŠ å…¥äº†æ“ä½œç³»ç»Ÿçš„åˆ¤æ–­
```

### åˆå§‹åŒ–æ¨¡å‹å‚æ•°

- åœ¨ä¹‹å‰çš„çº¿æ€§å›å½’ä¸­ï¼Œæ ·æœ¬æ˜¯ç”±å›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤ºçš„
- æ­¤å¤„ï¼ŒåŸå§‹æ ·æœ¬ä¸º 28\*28 çš„å›¾åƒï¼Œå°†å…¶å±•å¼€ä¸ºé•¿åº¦ä¸º 784 çš„å‘é‡ï¼Œå°†æ¯ä¸ªåƒç´ ä½ç½®è§†ä½œä¸€ä¸ªç‰¹å¾ï¼ˆä¹‹åå°†è®¨è®ºèƒ½æŠ•åˆ©ç”¨å›¾åƒç©ºé—´ç»“æ„çš„ç‰¹å¾ï¼‰
- åœ¨ softmsx ä¸­ï¼Œè¾“å‡ºä¸ªæ•°ä¸æ•°æ®é›†ç±»åˆ«ä¸ªæ•°ç›¸åŒã€‚å› ä¸ºæ•°æ®é›†æœ‰10ä¸ªç±»åˆ«ï¼Œæ‰€ä»¥ç½‘ç»œè¾“å‡ºç»´åº¦ä¸º10
- å› æ­¤æƒé‡å°†æ„æˆä¸€ä¸ª 784Ã—10 çš„çŸ©é˜µï¼Œ åç½®å°†æ„æˆä¸€ä¸ª 1Ã—10 çš„è¡Œå‘é‡
- ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ– wï¼Œåæ‰§åˆå§‹åŒ–ä¸º 0

```python
num_inputs = 784
num_outputs = 10

W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)
```

### å®šä¹‰ softmax æ“ä½œ

- $$\mathrm{softmax}(\mathbf{X})_{ij} = \frac{\exp(\mathbf{X}_{ij})}{\sum_k \exp(\mathbf{X}_{ik})}$$
- softmax æœ‰ä¸‰æ­¥
    - å¯¹æ¯ä¸ªé¡¹æ±‚å¹‚
    - å¯¹æ¯ä¸€è¡Œæ±‚å’Œï¼ˆå°æ‰¹é‡ä¸­ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯ä¸€æ ·ï¼‰ï¼Œå¾—åˆ°æ¯ä¸ªæ ·æœ¬çš„è§„èŒƒåŒ–å¸¸æ•°
    - å°†æ¯ä¸€è¡Œé™¤ä»¥å…¶è§„èŒƒåŒ–å¸¸æ•°ï¼Œç¡®ä¿ç»“æœçš„å’Œä¸º1

```python
def softmax(X):
    """è¾“å…¥çš„ X æ˜¯ä¸€ç»„é¢„æµ‹å€¼ï¼Œè¾“å‡º softmax å¤„ç†è¿‡çš„é¢„æµ‹å€¼"""
    X_exp = torch.exp(X)
    partition = X_exp.sum(dim=1, keepdim=True)
    return X_exp / partition  # åº”ç”¨äº†å¹¿æ’­æœºåˆ¶
```

- å¯¹äºä»»ä½•éšå³è¾“å…¥ï¼Œå°†æ¯ä¸ªå…ƒç´ å˜æˆä¸€ä¸ªéè´Ÿæ•°ã€‚ä¸”æ¯è¡Œç»¼åˆä¸º 1
- ä¸¾ä¾‹

```python
X = torch.normal(0, 1, (2, 5))
X_prob = softmax(X)
print(X_prob)
# tensor([[0.1227, 0.2125, 0.2198, 0.1211, 0.3239],  
#         [0.3685, 0.1344, 0.1431, 0.2738, 0.0803]])
```

### å®šä¹‰æ¨¡å‹

- å®šä¹‰ softmax åï¼Œæˆ‘ä»¬å¯ä»¥å®ç° softmax å›å½’æ¨¡å‹
- ä¸‹é¢çš„ä»£ç å®šä¹‰äº†è¾“å…¥å¦‚ä½•é€šè¿‡ç½‘ç»œæ˜ å°„åˆ°è¾“å‡º

```python
def net(X):
    """è¾“å…¥çš„ X æ˜¯å›¾åƒæ•°æ®ï¼Œè¾“å‡ºé¢„æµ‹å€¼"""
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)  # ä½¿ç”¨reshapeå‡½æ•°å°†æ¯å¼ åŸå§‹å›¾åƒå±•å¹³ä¸ºå‘é‡
```

### å®šä¹‰æŸå¤±å‡½æ•°

- äº¤å‰ç†µé‡‡ç”¨çœŸå®æ ‡ç­¾çš„é¢„æµ‹æ¦‚ç‡çš„è´Ÿå¯¹æ•°ä¼¼ç„¶

```python
def cross_entropy(y_hat, y):
    """è¾“å…¥çš„ y_hat æ˜¯é¢„æµ‹å€¼ï¼Œy æ˜¯çœŸå®æ ‡ç­¾ï¼Œè¾“å‡ºäº¤å‰ç†µæŸå¤±"""
    return -torch.log(y_hat[range(len(y_hat)), y])  # ç”±äºç‹¬çƒ­ç¼–ç ï¼Œåªéœ€è®¡ç®—å®é™…æ ‡ç­¾å¯¹åº”çš„é¢„æµ‹å€¼çš„å¯¹æ•°å³å¯
```

- ç”±äºç‹¬çƒ­ç¼–ç ï¼Œè¿™é‡Œè‡³äºè¦çŸ¥é“å®é™…ç±»åˆ«å¯¹åº”çš„é¢„æµ‹å€¼å³å¯ï¼Œä½†å¦‚ä½•å°†å®é™…å€¼å¯¹åº”çš„é¢„æµ‹å€¼å–å‡ºï¼Ÿ
    - è¿™é‡Œä¸ä½¿ç”¨ Python çš„ for å¾ªç¯è¿­ä»£é¢„æµ‹ï¼ˆè¿™å¾€å¾€æ˜¯ä½æ•ˆçš„ï¼‰ï¼Œ è€Œæ˜¯é€šè¿‡ä¸€ä¸ªè¿ç®—ç¬¦é€‰æ‹©æ‰€æœ‰å…ƒç´ 
    - ä¸‹é¢åˆ›å»ºä¸€ä¸ªæ•°æ®æ ·æœ¬ $\hat y$ï¼Œå…¶ä¸­åŒ…å« 2 ä¸ªæ ·æœ¬åœ¨ 3 ä¸ªç±»åˆ«çš„é¢„æµ‹ï¼Œä»¥åŠå®ƒä»¬å¯¹åº”çš„æ ‡ç­¾
    - æœ‰äº†`y`ï¼Œå¯ä»¥çŸ¥é“åœ¨ç¬¬ 0 ä¸ªæ ·æœ¬ä¸­ï¼Œç¬¬ 0 ç±»æ˜¯æ­£ç¡®çš„é¢„æµ‹ï¼› è€Œåœ¨ç¬¬ 1 ä¸ªæ ·æœ¬ä¸­ï¼Œç¬¬ 2 ç±»æ˜¯æ­£ç¡®çš„é¢„æµ‹ã€‚ ç„¶åä½¿ç”¨ `y` ä½œä¸º
      `y_hat` ä¸­æ¦‚ç‡çš„ç´¢å¼•ï¼Œ é€‰æ‹©ç¬¬ä¸€ä¸ªæ ·æœ¬ä¸­ç¬¬ 0 ä¸ªç±»çš„æ¦‚ç‡å’Œç¬¬äºŒä¸ªæ ·æœ¬ä¸­ç¬¬ 2 ä¸ªç±»çš„æ¦‚ç‡

```python
y = torch.tensor([0, 2])  # å®é™…æ ‡ç­¾  
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])  # é¢„æµ‹å€¼  
print(y_hat[[0, 1], y])  # è¾“å‡º  
# tensor([0.1000, 0.5000])  
print(cross_entropy(y_hat, y))
# tensor([2.3026, 0.6931])
```

### åˆ†ç±»ç²¾åº¦

- ç»™å®šé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ`y_hat`ï¼Œå½“æˆ‘ä»¬å¿…é¡»è¾“å‡ºç¡¬é¢„æµ‹ï¼ˆhard predictionï¼‰æ—¶ï¼Œ æˆ‘ä»¬é€šå¸¸é€‰æ‹©é¢„æµ‹æ¦‚ç‡æœ€é«˜çš„ç±»ï¼ˆåšå‡ºé€‰æ‹©ï¼‰
- å½“é¢„æµ‹ä¸æ ‡ç­¾åˆ†ç±»`y`ä¸€è‡´æ—¶ï¼Œå³æ˜¯æ­£ç¡®çš„ã€‚ ==åˆ†ç±»ç²¾åº¦==å³æ­£ç¡®é¢„æµ‹æ•°é‡ä¸æ€»é¢„æµ‹æ•°é‡ä¹‹æ¯”
    - è™½ç„¶ç›´æ¥ä¼˜åŒ–ç²¾åº¦å¯èƒ½å¾ˆå›°éš¾ï¼ˆå› ä¸ºç²¾åº¦çš„è®¡ç®—ä¸å¯å¯¼ï¼‰ï¼Œ ä½†ç²¾åº¦é€šå¸¸æ˜¯æˆ‘ä»¬æœ€å…³å¿ƒçš„æ€§èƒ½è¡¡é‡æ ‡å‡†ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒåˆ†ç±»å™¨æ—¶å‡ ä¹æ€»ä¼šå…³æ³¨å®ƒ
- è®¡ç®—ç²¾åº¦
    1. å¦‚æœ`y_hat`æ˜¯çŸ©é˜µï¼Œé‚£ä¹ˆå‡å®šç¬¬äºŒä¸ªç»´åº¦å­˜å‚¨æ¯ä¸ªç±»çš„é¢„æµ‹åˆ†æ•°
    2. ä½¿ç”¨`argmax`è·å¾—æ¯è¡Œä¸­æœ€å¤§å…ƒç´ çš„ç´¢å¼•æ¥è·å¾—é¢„æµ‹ç±»åˆ«
    3. å°†é¢„æµ‹ç±»åˆ«ä¸çœŸå®`y`å…ƒç´ è¿›è¡Œæ¯”è¾ƒï¼ˆç”±äºç­‰å¼è¿ç®—ç¬¦â€œ`==`â€å¯¹æ•°æ®ç±»å‹å¾ˆæ•æ„Ÿï¼Œ å› æ­¤æˆ‘ä»¬å°†`y_hat`çš„æ•°æ®ç±»å‹è½¬æ¢ä¸ºä¸`y`
       çš„æ•°æ®ç±»å‹ä¸€è‡´ï¼‰ï¼ˆç»“æœæ˜¯ä¸€ä¸ªåŒ…å«0ï¼ˆé”™ï¼‰å’Œ1ï¼ˆå¯¹ï¼‰çš„å¼ é‡ï¼‰
    4. æ±‚å’Œå¾—åˆ°æ­£ç¡®é¢„æµ‹çš„æ•°é‡

```python
def accuracy(y_hat, y):
    """è¾“å…¥çš„ y_hat æ˜¯é¢„æµ‹å€¼ï¼Œy æ˜¯çœŸå®æ ‡ç­¾ï¼Œè¾“å‡ºé¢„ç®—å€¼ä¸å®é™…ä¸€è‡´çš„ä¸ªæ•°"""


if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:  # y_hat ä¸ºçŸ©é˜µï¼Œy_hat.shape[1]>1è¡¨ç¤ºä¸æ­¢ä¸€ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ«æœ‰å„è‡ªçš„æ¦‚ç‡  
    # print("y_hat_before", y_hat)  
    # # y_hat_before tensor([[0.1000, 0.3000, 0.6000],
    # #         [0.3000, 0.2000, 0.5000]])
    y_hat = y_hat.argmax(axis=1)  # y_hat.argmax(axis=1)ä¸ºæ±‚è¡Œæœ€å¤§å€¼ç´¢å¼• axis=1è¡¨ç¤ºæŒ‰åˆ—æ±‚æœ€å¤§å€¼  
    # print("y_hat_after", y_hat)  
    # # y_hat_after tensor([2, 2])
    cmp = y_hat.type(y.dtype) == y  # å…ˆåˆ¤æ–­é€»è¾‘è¿ç®—ç¬¦==ï¼Œå†èµ‹å€¼ç»™cmpï¼Œcmpä¸ºå¸ƒå°”ç±»å‹çš„æ•°æ® type å‡½æ•°ç”¨äºè½¬æ¢æ•°æ®ç±»å‹  
return float(cmp.type(y.dtype).sum())  # è·å¾—y.dtypeçš„ç±»å‹ä½œä¸ºä¼ å…¥å‚æ•°ï¼Œå°†cmpçš„ç±»å‹è½¬ä¸ºyçš„ç±»å‹ï¼ˆintå‹ï¼‰ï¼Œç„¶åå†æ±‚å’Œ  
```

- ç»§ç»­ä½¿ç”¨ä¹‹å‰å®šä¹‰çš„å˜é‡`y_hat`å’Œ`y`åˆ†åˆ«ä½œä¸ºé¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒå’Œæ ‡ç­¾
- å¯ä»¥çœ‹åˆ°ï¼Œç¬¬ 0 ä¸ªæ ·æœ¬çš„é¢„æµ‹ç±»åˆ«æ˜¯ 2ï¼ˆè¯¥è¡Œçš„æœ€å¤§å…ƒç´ ä¸º0.6ï¼Œç´¢å¼•ä¸º2ï¼‰ï¼Œè¿™ä¸å®é™…æ ‡ç­¾ 0 ä¸ä¸€è‡´ã€‚ ç¬¬ 1
  ä¸ªæ ·æœ¬çš„é¢„æµ‹ç±»åˆ«æ˜¯2ï¼ˆè¯¥è¡Œçš„æœ€å¤§å…ƒç´ ä¸º0.5ï¼Œç´¢å¼•ä¸º2ï¼‰ï¼Œè¿™ä¸å®é™…æ ‡ç­¾ 2 ä¸€è‡´ã€‚ å› æ­¤ï¼Œè¿™ä¸¤ä¸ªæ ·æœ¬çš„åˆ†ç±»ç²¾åº¦ç‡ä¸º0.5

```python
print("accuracy(y_hat,y) / len(y):", accuracy(y_hat, y) / len(y))
print("accuracy(y_hat,y):", accuracy(y_hat, y))  # é¢„æµ‹ä¸å®é™…ä¸€è‡´çš„ä¸ªæ•°  
print("len(y):", len(y))
# accuracy(y_hat,y) / len(y): 0.5  
# accuracy(y_hat,y): 1.0  
# len(y): 2
```

- å¯¹äºä»»æ„æ•°æ®è¿­ä»£å™¨`data_iter`å¯è®¿é—®çš„æ•°æ®é›†ï¼Œ æˆ‘ä»¬å¯ä»¥è¯„ä¼°åœ¨ä»»æ„æ¨¡å‹`net`çš„ç²¾åº¦

```python
def evaluate_accuracy(net, data_iter):
    """è®¡ç®—åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šæ¨¡å‹çš„ç²¾åº¦"""
    if isinstance(net, torch.nn.Module):  # å¦‚æœnetæ¨¡å‹æ˜¯torch.nn.Moduleå®ç°çš„ç¥ç»ç½‘ç»œçš„è¯ï¼Œå°†å®ƒå˜æˆè¯„ä¼°æ¨¡å¼  
        net.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼  
    metric = Accumulator(2)  # æ­£ç¡®é¢„æµ‹æ•°ã€é¢„æµ‹æ€»æ•°ï¼Œmetricä¸ºç´¯åŠ å™¨çš„å®ä¾‹åŒ–å¯¹è±¡ï¼Œé‡Œé¢å­˜äº†ä¸¤ä¸ªæ•°  
    for X, y in data_iter:
        metric.add(accuracy(net(X), y), y.numel())  # net(X)å°†Xè¾“å…¥æ¨¡å‹ï¼Œè·å¾—é¢„æµ‹å€¼ã€‚y.numel()ä¸ºæ ·æœ¬æ€»æ•°  
    return metric[0] / metric[1]  # åˆ†ç±»æ­£ç¡®çš„æ ·æœ¬æ•° / æ€»æ ·æœ¬æ•°
```

- å®šä¹‰ä¸€ä¸ªå®ç”¨ç¨‹åºç±»`Accumulator`ï¼Œç”¨äºå¯¹å¤šä¸ªå˜é‡è¿›è¡Œç´¯åŠ 
    - åœ¨ä¸Šé¢çš„`evaluate_accuracy`å‡½æ•°ä¸­ï¼Œ æˆ‘ä»¬åœ¨`Accumulator`å®ä¾‹ä¸­åˆ›å»ºäº†2ä¸ªå˜é‡ï¼Œ åˆ†åˆ«ç”¨äºå­˜å‚¨æ­£ç¡®é¢„æµ‹çš„æ•°é‡å’Œé¢„æµ‹çš„æ€»æ•°é‡ã€‚
      å½“æˆ‘ä»¬éå†æ•°æ®é›†æ—¶ï¼Œä¸¤è€…éƒ½å°†éšç€æ—¶é—´çš„æ¨ç§»è€Œç´¯åŠ 

```python
# Accumulatorå®ä¾‹ä¸­åˆ›å»ºäº†2ä¸ªå˜é‡ï¼Œç”¨äºåˆ†åˆ«å­˜å‚¨æ­£ç¡®é¢„æµ‹çš„æ•°é‡å’Œé¢„æµ‹çš„æ€»æ•°é‡  
class Accumulator:
    """åœ¨nä¸ªå˜é‡ä¸Šç´¯åŠ """

    def __init__(self, n):
        self.data = [0, 0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]  # zipå‡½æ•°æŠŠä¸¤ä¸ªåˆ—è¡¨ç¬¬ä¸€ä¸ªä½ç½®å…ƒç´ æ‰“åŒ…ã€ç¬¬äºŒä¸ªä½ç½®å…ƒç´ æ‰“åŒ…....  

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

- éšæœºæƒé‡å’Œåç½®åˆå§‹åŒ– net æ¨¡å‹ï¼Œåˆ™è¯¥æ¨¡å‹çš„ç²¾åº¦æ˜¯éšæœºçš„

```python
print(evaluate_accuracy(net, test_iter))  # è¾“å‡ºæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„ç²¾åº¦  
# 0.0994
```

### è®­ç»ƒ

- å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®­ç»ƒä¸€ä¸ªè¿­ä»£å‘¨æœŸ
- `updater`æ˜¯æ›´æ–°æ¨¡å‹å‚æ•°çš„å¸¸ç”¨å‡½æ•°ï¼Œå®ƒæ¥å—æ‰¹é‡å¤§å°ä½œä¸ºå‚æ•°

```python
def train_epoch_ch3(net, train_iter, loss, updater):  # @save  
    """è®­ç»ƒæ¨¡å‹ä¸€ä¸ªè¿­ä»£å‘¨æœŸï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    if isinstance(net, torch.nn.Module):  # å¦‚æœnetæ¨¡å‹æ˜¯torch.nn.Moduleå®ç°çš„ç¥ç»ç½‘ç»œçš„è¯ï¼Œå°†å®ƒå˜æˆè®­ç»ƒæ¨¡å¼  
        net.train()
    # è®­ç»ƒæŸå¤±æ€»å’Œã€è®­ç»ƒå‡†ç¡®åº¦æ€»å’Œã€æ ·æœ¬æ•°  
    metric = Accumulator(3)
    for X, y in train_iter:
        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°  
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):  # å¦‚æœupdateræ˜¯torch.optim.Optimizerå®ä¾‹çš„è¯ï¼Œä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°  
            # ä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°  
            updater.zero_grad()  # æ¢¯åº¦æ¸…é›¶  
            l.mean().backward()  # è®¡ç®—æ¢¯åº¦  
            updater.step()  # æ›´æ–°å‚æ•°  
        else:
            # ä½¿ç”¨å®šåˆ¶çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°  
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # è¿”å›è®­ç»ƒæŸå¤±å’Œè®­ç»ƒç²¾åº¦  
    return metric[0] / metric[2], metric[1] / metric[2]
```

- å±•ç¤ºè®­ç»ƒå‡½æ•°çš„å®ç°ä¹‹å‰ï¼Œå®šä¹‰ä¸€ä¸ªåœ¨åŠ¨ç”»ä¸­ç»˜åˆ¶æ•°æ®çš„å®ç”¨ç¨‹åºç±»`Animator`ï¼Œ å®ƒèƒ½å¤Ÿç®€åŒ–æœ¬ä¹¦å…¶ä½™éƒ¨åˆ†çš„ä»£ç 
    - é€‚é…pycharm

```python
class Animator:
    """åœ¨åŠ¨ç”»ä¸­ç»˜åˆ¶æ•°æ®"""

    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        """  
        åˆå§‹åŒ–Animatorå¯¹è±¡  
  
        å‚æ•°:  
        - xlabel: xè½´æ ‡ç­¾  
        - ylabel: yè½´æ ‡ç­¾  
        - legend: å›¾ä¾‹  
        - xlim: xè½´èŒƒå›´  
        - ylim: yè½´èŒƒå›´  
        - xscale: xè½´æ¯”ä¾‹  
        - yscale: yè½´æ¯”ä¾‹  
        - fmts: çº¿æ¡æ ¼å¼  
        - nrows: å­å›¾è¡Œæ•°  
        - ncols: å­å›¾åˆ—æ•°  
        - figsize: å›¾è¡¨å¤§å°  
        """
        if legend is None:
            legend = []
        # ä½¿ç”¨SVGæ˜¾ç¤ºåŠ¨ç”»  
        d2l.use_svg_display()
        # åˆ›å»ºåŒ…å«å¤šä¸ªå­å›¾çš„å›¾è¡¨  
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        # å¦‚æœåªæœ‰ä¸€ä¸ªå­å›¾ï¼Œåˆ™å°†å…¶æ”¾å…¥åˆ—è¡¨ä¸­ï¼Œæ–¹ä¾¿åç»­å¤„ç†  
        if nrows * ncols == 1:
            self.axes = [self.axes]
        # ä½¿ç”¨lambdaå‡½æ•°é…ç½®åæ ‡è½´  
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        # åˆå§‹åŒ–Xå’ŒYåæ ‡ä»¥åŠçº¿æ¡æ ¼å¼  
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        """  
        å‘å›¾è¡¨ä¸­æ·»åŠ å¤šä¸ªæ•°æ®ç‚¹  
  
        å‚æ•°:  
        - x: xåæ ‡æ•°æ®  
        - y: yåæ ‡æ•°æ®  
        """  # å¦‚æœyä¸æ˜¯åˆ—è¡¨æˆ–æ•°ç»„ï¼Œå°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨  
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        # å¦‚æœxä¸æ˜¯åˆ—è¡¨æˆ–æ•°ç»„ï¼Œå°†å…¶è½¬æ¢ä¸ºä¸yç›¸åŒé•¿åº¦çš„åˆ—è¡¨  
        if not hasattr(x, "__len__"):
            x = [x] * n
        # å¦‚æœXå’ŒYåæ ‡ä¸ºç©ºï¼Œåˆ™åˆå§‹åŒ–å®ƒä»¬  
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        # å°†æ•°æ®æ·»åŠ åˆ°Xå’ŒYåæ ‡ä¸­  
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
            # æ¸…ç©ºå›¾è¡¨å¹¶ç»˜åˆ¶çº¿æ¡  
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        # é…ç½®åæ ‡è½´å¹¶æ˜¾ç¤ºå›¾è¡¨  
        self.config_axes()
        d2l.plt.draw()  # æ›´æ–°å›¾è¡¨  
        d2l.plt.pause(0.001)  # çŸ­æš‚æš‚åœä»¥æ›´æ–°å›¾è¡¨
```

- å®ç°ä¸€ä¸ªè®­ç»ƒå‡½æ•°ï¼Œ å®ƒä¼šåœ¨`train_iter`è®¿é—®åˆ°çš„è®­ç»ƒæ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªæ¨¡å‹`net`ã€‚
- è¯¥è®­ç»ƒå‡½æ•°å°†ä¼šè¿è¡Œå¤šä¸ªè¿­ä»£å‘¨æœŸï¼ˆç”±`num_epochs`æŒ‡å®šï¼‰ã€‚ åœ¨æ¯ä¸ªè¿­ä»£å‘¨æœŸç»“æŸæ—¶ï¼Œåˆ©ç”¨`test_iter`è®¿é—®åˆ°çš„æµ‹è¯•æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚
- åˆ©ç”¨`Animator`ç±»æ¥å¯è§†åŒ–è®­ç»ƒè¿›åº¦

```python
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  # @save  
    """è®­ç»ƒæ¨¡å‹ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train_loss', 'train_acc', 'test_acc'])  # ç»˜åˆ¶åŠ¨ç”»  
    for epoch in range(num_epochs):  # è®­ç»ƒnum_epochsä¸ªè¿­ä»£å‘¨æœŸ  
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)  # è®­ç»ƒä¸€ä¸ªè¿­ä»£å‘¨æœŸ  
        test_acc = evaluate_accuracy(net, test_iter)  # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹  
        animator.add(epoch + 1, train_metrics + (test_acc,))  # å‘åŠ¨ç”»ä¸­æ·»åŠ æ•°æ®  
        train_loss, train_acc = train_metrics
        print(f'num_epochs {epoch + 1}, train_loss {train_loss:.3f}, train_acc {train_acc:.3f}')
        # num_epochs 1, train_loss 0.786, train_acc 0.751  
        # num_epochs 2, train_loss 0.569, train_acc 0.813        
        # num_epochs 3, train_loss 0.524, train_acc 0.826        
        # num_epochs 4, train_loss 0.500, train_acc 0.833        
        # num_epochs 5, train_loss 0.486, train_acc 0.836        
        # num_epochs 6, train_loss 0.474, train_acc 0.840        
        # num_epochs 7, train_loss 0.465, train_acc 0.842        
        # num_epochs 8, train_loss 0.458, train_acc 0.846        
        # num_epochs 9, train_loss 0.453, train_acc 0.847        
        # num_epochs 10, train_loss 0.446, train_acc 0.849    
        assert train_loss < 0.5, train_loss  # è®­ç»ƒæŸå¤±ä¸åº”è¶…è¿‡0.5  
    assert train_acc <= 1 and train_acc > 0.7, train_acc  # è®­ç»ƒç²¾åº¦åº”ä»‹äº0.7å’Œ1ä¹‹é—´  
    assert test_acc <= 1 and test_acc > 0.7, test_acc  # æµ‹è¯•ç²¾åº¦åº”ä»‹äº0.7å’Œ1ä¹‹é—´
```

- ä¼˜åŒ–ç®—æ³•ï¼Œå°æ‰¹é‡æ¢¯åº¦ä¸‹é™

```python
lr = 0.1  # å­¦ä¹ ç‡  


# ä¼˜åŒ–å‡½æ•°  
def updater(batch_size):
    return d2l.sgd([W, b], lr, batch_size)  # ä½¿ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™
```

- è®­ç»ƒæ¨¡å‹10ä¸ªè¿­ä»£å‘¨æœŸ
    - è¿­ä»£å‘¨æœŸï¼ˆ`num_epochs`ï¼‰å’Œå­¦ä¹ ç‡ï¼ˆ`lr`ï¼‰éƒ½æ˜¯å¯è°ƒèŠ‚çš„è¶…å‚æ•°ã€‚ é€šè¿‡æ›´æ”¹å®ƒä»¬çš„å€¼ï¼Œæˆ‘ä»¬å¯ä»¥æé«˜æ¨¡å‹çš„åˆ†ç±»ç²¾åº¦

```python
num_epochs = 10
train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
```

- ![[00 Attachments/Figure_3.png|400]]

### é¢„æµ‹

- è®­ç»ƒå·²ç»å®Œæˆï¼Œå·²ç»å‡†å¤‡å¥½å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»é¢„æµ‹ã€‚
    - ç»™å®šä¸€ç³»åˆ—å›¾åƒï¼Œæ¯”è¾ƒå®ƒä»¬çš„å®é™…æ ‡ç­¾ï¼ˆæ–‡æœ¬è¾“å‡ºçš„ç¬¬ä¸€è¡Œï¼‰å’Œæ¨¡å‹é¢„æµ‹ï¼ˆæ–‡æœ¬è¾“å‡ºçš„ç¬¬äºŒè¡Œï¼‰

```python
def predict_ch3(net, test_iter, n=10):  # @save  
    """é¢„æµ‹æ ‡ç­¾ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    for X, y in test_iter:  # æµ‹è¯•æ•°æ®é›†  
        break
    trues = d2l.get_fashion_mnist_labels(y)  # è·å¾—çœŸå®æ ‡ç­¾  
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))  # è·å¾—é¢„æµ‹æ ‡ç­¾  
    titles = [true + '\n' + pred for true, pred in zip(trues, preds)]
    d2l.show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])


predict_ch3(net, test_iter)
```

- ![[00 Attachments/Figure_4.png]]

## Softmax å›å½’ç®€æ´ç®—æ³•

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

#### åˆå§‹åŒ–æ¨¡å‹å‚æ•°

- softmaxå›å½’çš„è¾“å‡ºå±‚æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚ã€‚ å› æ­¤ï¼Œä¸ºäº†å®ç°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œ æˆ‘ä»¬åªéœ€åœ¨`Sequential`ä¸­æ·»åŠ ä¸€ä¸ªå¸¦æœ‰10ä¸ªè¾“å‡ºçš„å…¨è¿æ¥å±‚ã€‚
    - åŒæ ·ï¼Œåœ¨è¿™é‡Œ`Sequential`å¹¶ä¸æ˜¯å¿…è¦çš„ï¼Œ ä½†å®ƒæ˜¯å®ç°æ·±åº¦æ¨¡å‹çš„åŸºç¡€
- æˆ‘ä»¬ä»ç„¶ä»¥å‡å€¼0å’Œæ ‡å‡†å·®0.01éšæœºåˆå§‹åŒ–æƒé‡

```python
# åˆå§‹åŒ–æ¨¡å‹å‚æ•°  
# PyTorchä¸ä¼šéšå¼åœ°è°ƒæ•´è¾“å…¥çš„å½¢çŠ¶ã€‚å› æ­¤ï¼Œ  
# æˆ‘ä»¬åœ¨çº¿æ€§å±‚å‰å®šä¹‰äº†å±•å¹³å±‚ï¼ˆflattenï¼‰ï¼Œæ¥è°ƒæ•´ç½‘ç»œè¾“å…¥çš„å½¢çŠ¶ ä¿å­˜ç¬¬ 0 ç»´ å…¶ä»–ç»´åº¦å±•æˆ 1 ç»´å¼ é‡
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))  # sequential å°†å¤šä¸ªå±‚ç»„åˆæˆä¸€ä¸ªç½‘ç»œ flatternå°†è¾“å…¥æ•°æ®å±•å¹³ 


def init_weights(m):
    if type(m) == nn.Linear:  # åˆ¤æ–­ m æ˜¯å¦ä¸ºçº¿æ€§å±‚  
        nn.init.normal_(m.weight, std=0.01)  # ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡  


net.apply(init_weights)  # åº”ç”¨åˆå§‹åŒ–å‡½æ•°
```

#### é‡æ–°å®¡è§† Softmax çš„å®ç°

- åœ¨ä¸ŠèŠ‚ä¸­ï¼Œè®¡ç®—äº†æ¨¡å‹çš„è¾“å‡ºï¼Œç„¶åå°†è¾“å‡ºé€å…¥äº¤å‰ç†µæŸå¤±ã€‚ä»æ•°å­¦ä¸Šå°†ï¼Œè¿™æ˜¯ä¸€ä»¶å®Œå…¨åˆç†çš„äº‹æƒ…
- ç„¶è€Œï¼Œä»è®¡ç®—è§’åº¦çœ‹ï¼ŒæŒ‡æ•°å¯èƒ½ä¼šé€ æˆæ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼ˆæº¢å‡ºï¼‰
    - $$\hat y_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$$
- åœ¨è®¡ç®— softmax ä¹‹å‰ï¼Œå…ˆä»æ‰€æœ‰çš„ $O_k$ ä¸­å‡å» $max(O_k)$ã€‚
    - æå‡ºæœ€å¤§é¡¹$$\begin{split}\begin{aligned}
      \hat y_j & = \frac{\exp(o_j - \max(o_k))\exp(\max(o_k))}{\sum_k \exp(o_k - \max(o_k))\exp(\max(o_k))} \\
      & = \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}.
      \end{aligned}\end{split}$$
- åœ¨å‡æ³•å’Œè§„èŒƒåŒ–æ­¥éª¤ä¹‹åï¼Œå¯èƒ½æœ‰äº› $ğ‘œ_ğ‘—âˆ’max(ğ‘œ_ğ‘˜)$ å…·æœ‰è¾ƒå¤§çš„è´Ÿå€¼ï¼ˆå¯¼è‡´æŒ‡æ•°è¿ç®—æº¢å‡ºï¼‰
- å°½ç®¡æˆ‘ä»¬è¦è®¡ç®—æŒ‡æ•°å‡½æ•°ï¼Œä½†æˆ‘ä»¬æœ€ç»ˆåœ¨è®¡ç®—äº¤å‰ç†µæŸå¤±æ—¶ä¼šå–å®ƒä»¬çš„å¯¹æ•°
- æ±‚å¯¹æ•°ä»¥æŠµæ¶ˆæ±‚æŒ‡æ•°$$\begin{split}\begin{aligned}
  \log{(\hat y_j)} & = \log\left( \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}\right) \\
  & = \log{(\exp(o_j - \max(o_k)))}-\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)} \\
  & = o_j - \max(o_k) -\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)}.
  \end{aligned}\end{split}$$
- å¸Œæœ›ä¿ç•™ä¼ ç»Ÿçš„ softmax å‡½æ•°ï¼Œä»¥å¤‡æˆ‘ä»¬éœ€è¦è¯„ä¼°é€šè¿‡æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡ã€‚ ä½†æ˜¯ï¼Œæˆ‘ä»¬æ²¡æœ‰å°† softmax æ¦‚ç‡ä¼ é€’åˆ°æŸå¤±å‡½æ•°ä¸­ï¼Œ
  è€Œæ˜¯ç›´æ¥åœ¨äº¤å‰ç†µæŸå¤±å‡½æ•°ä¸­ä¼ é€’æœªè§„èŒƒåŒ–çš„é¢„æµ‹ï¼Œå¹¶åŒæ—¶è®¡ç®— softmax åŠå…¶å¯¹æ•°

```python
loss = nn.CrossEntropyLoss(reduction='none')
```

#### ä¼˜åŒ–ç®—æ³•

```python
trainer = torch.optim.SGD(net.parameters(), lr=0.1)
```

#### è®­ç»ƒ

```python
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

- ![[00 Attachments/Figure_5.png]]
  ;

# 3.3ã€å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰

## 3.3.1ã€å•å±‚æ„ŸçŸ¥æœº

### å•å±‚æ„ŸçŸ¥æœº

- é€šè¿‡æ”¾å°„å‡½æ•°å°†è¾“å…¥æ˜ å°„åˆ°è¾“å‡ºï¼Œç„¶ååŠ å…¥ä¸€ä¸ªæ¿€æ´»å‡½æ•°![[00 Attachments/Pasted image 20240516153736.png|500]]
- æ¿€æ´»å‡½æ•°å¯è‡ªå®šä¹‰![[00 Attachments/Pasted image 20240516154324.png|500]]
    - ç›¸è¾ƒäºçº¿æ€§å›å½’ï¼šçº¿æ€§å›å½’è¾“å‡ºä¸ºå®æ•°ï¼›è€Œæ„ŸçŸ¥æœºè¾“å‡ºä¸ºç¦»æ•£çš„ç±»
    - ç›¸è¾ƒäº softmaxï¼šsoftmax å¦‚æœæœ‰ n ä¸ªç±»ï¼Œå°±ä¼šè¾“å‡º n ä¸ªå…ƒç´ ï¼ˆå¯¹åº”ç±»çš„æ¦‚ç‡ ï¼‰ï¼Œæ˜¯ä¸€ä¸ªå¤šåˆ†ç±»çš„é—®é¢˜ï¼›è€Œæ„ŸçŸ¥æœºåªèƒ½è¿›è¡ŒäºŒåˆ†ç±»

### è®­ç»ƒæ„ŸçŸ¥æœº

- ![[00 Attachments/Pasted image 20240516211254.png|500]]
    - å¦‚æœåˆ†ç±»æ­£ç¡®çš„è¯ $y_i<w,x_i>$ ä¸ºæ­£æ•°ï¼Œåˆ™æŸå¤±ä¸º 0ï¼Œåˆ™æ¢¯åº¦ä¸è¿›è¡Œæ›´æ–°
    - å¦‚æœåˆ†ç±»é”™è¯¯çš„è¯ $y_i<w,x_i>$ ä¸ºè´Ÿæ•°ï¼Œåˆ™æŸå¤±ä¸º $-y_i<w,x_i>$ï¼Œè¿›è¡Œæ¢¯åº¦æ›´æ–°
        - å­¦ä¹ ç‡ä¸º 1ï¼Œ$y_ix_i$ ä¸ºæ¢¯åº¦
    - é‡å¤ä¸Šè¿°è¿‡ç¨‹ç›´è‡³==æ‰€æœ‰åˆ†ç±»æ­£ç¡®==ï¼ˆåœæ­¢æ¡ä»¶ï¼‰
-

æ¨¡å‹æ›´æ–°ç¤ºæ„å›¾ï¼Œé»‘çº¿å³å¯è§†ä¸ºæƒé‡![[00 Attachments/Pasted image 20240516213531.png|200]]![[00 Attachments/Pasted image 20240516213550.png|200]]![[00 Attachments/Pasted image 20240516213613.png|200]]![[00 Attachments/Pasted image 20240516214332.png|200]]

### æ”¶æ•›å®šç†

- åœæ­¢æ¡ä»¶ï¼Œå¯¹æ‰€æœ‰çš„ç±»éƒ½åˆ†ç±»æ­£ç¡®![[00 Attachments/Pasted image 20240516214634.png|500]]
    - å‡è®¾æ•°æ®éƒ½åœ¨åŠå¾„ä¸º r çš„ä¸€ä¸ªåŒºåŸŸå†…
    - æ‰€æœ‰åˆ†ç±»å‡åˆ†ç±»æ­£ç¡®ï¼Œå¤§äº 0ï¼Œä¸”è¿˜ä¿ç•™ä¸€å®šä½™é‡ $y(\mathbf{X} ^T\mathbf{W} +b)\ge \rho$
      ï¼Œä¸”ä¿è¯åœ¨ $\left \| \mathbf{w} \right \|^2 +b^2 \le 1$ ä¹‹åæ”¶æ•›
    - $\rho$ è¶Šå¤§ï¼ˆå®½æ¾ï¼‰ï¼Œåˆ™æ”¶æ•›çš„æ­¥æ•°è¶Šå°‘

### XOR é—®é¢˜

- å¼‚æˆ–ï¼ˆç›¸åŒä¸º 0ï¼Œä¸åŒä¸º 1ï¼‰![[00 Attachments/Pasted image 20240516220625.png|500]]
    - å•å±‚æ„ŸçŸ¥æœºä¸èƒ½å¾ˆå¥½çš„åˆ†ç±»

### æ€»ç»“

- ![[00 Attachments/Pasted image 20240516221057.png|500]]

## 3.3.2ã€å¤šå±‚æ„ŸçŸ¥æœº

- Multilayer Perceptron, MLP

### å­¦ä¹ XOR

-

å…ˆç”¨è“è‰²çš„çº¿åˆ†ï¼Œå†ç”¨é»„è‰²çš„çº¿åˆ†ã€‚å†å¯¹è“è‰²çš„çº¿å’Œé»„è‰²çš„çº¿åˆ†å‡ºæ¥çš„ç»“æœåšåŒæˆ–![[00 Attachments/Pasted image 20240516221148.png|500]]

- ä¸€å±‚é€‰æ‹©ä¸äº†ï¼Œå°±åˆ†å¤šå±‚

### å•å±‚éšè—å±‚

- ![[00 Attachments/Pasted image 20240516223159.png|500]]
    - è¾“å…¥æ•°æ®å¤§å°å½±å“å› ç´ ä¸å¯æ›´æ”¹ï¼Œè¾“å‡ºä¸ªæ•°ç”±ç±»åˆ«ä¸ªæ•°å†³å®šï¼Œä½†å¯ä»¥è®¾ç½®éšè—å±‚çš„å¤§å°
- è¿™é‡Œåªæœ‰ä¸€ä¸ªè¾“å‡º![[00 Attachments/Pasted image 20240516224352.png|500]]
    - é—®ä»€ä¹ˆæ¿€æ´»å‡½æ•°ä¸€å®šè¦éçº¿æ€§
        - è‹¥æ¿€æ´»å‡½æ•°ä¸ºçº¿æ€§ï¼Œåˆ™ç­‰ä»·äºå•å±‚çº¿æ€§æ¨¡å‹ $o = \mathbf{w} _2^T\mathbf{W} _1\mathbf{x} +b^{'}$ï¼ˆçº¿æ€§å‡½æ•°ä¸çº¿æ€§å‡½æ•°çš„å¤åˆä¾ç„¶ä¸ºçº¿æ€§å‡½æ•°ï¼‰

### æ¿€æ´»å‡½æ•°

- ç”±äºé˜¶è·ƒå‡½æ•°éè¿ç»­ï¼Œä¸æ–¹ä¾¿æ±‚å¯¼
- Sigmoid å‡½æ•°![[00 Attachments/Pasted image 20240516225357.png|500]]
- Tanh å‡½æ•°![[00 Attachments/Pasted image 20240516225445.png|500]]
- ReLU![[00 Attachments/Pasted image 20240516225500.png|500]]
    - ReLUçš„å¥½å¤„åœ¨äºä¸éœ€è¦æ‰§è¡ŒæŒ‡æ•°è¿ç®—ï¼ˆåœ¨CPUä¸­ï¼Œä¸€æ¬¡æŒ‡æ•°è¿ç®—ç­‰ä»·äº100æ¬¡ä¹˜æ³•è¿ç®—ï¼Œåœ¨GPUä¸­æœ‰ç‰¹å®šçš„å•å…ƒï¼Œä½†ä¾ç„¶è´¹æ—¶ï¼‰

### å¤šç±»åˆ†ç±»

- æ²¡æœ‰éšè—å±‚ï¼Œå³ä¸º Softmax å›å½’ï¼›åŠ äº†éšè—å±‚ï¼Œå³ä¸ºå¤šå±‚æ„ŸçŸ¥æœº![[00 Attachments/Pasted image 20240516231806.png|500]]
- ä¸ä¹‹å‰å•è¾“å‡ºç›¸æ¯”ï¼Œè¾“å‡ºå±‚å¤§å°ä¸ºç±»åˆ«ä¸ªæ•°![[00 Attachments/Pasted image 20240516232158.png|500]]
    - å¯¹è¾“å‡ºè¦è¿›è¡Œä¸€æ¬¡ softmaxï¼Œä»¥ç¬¦åˆæ¦‚ç‡æ¡ä»¶

### å¤šéšè—å±‚

- éšè—å±‚å¯ä»¥å»ºå¾—æ›´å¤š![[00 Attachments/Pasted image 20240516232418.png|500]]
    - æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œä¼šå¯¼è‡´å±‚æ•°å¡Œé™·ï¼Œå˜æˆå•å±‚æ„ŸçŸ¥æœºï¼Œä¸èƒ½è§£å†³ XOR é—®é¢˜
    - å±‚æ•°åšæ·±ï¼Œä¸ºäº†ä½¿ä¿¡æ¯é‡é€å±‚å‡å°‘ï¼ˆä¸€æ¬¡å‹å¤šäº†å¯èƒ½ä¼šå‡å°‘å¾ˆå¤šä¿¡æ¯ï¼‰

### æ€»ç»“

- ä½¿ç”¨éšè—å±‚å’Œæ¿€æ´»å‡½æ•°ç±»çš„åˆ°éçº¿æ€§æ¨¡å‹ï¼Œè§£å†³äº†å•å±‚æ„ŸçŸ¥æœºä¸èƒ½è§£å†³ xor çš„å±€é™æ€§
- å¸¸ç”¨æ¿€æ´»å‡½æ•°æ˜¯ Sigmoidï¼ŒTanhï¼ŒReLU
    - ReLU ç”±äºå®ç°ç®€å•ï¼Œä»è€Œæ›´å¸¸ç”¨
- ä½¿ç”¨ Softmax æ¥å¤„ç†å¤šç±»åˆ†ç±»
    - åœ¨ Softmax å›å½’ä¸­åŠ å…¥äº† éšè—å±‚
- è‰å‚æ•°ä¸ºéšè—å±‚æ•°å’Œå„ä¸ªéšè—å±‚å¤§å°

## 3.3.3ã€å¤šå±‚æ„ŸçŸ¥çš„ä»é›¶å¼€å§‹å®ç°

- ä¸ºäº†ä¸ä¹‹å‰ softmax å›å½’è·å¾—çš„ç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œ æˆ‘ä»¬å°†ç»§ç»­ä½¿ç”¨ Fashion-MNIST å›¾åƒåˆ†ç±»æ•°æ®é›†

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

### åˆå§‹åŒ–æ¨¡å‹å‚æ•°

- Fashion-MNIST ä¸­çš„æ¯ä¸ªå›¾åƒç”± 28Ã—28=784 ä¸ªç°åº¦åƒç´ å€¼ç»„æˆã€‚æ‰€æœ‰å›¾åƒå…±åˆ†ä¸º 10 ä¸ªç±»åˆ«ã€‚ å¿½ç•¥åƒç´ ä¹‹é—´çš„ç©ºé—´ç»“æ„ï¼Œ
  å¯ä»¥å°†æ¯ä¸ªå›¾åƒè§†ä¸ºå…·æœ‰784ä¸ªè¾“å…¥ç‰¹å¾ å’Œ10ä¸ªç±»çš„ç®€å•åˆ†ç±»æ•°æ®é›†
- é¦–å…ˆï¼Œå°†å®ç°ä¸€ä¸ªå…·æœ‰å•éšè—å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºï¼Œ å®ƒåŒ…å« 256 ä¸ªéšè—å•å…ƒã€‚
    - æ³¨æ„ï¼Œå¯ä»¥å°†è¿™ä¸¤ä¸ªå˜é‡éƒ½è§†ä¸ºè¶…å‚æ•°ã€‚ é€šå¸¸ï¼Œæˆ‘ä»¬é€‰æ‹© 2 çš„è‹¥å¹²æ¬¡å¹‚ä½œä¸ºå±‚çš„å®½åº¦ã€‚ å› ä¸ºå†…å­˜åœ¨ç¡¬ä»¶ä¸­çš„åˆ†é…å’Œå¯»å€æ–¹å¼ï¼Œè¿™ä¹ˆåšå¾€å¾€å¯ä»¥åœ¨è®¡ç®—ä¸Šæ›´é«˜æ•ˆ
- ç”¨å‡ ä¸ªå¼ é‡æ¥è¡¨ç¤ºå‚æ•°
    - æ³¨æ„ï¼Œå¯¹äºæ¯ä¸€å±‚éƒ½è¦è®°å½•ä¸€ä¸ªæƒé‡çŸ©é˜µå’Œä¸€ä¸ªåç½®å‘é‡ã€‚ è·Ÿä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬è¦ä¸ºæŸå¤±å…³äºè¿™äº›å‚æ•°çš„æ¢¯åº¦åˆ†é…å†…å­˜
- å‚æ•°åˆå§‹åŒ–ä¸ºéšæœºï¼šå¦‚æœæ˜¯å…¨éƒ¨è®¾ç½®ä¸º0çš„è¯ï¼Œæ¯ä¸€ä¸ªç¥ç»å…ƒçš„è¾“å‡ºéƒ½æ˜¯ç›¸åŒçš„ï¼Œé‚£ä¸å°±æ˜¯å¦‚åŒä¸€ä¸ªç¥ç»å…ƒäº†å˜›ï¼Œå¤šä¸ªç¥ç»å…ƒçš„ç‰¹æ€§å°±æ²¡æœ‰äº†
- è®¾ç½®ä¸ºé›¶çš„è¯æ¢¯åº¦ä¸º0ï¼Œå‚æ•°ä¸ä¼šæ›´æ–°ï¼Œç›¸å½“äºéšè—å±‚åªæœ‰ä¸€ä¸ªå•å…ƒ

```python
num_inputs, num_outputs, num_hiddens = 784, 10, 256  # num_inputs:è¾“å…¥ç‰¹å¾æ•°ï¼Œnum_outputs:è¾“å‡ºç‰¹å¾æ•°ï¼Œnum_hiddens:éšè—å±‚ç‰¹å¾æ•°  

W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)  # æƒé‡å‚æ•°åˆå§‹åŒ– è¡Œæ•°ä¸ºè¾“å…¥ç‰¹å¾æ•°ï¼Œåˆ—æ•°ä¸ºéšè—å±‚ç‰¹å¾æ•°  
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))  # åç½®å‚æ•°åˆå§‹åŒ–  
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)  # æƒé‡å‚æ•°åˆå§‹åŒ– è¡Œæ•°ä¸ºéšè—å±‚ç‰¹å¾æ•°ï¼Œåˆ—æ•°ä¸ºè¾“å‡ºç‰¹å¾æ•°  
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))  # åç½®å‚æ•°åˆå§‹åŒ–  

params = [W1, b1, W2, b2]  # æ¨¡å‹å‚æ•°åˆ—è¡¨
```

### å®ç° ReLU æ¿€æ´»å‡½æ•°

```python
def relu(X):
    a = torch.zeros_like(X)  # æ–°å»ºä¸€ä¸ªå…¨é›¶å¼ é‡ï¼Œå½¢çŠ¶å’Œ X ç›¸åŒ  
    return torch.max(X, a)  # é€‰æ‹© X å’Œ å…¨é›¶å¼ é‡ä¸­çš„æœ€å¤§å€¼ä½œä¸ºè¾“å‡º
```

### å®ç°æ¨¡å‹

- å› ä¸ºæˆ‘ä»¬å¿½ç•¥äº†ç©ºé—´ç»“æ„ï¼Œ æ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨`reshape`å°†æ¯ä¸ªäºŒç»´å›¾åƒè½¬æ¢ä¸ºä¸€ä¸ªé•¿åº¦ä¸º`num_inputs`çš„å‘é‡

```python
def net(X):
    X = X.reshape((-1, num_inputs))  # å°†å›¾åƒå±•å¹³ æ”¹å˜ X çš„å½¢çŠ¶ä¸º (batch_size, num_inputs) æ ·æœ¬æŒ‰è¡Œæ’åˆ—  
    H = relu(X @ W1 + b1)  # éšè—å±‚è¾“å‡º X @ W1 ç­‰ä»·äº torch.matmul(X, W1)    return H @ W2 + b2  # è¾“å‡ºå±‚è¾“å‡º
```

### æŸå¤±å‡½æ•°

- ç”±äºæˆ‘ä»¬å·²ç»ä»é›¶å®ç°è¿‡ softmax å‡½æ•°ï¼Œ å› æ­¤åœ¨è¿™é‡Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨é«˜çº§APIä¸­çš„å†…ç½®å‡½æ•°æ¥è®¡ç®—softmaxå’Œäº¤å‰ç†µæŸå¤±

```python
loss = nn.CrossEntropyLoss(reduction='none')  # reduction='none' è¡¨ç¤ºè¿”å›æ¯ä¸ªæ ·æœ¬çš„æŸå¤±å€¼
```

### è®­ç»ƒ

- å¤šå±‚æ„ŸçŸ¥æœºçš„è®­ç»ƒè¿‡ç¨‹ä¸ softmax å›å½’çš„è®­ç»ƒè¿‡ç¨‹å®Œå…¨ç›¸åŒã€‚ å¯ä»¥ç›´æ¥è°ƒç”¨`d2l`åŒ…çš„`train_ch3`å‡½æ•°ï¼Œå°†è¿­ä»£å‘¨æœŸæ•°è®¾ç½®ä¸º
  10ï¼Œå¹¶å°†å­¦ä¹ ç‡è®¾ç½®ä¸º 0.1

```python
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)  # ä¼˜åŒ–å™¨  
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
```

- ![[00 Attachments/Figure_6.png|400]]
    - ä¸ Softmax å›å½’ç›¸æ¯”ï¼ŒæŸå¤±å¾€ä¸‹é™äº†ï¼Œä½†ç²¾åº¦å¥½åƒæ²¡æœ‰æ˜æ˜¾çš„å˜åŒ–
- æµ‹è¯•

```python
d2l.predict_ch3(net, test_iter)
```

- ![[00 Attachments/Figure_7.png|400]]

## 3.3.4ã€å¤šå±‚æ„ŸçŸ¥æœºçš„ç®€æ´å®ç°

- ä¸ softmax å›å½’çš„ç®€æ´å®ç°ç›¸æ¯”ï¼Œ å”¯ä¸€çš„åŒºåˆ«æ˜¯æ·»åŠ äº† 2 ä¸ªå…¨è¿æ¥å±‚ï¼ˆä¹‹å‰æˆ‘ä»¬åªæ·»åŠ äº†1ä¸ªå…¨è¿æ¥å±‚ï¼‰
    - ç¬¬ä¸€å±‚æ˜¯éšè—å±‚ï¼Œå®ƒåŒ…å«256ä¸ªéšè—å•å…ƒï¼Œå¹¶ä½¿ç”¨äº†ReLUæ¿€æ´»å‡½æ•°
    - ç¬¬äºŒå±‚æ˜¯è¾“å‡ºå±‚

```python
import torch
from torch import nn
from d2l import torch as d2l

# åˆå§‹åŒ–æ¨¡å‹å‚æ•°  
# PyTorchä¸ä¼šéšå¼åœ°è°ƒæ•´è¾“å…¥çš„å½¢çŠ¶ã€‚å› æ­¤ï¼Œ  
# æˆ‘ä»¬åœ¨çº¿æ€§å±‚å‰å®šä¹‰äº†å±•å¹³å±‚ï¼ˆflattenï¼‰ï¼Œæ¥è°ƒæ•´ç½‘ç»œè¾“å…¥çš„å½¢çŠ¶ ä¿å­˜ç¬¬ 0 ç»´ å…¶ä»–ç»´åº¦å±•æˆ 1 ç»´å¼ é‡  
net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))  # sequential å°†å¤šä¸ªå±‚ç»„åˆæˆä¸€ä¸ªç½‘ç»œ flatternå°†è¾“å…¥æ•°æ®å±•å¹³  


def init_weights(m):
    if type(m) == nn.Linear:  # åˆ¤æ–­ m æ˜¯å¦ä¸ºçº¿æ€§å±‚  
        nn.init.normal_(m.weight, std=0.01)  # ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡  


net.apply(init_weights)  # åº”ç”¨åˆå§‹åŒ–å‡½æ•°  

batch_size, lr, num_epochs = 256, 0.1, 10

# å®šä¹‰äº¤å‰ç†µæŸå¤±å‡½æ•°  
loss = nn.CrossEntropyLoss(reduction='none')  # å®šä¹‰æŸå¤±å‡½æ•°ï¼Œreduction='none'è¡¨ç¤ºè¿”å›æ¯ä¸ªæ ·æœ¬çš„æŸå¤±å€¼  

# å®šä¹‰ä¼˜åŒ–ç®—æ³•  
trainer = torch.optim.SGD(net.parameters(), lr=lr)

# åŠ è½½æ•°æ®é›†  
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

# è®­ç»ƒæ¨¡å‹  
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

- ![[00 Attachments/Figure_8.png|400]]
