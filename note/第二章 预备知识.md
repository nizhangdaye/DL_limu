```toc
```

# 2.1 数据操作

- 深度学习存储和操作数据的主要接口是张量（𝑛维数组）。

## 2.1.1 入门

- 张量（tensor）：n 维数组
- 张量中的每个值称为张量的元素（element）

```python
import torch

x = torch.arange(12)  # 创建一维张量，轴长12  
print(x)
# tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])  
print(x.shape)  # 访问张量的形状，每个轴的长度  
# torch.Size([12])
print(x.numel())  # 访问张量中元素的总数  
# 12  
X = x.reshape(3, 4)  # 张量形状改变，但元素值不变  
# X = x.reshape(-1, 4) # 自动推断出维度，-1表示其他维度由其他元素决定  
# X = x.reshape(3, -1) # 自动推断出维度，-1表示其他维度由其他元素决定  
print(X)
# tensor([[ 0,  1,  2,  3],  
#         [ 4,  5,  6,  7],  
#         [ 8,  9, 10, 11]])  
print(torch.zeros(2, 3, 4))  # 创建全零张量  
# tensor([[[0., 0., 0., 0.],  
#          [0., 0., 0., 0.],  
#          [0., 0., 0., 0.]],  
#         [[0., 0., 0., 0.],  
#          [0., 0., 0., 0.],  
#          [0., 0., 0., 0.]]])  
print(torch.ones(2, 3, 4))  # 创建全一张量  
# tensor([[[1., 1., 1., 1.],  
#          [1., 1., 1., 1.],  
#          [1., 1., 1., 1.]],  
#         [[1., 1., 1., 1.],  
#          [1., 1., 1., 1.],  
#          [1., 1., 1., 1.]]])  
print(torch.rand(3, 4))  # 创建随机张量  
# tensor([[0.4100, 0.1523, 0.3693, 0.1623],  
#         [0.1415, 0.7182, 0.8818, 0.9743],  
#         [0.5449, 0.5448, 0.9258, 0.1914]])  
print(torch.randn(3, 4))  # 创建服从正态分布的随机张量，其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。  
# tensor([[ 0.3823, -0.2671,  0.1212, -0.1859],  
#         [-0.1073,  0.6807,  0.6082,  0.2897],  
#         [ 0.2676,  0.2675,  0.7629, -0.0456]])  
print(torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]))  # 通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值  
# tensor([[2, 1, 4, 3],  
#         [1, 2, 3, 4],  
#         [4, 3, 2, 1]])
```

## 2.1.2 运算符

- 常见的标准算术运算符(+、-、\*、/、和 \*\*)都可以被升级为按元素运算。
- 按元素运算
    - 将标准标量运算符应用于数组的每个元素。
    - $c_i \gets f(u_i, v_i)$
  ```python
  x = torch.tensor([1.0, 2, 4, 8])  
  y = torch.tensor([2, 2, 2, 2])  
  print(x + y, x - y, x * y, x / y, x ** y, sep="\n")  # 标准算术运算符（+、-、*、/和**）  
  # tensor([ 3.,  4., 16., 32.])  
  # tensor([-1.,  0.,  2.,  4.])  
  # tensor([ 2.,  4., 16., 64.])  
  # tensor([0.5000, 1.0000, 2.0000, 4.0000])  
  print(torch.exp(x))  # 计算自然对数的指数函数  
  # tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])
  ```
- 张量拼接
  ```python
  X = torch.arange(12, dtype=torch.float32).reshape((3,4))  
  Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])  
  print(torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1), sep="\n")  # 连接两个张量 dim=0 表示在行方向上连接，dim=1 表示在列方向上连接  
  # tensor([[ 0.,  1.,  2.,  3.],  
  #         [ 4.,  5.,  6.,  7.],  
  #         [ 8.,  9., 10., 11.],  
  #         [ 2.,  1.,  4.,  3.],  
  #         [ 1.,  2.,  3.,  4.],  
  #         [ 4.,  3.,  2.,  1.]])  
  # tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],  
  #         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],  
  #         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])
  ```
- 通过逻辑运算符构建布尔张量
  ```python
  print(X == y)  # 按元素比较两个张量，并返回一个新的布尔张量，其中每个元素表示两个输入张量中对应的元素是否相等  
  # tensor([[False,  True, False, False],  
  #         [ True, False, False, False],  
  #         [False, False, False,  True]])
  ```
- 张量所有元素求和
  ```python
  X.sum()  会产生一个单元素张量
  # tenosr(66.)
  ```

## 2.1.3 广播机制

- 以上都是在向量形状相同的条件下进行的，但是某些情况下，依然可以调用==广播机制==来执行按元素操作
    1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
    2. 对生成的数组执行按元素操作。
  ```python
  a = torch.arange(3).reshape((3, 1))  # 创建一维张量  
  b = torch.arange(2).reshape((1, 2))  # 创建一维张量  
  print(a, b, sep="\n")  # 打印张量a和b  
  # tensor([[0],  
  #         [1],  
  #         [2]])  
  # tensor([[0, 1]])  
  print(a + b)  # 由于向量形状不同，需要先将向量变形成相同的形状，矩阵a将复制列，矩阵b将复制行，然后再按元素相加。  
  # tensor([[0, 1],  
  #         [1, 2],  
  #         [2, 3]])
  ```

## 2.1.4 索引和切片

- 与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。

```python
print(X[-1], X[1:3], sep="\n")  # 访问张量的元素，支持负数索引，切片操作  
# tensor([ 8.,  9., 10., 11.])  
# tensor([[ 4.,  5.,  6.,  7.],  
#         [ 8.,  9., 10., 11.]])  
X[1, 2] = 9  # 指定索引修改张量元素  
print(X)
# tensor([[ 0.,  1.,  2.,  3.],  
#         [ 4.,  5.,  9.,  7.],  
#         [ 8.,  9., 10., 11.]])  
X[0:2, :] = 12  # 为多个元素赋相同的值。访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。  
print(X)
# tensor([[12., 12., 12., 12.],  
#         [12., 12., 12., 12.],  
#         [ 8.,  9., 10., 11.]])
```

## 3.1.5 节省内存

- 一些操作会重新分配内存。
    - 如果我们用`Y = X + Y`，我们将取消引用`Y`指向的张量，而是指向新分配的内存处的张量。
- 但要避免重新分配地址
    1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
    2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。
- 使用切片表示法将操作的结果分配给先前分配的数组，例如`Y[:] = <expression>`
  ```python
  Z = torch.zeros_like(Y)  # 创建与Y张量形状相同的全零张量  
  print(f"id(Z): {id(Z)}")  # 打印张量Z的内存地址  
  # id(Z): 140415247282880  
  Z[:] = X + Y  # 将X和Y张量中对应元素相加，并赋值给Z张量  
  print(f"id(Z): {id(Z)}")    # 打印张量Z的内存地址  
  # id(Z): 140415247282880     # 内存地址没有改变，说明Z张量是共享内存的。
  ```
- 如果在后续计算中没有重复使用`X`， 也可以使用`X[:] = X + Y`或`X += Y`来减少操作的内存开销。
  ```python
  before = id(X)  # 记录X张量的内存地址  
  X += Y  # 将X张量中对应元素相加，并赋值给X张量  
  print(before == id(X))  
  # True  # 内存地址没有改变，说明X张量是共享内存的。
  ```

## 2.1.6 转换为其他python对象

- 将深度学习框架定义的张量转换为NumPy张量（`ndarray`）很容易，反之也同样容易。
- torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。
  ```python
  A = X.numpy()  # 将张量X转换为NumPy数组  
  B = torch.tensor(A)  # 将NumPy数组转换为张量  
  print(type(A), type(B))  # 打印NumPy数组和张量的类型  
  # <class 'numpy.ndarray'> <class 'torch.Tensor'>
  ```
- 要将大小为1的张量转换为Python标量，我们可以调用`item`函数或Python的内置函数。
  ```python
  a = torch.tensor([3.5])  # 创建一维张量  
  print(a, a.item(), int(a), float(a), sep=", ")  # 打印张量a，item()方法可以将张量转换为Python标量
  # tensor([3.5000]), 3.5, 3, 3.5
  ```

# 2.2 数据预处理

- 使用`pandas`预处理原始数据，并将原始数据转换为张量格式
- `pandas`可以与张量（tensor）兼容

## 2.2.1 读取数据集

### 创建

1. 创建文件夹
   ```python
   import os
   
   os.makedirs(os.path.join('..', 'data'), exist_ok=True)
   ```
    - `os.makedirs`函数用来递归创建目录。
    - `os.path.join('..', 'data')`用于将路径组合成一个完整的文件路径，这里是在当前目录的上一级目录中创建一个名为 "data"
      的文件夹。
    - `exist_ok=True`参数表示如果目标文件夹已经存在，就不抛出异常，这样做是为了在文件夹已存在时不会产生错误。
1. 定义数据文件路径
   ```python
   data_file = os.path.join('..', 'data', 'house_tiny.csv')
   ```
    - `os.path.join('..', 'data', 'house_tiny.csv')`
      用于将路径组合成一个完整的文件路径，这里是在当前目录的上一级目录中的 "data" 文件夹中创建名为 "house_tiny.csv" 的文件。
2. 写入数据到文件
   ```python
   with open(data_file, 'w') as f:
       f.write("NumRooms,Alley,Price\n")  # # 写入列名
       f.write("NA,Pave,127500\n")
       f.write("2,NA,106000\n")
       f.write("4,NA,178100\n")
       f.write("NA,NA,140000\n")
   ```
    - `with open(data_file, 'w') as f:`这行代码打开名为 "house_tiny.csv" 的文件以便写入数据。
    - `f.write("NumRooms,Alley,Price\n")`这段代码写入了列名到文件中。
    - 之后的每一行`f.write()`代码都向文件中写入一个数据样本。每行的数据以逗号分隔，表示每个样本的 NumRooms, Alley, 和
      Price。

### 读取

- 要从创建的CSV文件中加载原始数据集，需导入`pandas`包并调用`read_csv`
  函数。该数据集有四行三列。其中每行描述了房间数量（“NumRooms”）、巷子类型（“Alley”）和房屋价格（“Price”）。

```python
import pandas as pd

data = pd.read_csv(data_file)
print(data, type(data), sep='\n')
#    NumRooms Alley   Price  
# 0       NaN  Pave  127500  
# 1       2.0   NaN  106000  
# 2       4.0   NaN  178100  
# 3       NaN   NaN  140000  
# <class 'pandas.core.frame.DataFrame'>
```

## 2.2.2 处理缺失值

- NA：通常用于表示缺失的类别型数据，在 pandas 中经常用来代表缺失的值。例如，在数据集中用"NA"表示某一列的数据缺失。
- NaN：代表"Not a Number"，在数据处理中通常用来表示缺失的数值型数据。当在数值类型的数据中存在缺失值时，通常会使用 NaN
  来表示这种缺失情况。
- 插值法
    - 通过位置索引`iloc`，我们将`data`分成`inputs`和`outputs`（切片）， 其中前者为`data`的前两列，而后者为`data`的最后一列。
      对于`inputs`中缺少的数值，用同一列的==均值==替换“NaN”项。
  ```python
  inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]  # 输入和输出分别是NumRooms和Alley，Price  
  inputs = inputs.fillna(inputs.mean())  # 用平均值填充缺失值  
  print(inputs, type(inputs), sep='\n')  
  #    NumRooms Alley  
  # 0       3.0  Pave  
  # 1       2.0   NaN  
  # 2       4.0   NaN  
  # 3       3.0   NaN  
  # <class 'pandas.core.frame.DataFrame'>
  ```
- 对于`inputs`中的类别值或离散值，将“NaN”视为一个类别。 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”，
  `pandas`可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。 巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。
  缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。
  ```python
  inputs = pd.get_dummies(inputs, dummy_na=True)  # 独热编码  
  # pd.get_dummies(inputs, dummy_na=True) 会对输入的 inputs DataFrame 进行独热编码，并将结果存储在新的DataFrame中。  
  # dummy_na=True可以指定是否为缺失值创建指示变量，如果为True，则会为缺失值单独创建一列。  
  print(inputs)  
  #    NumRooms  Alley_Pave  Alley_nan  
  # 0       3.0           1          0  
  # 1       2.0           0          1  
  # 2       4.0           0          1  
  # 3       3.0           0          1
  ```

## 2.2.3 转换为张量格式

- 现在`inputs`和`outputs`中的所有条目都是数值类型，它们可以转换为张量格式。

```python
import torch

X = torch.tensor(inputs.to_numpy(dtype=float))  # 转换为张量 Tensor, to_numpy() 方法可以将 DataFrame 转换为 numpy 数组  
y = torch.tensor(outputs.to_numpy(dtype=float))
print(X, y, sep='\n')
# tensor([[3., 1., 0.],  
#         [2., 0., 1.],  
#         [4., 0., 1.],  
#         [3., 0., 1.]], dtype=torch.float64)  
# tensor([127500., 106000., 178100., 140000.], dtype=torch.float64)
```

## 2.2.4 练习

```python
missing_counts = data.isnull().sum()  # 统计缺失值个数  
max_missing_col = missing_counts.idxmax()  # 找出拥有最大缺失值个数的列名  
data.drop(columns=max_missing_col, inplace=True)  # 删除拥有最大缺失值个数的列  
print(data, type(data), sep='\n')
#    NumRooms   Price  
# 0       NaN  127500  
# 1       2.0  106000  
# 2       4.0  178100  
# 3       NaN  140000  
# <class 'pandas.core.frame.DataFrame'>  
data_new = torch.tensor(data.to_numpy(dtype=float))  # 转换为张量 Tensorprint(data_new, type(data_new), sep='\n')  
# tensor([[       nan, 1.2750e+05],  
#         [2.0000e+00, 1.0600e+05],  
#         [4.0000e+00, 1.7810e+05],  
#         [       nan, 1.4000e+05]], dtype=torch.float64)  
# <class 'torch.Tensor'>
```

# 2.3 线性代数

## 2.3.1 标量

- 标量（scalar）：仅包含一个数值，由只有一个元素的张量表示

```python
import torch

x = torch.tensor(3.0)
y = torch.tensor(2.0)
print(x + y, x - y, x / y, x ** y, sep='\n')
# tensor(5.)  
# tensor(1.)  
# tensor(1.5000)  
# tensor(9.)
```

## 2.3.2 向量

- 向量：标量值组成的列表，一般用一维数组表示向量

```python
x = torch.arange(4)
print(x)
# tensor([0, 1, 2, 3])
```

- 使用下标来访问向量的任一元素
- 一般认为列向量是向量的默认方向

```python
print(x[3])
# tensor(3)
```

### 长度、维度和形状

- 向量的长度通常称为向量的维度（dimension）

```python
print(x[3])
# tensor(3)
```

- 用张量表示一个向量（只有一个轴）时，可以通过 `.shape` 属性访问向量长度。
- 形状是一个元素组，列出来张量沿每个轴的长度（维数）。对于只有一个轴的张量，形状只有一个元素。

```python
print(x.shape)
# torch.Size([4])
```

## 2.3.3 矩阵

- 调用函数实例化一个张量时，可以通过指定两个分量 m 和 n 来创建一个形状为 $m \times n$ 的矩阵

```python
A = torch.arange(20).reshape(5, 4)
print(A)
# tensor([[ 0,  1,  2,  3],  
#         [ 4,  5,  6,  7],  
#         [ 8,  9, 10, 11],  
#         [12, 13, 14, 15],  
#         [16, 17, 18, 19]])
```

- 矩阵转置

```python
print(A.T)
# tensor([[ 0,  4,  8, 12, 16],  
#         [ 1,  5,  9, 13, 17],  
#         [ 2,  6, 10, 14, 18],  
#         [ 3,  7, 11, 15, 19]])
```

## 2.3.4 张量

- 张量是描述具有任意数量轴的 n 维数组的通用方法
    - 向量是一阶张量，矩阵是二阶张量

```python
X = torch.arange(24).reshape(2, 3, 4)
print(X)
# tensor([[[ 0,  1,  2,  3],  
#          [ 4,  5,  6,  7],  
#          [ 8,  9, 10, 11]],  
#  
#         [[12, 13, 14, 15],  
#          [16, 17, 18, 19],  
#          [20, 21, 22, 23]]])
```

## 2.3.5 张量算法的基本性质

#### 克隆

- 给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量

```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()  # 通过分配新内存，将 A 的一个副本分配给 B
print(A, A + B, sep='\n')
# tensor([[ 0.,  1.,  2.,  3.],  
#         [ 4.,  5.,  6.,  7.],  
#         [ 8.,  9., 10., 11.],  
#         [12., 13., 14., 15.],  
#         [16., 17., 18., 19.]])  
# tensor([[ 0.,  2.,  4.,  6.],  
#         [ 8., 10., 12., 14.],  
#         [16., 18., 20., 22.],  
#         [24., 26., 28., 30.],  
#         [32., 34., 36., 38.]])
```

### 矩阵相乘（对应元素相乘）

- 两个矩阵的==按元素乘法==称为 Hadamard 积（Hadamard product）（数学符号 ⊙）
    - $$\begin{split}\mathbf{A} \odot \mathbf{B} =
      \begin{bmatrix}
      a_{11} b_{11} & a_{12} b_{12} & \dots & a_{1n} b_{1n} \\
      a_{21} b_{21} & a_{22} b_{22} & \dots & a_{2n} b_{2n} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{m1} b_{m1} & a_{m2} b_{m2} & \dots & a_{mn} b_{mn}
      \end{bmatrix}.\end{split}$$

```python
print(A * B)
# tensor([[  0.,   1.,   4.,   9.],  
#         [ 16.,  25.,  36.,  49.],  
#         [ 64.,  81., 100., 121.],  
#         [144., 169., 196., 225.],  
#         [256., 289., 324., 361.]])
```

### 矩阵加标量

- 张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。

```python
a = 2
X = torch.arange(24).reshape(2, 3, 4)
print(a + X, (a * X).shape, sep='\n')
# tensor([[[ 2,  3,  4,  5],  
#          [ 6,  7,  8,  9],  
#          [10, 11, 12, 13]],  
#  
#         [[14, 15, 16, 17],  
#          [18, 19, 20, 21],  
#          [22, 23, 24, 25]]])  
# torch.Size([2, 3, 4])
```

## 2.3.6 降维求和

- 默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。

```python
x = torch.arange(4, dtype=torch.float32)
print(x, x.sum())
# tensor([0., 1., 2., 3.]) tensor(6.)  

print(A.shape, A.sum())
# torch.Size([5, 4]) tensor(190.)
```

- 指定张量沿某一个轴通过求和函数降低维度
- 以数组为例
- 指定`axis=0`，通过求和所有行的元素来降维（轴0），行累加，消去维度 0。（沿此维度进行压缩）
- A.shape 为 [4, 5]，这里axis=0，相当于把 [5，4] 的“5”给消去，剩下的就是“4”

```python
A_sum_axis0 = A.sum(axis=0)  # 按行求和, 输入矩阵沿 0 轴降维以生成输出向量
print(A_sum_axis0, A_sum_axis0.shape)
# tensor([40., 45., 50., 55.]) torch.Size([4])
```

- 指定`axis=1`，通过求和所有列的元素降维（轴1），列累加，消去维度 1

```python
A_sum_axis1 = A.sum(axis=1)  # 按列求和  
print(A_sum_axis1, A_sum_axis1.shape)
# tensor([ 6., 22., 38., 54., 70.]) torch.Size([5])
```

- 沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。
- 消去维度 0，1

```python
print(A.sum(axis=[0, 1]))  # 结果和A.sum()相同  
# tensor(190.)
```

- 通过计算平均值函数沿指定轴降低张量的维度

```python
print(A.mean(axis=0), A.sum(axis=0) / A.shape[0])  # 按行求均值  
# tensor([ 8.,  9., 10., 11.]) tensor([ 8.,  9., 10., 11.])
```

### 2.3.6.1 非降维求和

- 通过 `keepdims=True` 保持维度不变
- 对 [4, 5]， 按轴1，进行不降维求和，得 [4, 1]

```python
sum_A = A.sum(axis=1, keepdim=True)  # 按列求和，保持维度  
print(sum_A, sum_A.shape, sep='\n')
# tensor([[ 6.],  
#         [22.],  
#         [38.],  
#         [54.],  
#         [70.]])  
# torch.Size([5, 1])
```

- 由于`sum_A`在对每行进行求和后仍保持两个轴，可以通过广播将`A`除以`sum_A`
- 得到每列元素除以该列

```python
print(A / sum_A)
# tensor([[0.0000, 0.1667, 0.3333, 0.5000],  
#         [0.1818, 0.2273, 0.2727, 0.3182],  
#         [0.2105, 0.2368, 0.2632, 0.2895],  
#         [0.2222, 0.2407, 0.2593, 0.2778],  
#         [0.2286, 0.2429, 0.2571, 0.2714]])
```

- 如果想沿某个轴计算`A`元素的累加和， 比如`axis=0`（按行计算），可以调用`cumsum`函数。 此函数不会沿任何轴降低输入张量的维度。

```python
print(A.cumsum(axis=0))  # 按行累积求和  
# tensor([[ 0.,  1.,  2.,  3.],  
#         [ 4.,  6.,  8., 10.],  
#         [12., 15., 18., 21.],  
#         [24., 28., 32., 36.],  
#         [40., 45., 50., 55.]])
```

## 2.3.7 点积

- 又称内积
- $<x, y>=\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i$
- 一行点乘一列，得一值（对应元素相乘，求和）

```python
y = torch.ones(4, dtype=torch.float32)
print(x, y, torch.dot(x, y))
# tensor([0., 1., 2., 3.]) tensor([1., 1., 1., 1.]) tensor(6.)
```

- 点积也可以通过先按元素乘法，然后进行求和得到

```python
print(torch.sum(x * y))
# tensor(6.)
```

## 2.3.8 矩阵-向量积

- 即矩阵乘法
- ![[00 Attachments/a1845823f1e00f17313ea724d7ea8be.jpg|500]]$$\begin{split}\mathbf{A}\mathbf{x}
  = \begin{bmatrix}
  \mathbf{a}^\top_{1} \\
  \mathbf{a}^\top_{2} \\
  \vdots \\
  \mathbf{a}^\top_m \\
  \end{bmatrix}\mathbf{x}
  = \begin{bmatrix}
  \mathbf{a}^\top_{1} \mathbf{x} \\
  \mathbf{a}^\top_{2} \mathbf{x} \\
  \vdots\\
  \mathbf{a}^\top_{m} \mathbf{x}\\
  \end{bmatrix}\end{split}$$

```python
print(A.shape, x.shape, torch.mv(A, x))  # 矩阵向量乘法  
# torch.Size([5, 4]) torch.Size([4]) tensor([ 6., 22., 38., 54., 70.])
```

## 2.3.9 矩阵-矩阵乘法

- 注意与 A\*B不同
- ![[00 Attachments/5f113cfa7127a984fdf1694b76241f5.jpg|500]]$$\begin{split}\mathbf{C} = \mathbf{AB} = \begin{bmatrix}
  \mathbf{a}^\top_{1} \\
  \mathbf{a}^\top_{2} \\
  \vdots \\
  \mathbf{a}^\top_n \\
  \end{bmatrix}
  \begin{bmatrix}
  \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
  \end{bmatrix}
  = \begin{bmatrix}
  \mathbf{a}^\top_{1} \mathbf{b}_1 & \mathbf{a}^\top_{1}\mathbf{b}_2& \cdots & \mathbf{a}^\top_{1} \mathbf{b}_m \\
  \mathbf{a}^\top_{2}\mathbf{b}_1 & \mathbf{a}^\top_{2} \mathbf{b}_2 & \cdots & \mathbf{a}^\top_{2} \mathbf{b}_m \\
  \vdots & \vdots & \ddots &\vdots\\
  \mathbf{a}^\top_{n} \mathbf{b}_1 & \mathbf{a}^\top_{n}\mathbf{b}_2& \cdots& \mathbf{a}^\top_{n} \mathbf{b}_m
  \end{bmatrix}.\end{split}$$

```python
B = torch.ones(4, 3)
print(torch.mm(A, B))  # 矩阵乘法  
# tensor([[ 6.,  6.,  6.],  
#         [22., 22., 22.],  
#         [38., 38., 38.],  
#         [54., 54., 54.],  
#         [70., 70., 70.]])
```

## 2.3.10 范数

- 非正式地说，向量地==范数==时表示一个向量有多大。（不涉及维度，而是分量的大小）
- 在线代中，向量范数是将向量映射到标量的函数 $f$
- 向量范数满足以下性质
    - 向量缩放 $\alpha$，则范数缩放 $|\alpha|$$$f(\alpha \mathbf{x}) = |\alpha| f(\mathbf{x})$$
    - 三角不等式$$f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y})$$
    - 范数必须非负 $$f(\mathbf{x}) \geq 0.$$所以，范数最小为 0，当且仅当向量全由 0
      组成$$\forall i, [\mathbf{x}]_i = 0 \Leftrightarrow f(\mathbf{x})=0$$

### $L_2$ 范数

- 假设 𝑛 维向量 𝑥 中的元素是 $𝑥_1,…,𝑥_𝑛$，其 $𝐿_2$
  范数是==向量==元素平方和的平方根：$$\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2},$$
- 使用时更加常用 $L_2$ 范数的平方

```python
u = torch.tensor([3.0, -4.0])
print(torch.norm(u))
# tensor(5.)
```

### $L_1$ 范数

- ==向量==元素的绝对值之和：$$\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.$$

```python
print(torch.abs(u).sum())
# tensor(7.)
```

### $L_p$ 范数

- $L_2$ 范数和 $L_1$ 范数都是更一般的 $L_P$
  范数的特例：$$\|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}$$

### Frobenius 范数

- ==矩阵==元素平方和的平方根：$$\|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}$$
- 像是矩阵形向量的 $𝐿_2$ 范数

```python
print(torch.norm(torch.ones((4, 9))))  # Frobenius范数  
# tensor(6.)
```

### 范数与目标

- 在深度学习中，我们经常试图解决优化问题： 最大化分配给观测数据的概率；最小化预测和真实观测之间的距离。
  用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。
  目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。

# 2.4 矩阵运算

## 2.4.1 标量导数

- ![[00 Attachments/Pasted image 20240508135105.png|500]]

## 2.4.2 亚导数

- ![[00 Attachments/Pasted image 20240508135159.png|500]]

## 2.4.3 求导

### 2.4.3.1 梯度

- ![[00 Attachments/Pasted image 20240508135419.png|300]]
    - 向量关于标量求导，结果为标量（向量一般通指列向量）
    - 向量关于向量求导，结果为矩阵 ^230874

### 2.4.3.2 标量对向量求导

- ![[00 Attachments/Pasted image 20240508140305.png|500]]![[00 Attachments/Pasted image 20240508140401.png|500]]

### 2.4.3.3 向量对标量求导

- ![[00 Attachments/Pasted image 20240508141033.png|500]]

### 2.4.3.4 向量对向量求导

- ![[00 Attachments/Pasted image 20240508141109.png|500]]![[00 Attachments/Pasted image 20240508141127.png|500]]

### 2.4.3.5 扩展矩阵对矩阵求导

- 分子不变，分母反过来![[00 Attachments/Pasted image 20240508141206.png|500]]

# 2.5 自动求导

## 2.5.1 向量链式法则

- ![[00 Attachments/Pasted image 20240508142146.png|500]]
- 举例![[00 Attachments/Pasted image 20240508142219.png|500]]![[00 Attachments/Pasted image 20240508153810.png|500]]

## 2.5.2 自动求导

- ![[00 Attachments/Pasted image 20240508154707.png|500]]

- 计算图，正向![[00 Attachments/Pasted image 20240508154852.png|500]]
- 自动求导的两种模式![[00 Attachments/Pasted image 20240508155153.png|500]]
-

反向累积，一般对于输入数据会先正向计算函数值，再反向计算偏导数，梯度，正向计算时会保存所有的中间变量![[00 Attachments/Pasted image 20240508155347.png|500]]

- 反向累积总结![[00 Attachments/Pasted image 20240508155623.png]]
- 复杂度![[00 Attachments/Pasted image 20240508155641.png|500]]

## 2.5.3 Code

### 2.5.3.1 自动求导

- 对函数 $𝑦=2𝑥^⊤𝑥$ 关于列向量 𝑥 求导

```python
import torch

x = torch.arange(4.0)
print(x)
# tensor([0., 1., 2., 3.])
```

- 在我们计算 𝑦 关于 𝑥 的梯度之前，需要一个地方来存储梯度。
    - 不会在每次对一个参数求导时都分配新的内存。 因为经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽
- 梯度是 x 的属性

```python
x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)，将 x 标记为可被求导
print(x.grad)  # 默认值是None
# None
```

- 计算 y

```python
y = 2 * torch.dot(x, x)  # 2*x.T*x  
print(y)
# tensor(28., grad_fn=<MulBackward0>)
```

- x 是一个长度为 4 的向量，计算 x 和 x 的点积，得到了我们赋值给 y 的标量输出。 接下来，通过调用反向传播函数来自动计算 y 关于
  x 每个分量的梯度，并打印这些梯度
- $$\begin{align}grad & = \frac{\partial 2\mathbf{x} ^T\mathbf{x} }{\partial \mathbf{x} }\\& = 2\frac{\partial\mathbf{x} ^T}{\partial \mathbf{x}}\mathbf{x}+2\mathbf{x}^T\frac{\partial\mathbf{x}}{\partial \mathbf{x}}\\& = 2\mathbf{x}^T+2\mathbf{x}^T \end{align}$$

```python
y.backward()  # 计算梯度  
print(x.grad)
# tensor([ 0.,  4.,  8., 12.])
```

- 计算 x 的求和函数
- ==sum函数其实就是 $x_1+x_2+...x_n$，求偏导自然是全 1==
- 在默认情况下，PyTorch会累积梯度，需要清除之前的值

```python
x.grad.zero_()  # 清空梯度  
y = x.sum()
y.backward()
print(x.grad)
# tensor([1., 1., 1., 1.])
```

### 2.5.3.2 非标量变量的反向传播

- 在机器学习、深度学习中，一般不会对向量求导，大部分情况是对标量进行求导
- 但当调用向量的反向计算时，通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和
- 对非标量调用 `backward` 需要传入一个 `gradient` 参数，该参数指定微分函数关于 `self` 的梯度
- `y.sum().backword()` 先得到 $y=x_1^2+x_2^2+x_3^2+x_4^2$（转化为标量）， 然后再分别求导

```python
x.grad.zero_()  # 清空梯度  
y = x * x
y.sum().backward()  # 等价于 y.backward(torch.ones(y)) 将 y 转化为标量，计算 y 对 x 的导数  
print(x.grad)
# tensor([0., 2., 4., 6.])
```

### 2.5.3.3 分离计算

- 将某些计算移到计算图以外
- 如 $y=f(x)=x^2$，$z=f(x,y)=xy$，求 z 关于 x 的偏导，但又希望将 y 视为常数（虽然我不知道为什么要这样做）。

```python
x.grad.zero_()  # 清空梯度  
y = x * x
u = y.detach()  # 切断梯度流，u 不会影响 x 的梯度计算  
z = u * x  # 将 u 作为常数项参与运算  
z.sum().backward()
print(x.grad == u)
# tensor([True, True, True, True])
```

- 由于已经记录了 y 的计算结果，可以随后在 y 上进行反向传播，得到 y 关于 x 的导数

```python
x.grad.zero_()  # 清空梯度  
y.sum().backward()
print(x.grad == 2 * x)
# tensor([True, True, True, True])
```

### 2.5.3.4 Python 梯度流的计算

- 在使用 while，for，if 的情况下，未知函数的层数，依然可以使用自动微分得到变量的梯度
- $f(a)$ 是 a 的分段线性函数，则 $$\exists k\in R，使得 f(a) = k\times a，则 k=\frac{d}{a}$$

```python
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c


a = torch.randn(size=(), requires_grad=True)  # 随机初始化参数，需要求导，形状为()，即标量  
d = f(a)
d.backward()
print(a.grad == d / a)
# tensor(True)
```

## 2.5.6 练习

1. 使 $𝑓(𝑥)=sin⁡(𝑥)$，绘制 $𝑓(𝑥)$ 和 $𝑑𝑓(𝑥)/𝑑𝑥$ 的图像，其中后者不使用 $𝑓^′(𝑥)=cos⁡(𝑥)$。

```python
import torch
import matplotlib.pyplot as plt
import numpy as np

x = torch.linspace(0, 3 * np.pi, 128)  # 生成一个包含128个点的张量，范围从0到3π  
x.requires_grad_(True)  # 标记张量 x 需要计算梯度  
y = torch.sin(x)  # 计算张量 x 中每个元素的正弦值，得到新的张量 y，对 y 的所有元素求和，并进行反向传播计算梯度  
y.sum().backward()
# 绘制 sin(x) 曲线和其导数曲线 ∂y/∂x=cos(x)，并在图例中给出相应的标签  
plt.plot(x.detach(), y.detach(), label='y=sin(x)')
plt.plot(x.detach(), x.grad, label='∂y/∂x=cos(x)')  # dy/dx = cos(x)  
# 添加图例并显示图形  
plt.xticks(np.arange(0, 3 * np.pi + np.pi, np.pi), ['0', 'π', '2π', '3π'])
plt.legend(loc='upper right')
plt.show()
```

- ![[00 Attachments/Figure_1.png|500]]
