```toc
```

# 4.1、模型选择 + 过拟合和欠拟合

## 4.1.1、模型选择

- 举例![[00 Attachments/Pasted image 20240517093956.png|300]]![[00 Attachments/Pasted image 20240517094019.png|300]]
    - 这样会使模型误判，以为蓝色衣服与违约行为强相关
    - 就是说有些信息不仅不助于正确收敛，反而会误导神经网络

### 训练误差

- 两种误差，关心的是泛化误差而不是训练误差![[00 Attachments/Pasted image 20240517094304.png|400]]
    - A 背题获得好成绩，B 靠自己的实力获得好成绩（训练误差）
    - 但在下次考试（新数据）中，A 没有背题成绩降低，B 保持稳定成绩（泛化误差）

### 验证数据集和测试数据集

- 两种数据集![[00 Attachments/Pasted image 20240517094625.png|400]]
    - 将训练用的数据集分为两份：一半用于训练模型，一般用于验证模型（验证数据集，用于选择超参数）
    - 测试数据集理论上只能用一次，不能用于调超参数（判断超参数的好坏）（用过一次用，就不看下次使用的结果了）

### K-折交叉验证

-

一般来说没有太多的训练数据（一半的数据用于验证模型，而不是训练模型）![[00 Attachments/Pasted image 20240517095512.png|400]]
- 将数据分为 k 份，留一份做验证数据集，剩下的用于训练数据集
- 验证集由原来的一半变为 k 分之一，会导致误差，所以遍历求平均以降低误差
- 虽然在 for 中使用了验证集，但在循环中不改变超参数。循环后根据验证集的平均误差调整超参数
- 每一折的数据都是重新开始训练的（相互独立）

## 4.1.2、欠拟合和过拟合

### 欠拟合过拟合

- 要根据数据的复杂度正确使用模型![[00 Attachments/Pasted image 20240517100904.png|300]]
- 如果有足够多的神经元、层数和训练迭代周期，
  模型最终可以在验证集上达到完美的精度，此时测试集的准确性却下降了![[00 Attachments/Pasted image 20240519104416.png|300]]
-

当模型过拟合时，会导致训练误差减小但是泛化误差增大（过于拟合验证数据集而不能拟合校验数据集）![[00 Attachments/Pasted image 20240519104506.png|300]]
- 所以在调试时，一般先使用大的模型容量，然后尽量改正模型容量，使泛化误差与训练误差尽可能地接近

### 估计模型容量

- ![[00 Attachments/Pasted image 20240519104941.png|300]]
    - 线性回归模型![[00 Attachments/Pasted image 20240519104955.png|150]]
        - d 为输入个数（权重），1 为偏置
    - 多层感知机模型（一层隐藏层）![[00 Attachments/Pasted image 20240519105009.png|150]]

#### CV维

- ![[00 Attachments/Pasted image 20240519105424.png|400]]
- ![[00 Attachments/Pasted image 20240519105448.png|400]]
- 局限性![[00 Attachments/Pasted image 20240519105501.png|400]]

### 数据复杂度

- 模型容量需要匹配数据复杂度![[00 Attachments/Pasted image 20240519105625.png|400]]

### 多项式解释过拟合和欠拟合

#### 生成数据集

-

使用以下三阶多项式来生成训练和测试数据的标签$$y = 5 + 1.2x - 3.4\frac{x^2}{2!} + 5.6 \frac{x^3}{3!} + \epsilon \text{ where }
\epsilon \sim \mathcal{N}(0, 0.1^2)$$
- 噪声项 $𝜖$ 服从均值为 0 且标准差为 0.1 的正态分布
- 优化的过程中，通常希望避免非常大的梯度值或损失值。 这就是我们将特征从 $𝑥^𝑖$ 调整为 $\frac{x^i}{i!}$ 的原因，
这样可以避免很大的 𝑖 带来的特别大的指数值
- 为训练集和测试集各生成100个样本

```python
import math
import numpy as np
import torch
from torch import nn
from d2l import torch as d2l

# 生成数据集  
max_degree = 20  # 多项式的最大阶数  
n_train, n_test = 100, 100  # 训练集和测试集的大小  
true_w = np.zeros(max_degree)  # 分配大量的空间  
true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])  # 存储多项式的系数（权重）  
# 随机生成 xfeatures = np.random.normal(size=(n_train + n_test, 1))  # 生成一个大小为(n_train + n_test, 1)的服从标准正态分布的随机特征数组  
np.random.shuffle(features)  # 打乱特征数据的顺序  
poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))  # 对第所有维的特征取0次方、1次方、2次方...19次方  
for i in range(max_degree):
    poly_features[:, i] /= math.gamma(i + 1)  # i次方的特征除以(i+1)阶乘  
# 创建标签  
labels = np.dot(poly_features, true_w)  # 根据多项式生成y，即生成真实的labels  
labels += np.random.normal(scale=0.1, size=labels.shape)  # 为标签添加服从正态分布的噪声，以模拟真实数据的随机性
```

- 存储在`poly_features`中的单项式由gamma函数重新缩放， 其中 $\Gamma(n)=(n-1)!$
- 从生成的数据集中查看一下前2个样本， 第一个值是与偏置相对应的常量特征

```python
# NumPy ndarray转换为tensor  
true_w, features, poly_features, labels = [torch.tensor(x, dtype=torch.float32)
                                           for x in [true_w, features, poly_features, labels]]

print(features[:2], poly_features[:2, :], labels[:2], sep='\n')
# tensor([[-0.8282],  
#         [ 1.4003]])  
# tensor([[ 1.0000e+00, -8.2825e-01,  3.4300e-01, -9.4695e-02,  1.9608e-02,  
#          -3.2480e-03,  4.4836e-04, -5.3050e-05,  5.4923e-06, -5.0545e-07,  
#           4.1863e-08, -3.1521e-09,  2.1756e-10, -1.3861e-11,  8.2003e-13,  
#          -4.5279e-14,  2.3439e-15, -1.1419e-16,  5.2545e-18, -2.2905e-19],  
#         [ 1.0000e+00,  1.4003e+00,  9.8041e-01,  4.5762e-01,  1.6020e-01,  
#           4.4866e-02,  1.0471e-02,  2.0946e-03,  3.6663e-04,  5.7044e-05,  
#           7.9878e-06,  1.0168e-06,  1.1866e-07,  1.2781e-08,  1.2784e-09,  
#           1.1934e-10,  1.0444e-11,  8.6030e-13,  6.6927e-14,  4.9325e-15]])  
# tensor([2.4621, 5.9544])
```

#### 对模型进行训练和测试

- 评估模型

```python
def evaluate_loss(net, data_iter, loss):  # @save  
    """评估给定数据集上模型的损失。"""
    metric = d2l.Accumulator(2)  # 两个数的累加器  
    for X, y in data_iter:
        out = net(X)  # 预测输出  
        y = y.reshape(out.shape)  # 标签的形状需要和预测输出的形状一致  
        l = loss(out, y)  # 计算损失  
        metric.add(l.sum(), l.numel())  # 累加损失 和 样本数量  
    return metric[0] / metric[1]  # 计算平均损失
```

- 训练函数

```python
def train(train_features, test_features, train_labels, test_labels,
          num_epochs=400):
    loss = nn.MSELoss(reduction='none')  # 均方误差损失  
    input_shape = train_features.shape[-1]
    # 不设置偏置，因为我们已经在多项式中实现了它  
    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))  # 定义模型  
    batch_size = min(10, train_labels.shape[0])  # 训练集的批量大小  
    train_iter = d2l.load_array((train_features, train_labels.reshape(-1, 1)),
                                batch_size)  # 训练集数据迭代器  
    test_iter = d2l.load_array((test_features, test_labels.reshape(-1, 1)),
                               batch_size, is_train=False)  # 测试集数据迭代器  
    trainer = torch.optim.SGD(net.parameters(), lr=0.01)  # 小批量随机梯度下降  
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',
                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],
                            legend=['train', 'test'])  # 绘制训练过程  
    for epoch in range(num_epochs):  # 训练模型  
        d2l.train_epoch_ch3(net, train_iter, loss, trainer)  # 训练模型一个 epoch        
        if epoch == 0 or (epoch + 1) % 20 == 0:  # 每 20 个 epoch 评估模型  
            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),
                                     evaluate_loss(net, test_iter, loss)))
    print('weight:', net[0].weight.data.numpy())
```

#### 三阶多项式函数拟合(正常)

```python
# 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!  
train(poly_features[:n_train, :4], poly_features[n_train:, :4],
      labels[:n_train], labels[n_train:])
# weight: [[ 4.9890676  1.1630476 -3.4193513  5.6815   ]]
```

- 首先使用三阶多项式函数，它与数据生成函数的阶数相同。 结果表明，该模型能有效降低训练损失和测试损失。
  学习到的模型参数也接近真实值 $w = [5, 1.2, -3.4, 5.6]$![[00 Attachments/Pasted image 20240520012428.png|500]]

#### 三阶多项式函数拟合(欠拟合)

- 模型容量小

```python
# 从多项式特征中选择前2个维度，即1和x  
train(poly_features[:n_train, :2], poly_features[n_train:, :2],
      labels[:n_train], labels[n_train:])
# weight: [[2.9832623 4.2033267]]
```

- 线性函数拟合，减少该模型的训练损失相对困难。 在最后一个迭代周期完成后，训练损失仍然很高。
  当用来拟合非线性模式（如这里的三阶多项式函数）时，线性模型容易欠拟合![[00 Attachments/Pasted image 20240520012825.png|500]]

#### 三阶多项式函数拟合(过拟合)

```python
# 从多项式特征中选取所有维度  
train(poly_features[:n_train, :], poly_features[n_train:, :],
      labels[:n_train], labels[n_train:], num_epochs=1500)
# weight: [[ 4.9926720e+00  1.2720633e+00 -3.2974391e+00  5.2138767e+00  
#   -3.4428847e-01  1.3618331e+00  1.5626380e-01  1.1630570e-01  
#    1.5991014e-01  2.8070834e-02 -1.0929724e-01  2.7722307e-03  
#    7.9240397e-02  1.4077527e-02  5.4253254e-02  1.8295857e-01  
#    1.6828278e-01 -3.8895667e-02  1.5147164e-02  1.3848299e-01]]
```

- 使用一个阶数过高的多项式来训练模型。 在这种情况下，没有足够的数据用于学到高阶系数应该具有接近于零的值。
  因此，这个过于复杂的模型会轻易受到训练数据中噪声的影响。 虽然训练损失可以有效地降低，但测试损失仍然很高。
  结果表明，复杂模型对数据造成了过拟合![[00 Attachments/Pasted image 20240520013616.png|500]]

### 总结

- 欠拟合是指模型无法继续减少训练误差。过拟合是指训练误差远小于验证误差。
- 由于不能基于训练误差来估计泛化误差，因此简单地最小化训练误差并不一定意味着泛化误差的减小。机器学习模型需要注意防止过拟合，即防止泛化误差过大。
- 验证集可以用于模型选择，但不能过于随意地使用它。
- 我们应该选择一个复杂度适当的模型，避免使用数量不足的训练样本。

# 4.2、权重衰退

- 解决过拟合问题
- 我们总是可以通过收集更多的训练数据来缓解过拟合。（但是会有很高的成本）
    - 为解决过拟合，也可以减小模型容量。把模型容量控制比较小有两种方法
        - 方法一：模型控制的比较小，使得模型中参数比较少
        - 方法二：==控制参数选择范围来控制模型容量==
- 假设已有尽可能多的高质量数据，便可以将重点放在正则化技术上
- 在多项式回归中，可以调整拟合多项式的阶数来限制模型的容量
    - 但是==限制特征的数量是一种过于生硬的方法，而且可能不尽人意==（使模型在过简单和过复杂中徘徊）
- 约束就是正则项。==每个特征的权重都大会导致模型复杂，从而导致过拟合。控制权重矩阵范数可以使得减少一些特征的权重，甚至使他们权重为0，从而导致模型简单，减轻过拟合==
- ![[00 Attachments/Pasted image 20240524174025.png|400]]

## 4.2.1、权重衰减:  weight decay

### 使用均方范数作为限制

- ![[00 Attachments/Pasted image 20240524174037.png|400]]
- 在训练参数化机器学习模型时， 权重衰减（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为 $𝐿_2$ 正则化
    - 这项技术通过==函数与零的距离来衡量函数（模型）的复杂度==， 因为在所有函数 $𝑓$ 中，函数 $𝑓=0$（所有输入都得到值0）
      在某种意义上是最简单的
- 一种简单的方法是通过线性函数$f(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}$中的权重向量的某个范数来度量其复杂性，
  例如 $\| \mathbf{w} \|^2$
    - 要保证权重向量比较小，最常用的方法是将其范数作为惩罚项加到最小化损失的问题中（$L_2$ 正则项会对大数值的权值进行惩罚。）
    - 将原来的训练目标最小化训练标签上的预测损失，调整为最小化预测损失和==惩罚项==之和， 那么，如果我们的权重向量增长的太大，
      我们的学习算法可能会更集中于最小化权重范数 $\| \mathbf{w} \|^2$
      （==拉格朗日乘数法==：解决约束条件下的多元函数极值问题，$\| \mathbf{w} \|^2$ 为限制条件）
        -
      损失函数 $$L(\mathbf{w}, b) = \frac{1}{n}\sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2$$
        - 为了惩罚权重向量的大小，通过正则化常数 $\lambda$ 来描述惩罚力度（$\lambda$
          为非负超参数）$$L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2$$
            -
          损失函数加上正则项成为目标函数，目标函数最优解不是损失函数最优解。正则项就是防止达到损失函数最优导致过拟合，把损失函数最优点往外拉一拉。鼓励权重分散，将所有额特征运用起来，而不是依赖其中的少数特征，并且权重分散的话它的内积就小一些。
            - 对于 $𝜆=0$，我们恢复了原来的损失函数。 对于 $𝜆>0$，我们限制 $\| \mathbf{w} \|$ 的大小。 这里我们仍然除以
              2：当我们取一个二次函数的导数时， 2 和 1/2 会抵消，以确保更新表达式看起来既漂亮又简单
            - ==为什么在这里我们使用平方范数而不是标准范数==？
                - 为了便于计算。 通过平方 $𝐿_2$ 范数，我们去掉平方根，留下权重向量每个分量的平方和。
                  这使得惩罚的导数很容易计算：导数的和等于和的导数
                - 使用 $𝐿_2$ 范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。
                  在实践中，这可能使它们对单个变量中的观测误差更为稳定。 相比之下，$𝐿_1$ 惩罚会导致模型将权重集中在一小部分特征上，
                  而将其他权重清除为零。 这称为特征选择（feature selection），这可能是其他场景下需要的。
- ![[00 Attachments/Pasted image 20240524180146.png|400]]

### 参数更新

- ![[00 Attachments/Pasted image 20240524174252.png|400]]
    - 由于 $\lambda$ 的引入，在梯度更新前，对当前的值（权重）进行一次放小（权重衰退）
-

那么小批量梯度下降法$$\begin{split}\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)\end{aligned}\end{split}$$
可以更新为下式$$\begin{aligned}
\mathbf{w} & \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)
\end{aligned}$$
- 根据估计值与观测值之间的差异来更新 $\mathbf{w}$。同时也在试图将 $\mathbf{w}$ 的大小缩小到零。
这就是为什么这种方法有时被称为==权重衰减==

- 与特征选择相比，权重衰减提供了一种连续的机制来调整函数的复杂度。 较小的 $𝜆$ 值对应较少约束的 $\mathbf{w}$， 而较大的 $𝜆$
  值对 $\mathbf{w}$ 的约束更大
- 是否对相应的偏置 $𝑏$ 进行惩罚在不同的实践中会有所不同， 在神经网络的不同层中也会有所不同。 通常，网络输出层的偏置项不会被正则化

### 总结

- 权重衰退通过 $L_2$ 正则项使得模型参数不会过大，从而控制模型复杂度，从而解决过拟合问题
- 正则项权重使控制模型复杂度的超参数

## 4.2.2、权重衰退从零开始

```python
import torch
from torch import nn
from d2l import torch as d2l
```

- 生成公式$$y = 0.05 + \sum_{i = 1}^d 0.01 x_i + \epsilon \text{ where }
  \epsilon \sim \mathcal{N}(0, 0.01^2)$$
    - 选择标签是关于输入的线性函数。 标签同时被均值为0，标准差为0.01高斯噪声破坏。
      为了使过拟合的效果更加明显，我们可以将问题的维数增加到 $𝑑=200$， 并使用一个只包含 20 个样本的小训练集

```python
n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5
true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05
train_data = d2l.synthetic_data(true_w, true_b, n_train)
train_iter = d2l.load_array(train_data, batch_size)
test_data = d2l.synthetic_data(true_w, true_b, n_test)
test_iter = d2l.load_array(test_data, batch_size, is_train=False)
```

### 初始化模型参数

```python
def init_params():
    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    return [w, b]
```

### 定义 $L_2$ 范数惩罚

- 实现这一惩罚最方便的方法是对所有项求平方后并将它们求和

```python
def l2_penalty(w):
    return torch.sum(w.pow(2)) / 2
```

### 定义训练代码实现

- 线性网络和平方损失没有变化

```python
def train(lambd):
    w, b = init_params()
    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss  # 线性回归模型和平方损失函数  
    num_epochs, lr = 100, 0.003  # 训练周期和学习率  
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    for epoch in range(num_epochs):
        for X, y in train_iter:
            # 增加了L2范数惩罚项，  
            # 广播机制使l2_penalty(w)成为一个长度为batch_size的向量  
            l = loss(net(X), y) + lambd * l2_penalty(w)
            l.sum().backward()
            d2l.sgd([w, b], lr, batch_size)
        if (epoch + 1) % 5 == 0:  # 每5个周期评估模型  
            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),
                                     d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数是：', torch.norm(w).item())
```

### 训练对比

- 忽略正则化直接训练

```python
train(lambd=0)
# w的L2范数是： 14.708277702331543
```

- ![[00 Attachments/Pasted image 20240524190725.png|400]]
- 使用权重衰退

```python
train(lambd=3)
# w的L2范数是： 0.36321237683296204
```

- ![[00 Attachments/Pasted image 20240524190910.png|400]]
- 从上面的实验结果可以看出，L2范数惩罚项可以使模型参数更加稀疏，从而减少过拟合

## 4.3.3、权重衰退的简洁实现

- 使用框架
- 由于权重衰减在神经网络优化中很常用， 深度学习框架为了便于我们使用权重衰减， 将权重衰减集成到优化算法中，以便与任何损失函数结合使用。
  此外，这种集成还有计算上的好处， 允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。 由于更新的权重衰减部分仅依赖于每个参数的当前值，
  因此优化器必须至少接触每个参数一次

```python
def train_concise(wd):
    net = nn.Sequential(nn.Linear(num_inputs, 1))
    for param in net.parameters():
        param.data.normal_()
    loss = nn.MSELoss(reduction='none')
    num_epochs, lr = 100, 0.003
    # 偏置参数没有衰减
    trainer = torch.optim.SGD([
        {"params": net[0].weight, 'weight_decay': wd},
        {"params": net[0].bias}], lr=lr)
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    for epoch in range(num_epochs):
        for X, y in train_iter:
            trainer.zero_grad()
            l = loss(net(X), y)
            l.mean().backward()
            trainer.step()
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1,
                         (d2l.evaluate_loss(net, train_iter, loss),
                          d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数：', net[0].weight.norm().item())
```

- 在实例化优化器时直接通过 `weight_decay` 指定 weight decay 超参数。 默认情况下，PyTorch 同时衰减权重和偏移。 这里我们只为权重设置了
  `weight_decay`，所以偏置参数𝑏不会衰减

# 4.2、丢弃法（Dropout）

## 重新审视过拟合

- 当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。
  不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，线性模型没有考虑到特征之间的交互作用。
  对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。
- 与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。 例如，神经网络可能推断“尼日利亚”和“西联汇款”一起出现在电子邮件中表示垃圾邮件，
  但单独出现则不表示垃圾邮件
- 即使我们有比特征多得多的样本，深度神经网络也有可能过拟合（未解之谜）

## 扰动的鲁棒性

- ![[00 Attachments/Pasted image 20240525140929.png|400]]
- 如何定义一个“好”的预测模型？我们期待“好”的预测模型能在未知的数据上有很好的表现
    1. 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。例如在权重衰减中使用权重的 $L_2$
       范数来表示模型的简单性（复杂度）
    2.
  简单模型的另一个角度是鲁棒性（一个系统、算法或函数在面对不精确输入、变化或误差时仍能保持其性能和功能的能力），具体来说，当一个函数对其输入的微小变化不敏感时，意味着即使输入存在一些噪声或小的误差，函数的输出仍然是可靠和稳定的（模型应能够处理训练数据中的噪声，并在面对测试数据时表现良好）
- 克里斯托弗·毕晓普证明了 具有输入噪声的训练（丢弃法）等价于Tikhonov正则化（使权重平均化，不过于强调某个特征）

## 丢弃法

- ![[00 Attachments/Pasted image 20240525142337.png|400]]
- 增强鲁棒性：在训练过程中，在计算后续层之前向网络的每一层注入噪声。
  因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强鲁棒性（==丢弃法==、暂退法、dropout）
- 暂退法在前向传播过程中，计算每一内部层的同时注入噪声
    - 这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。
      在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零

### 如何注入噪声

- 以一种无偏向（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。
    - 在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差$$\begin{split}\begin{aligned}
      h' =
      \begin{cases}
      0 & \text{ 概率为 } p \\
      \frac{h}{1-p} & \text{ 其他情况}
      \end{cases}
      \end{aligned}\end{split}$$

## 丢弃法的应用

- 通常将暂退法应用于多层感知机的隐藏层![[00 Attachments/Pasted image 20240525143022.png|400]]
    - 线性回归后通过激活函数，再通过丢弃函数，得到下一层的输入
- 当将暂退法应用到隐藏层，以 $𝑝$ 的概率将隐藏单元置为零时，结果可以看作一个只包含原始神经元子集的网络
    - 比如在上图中，删除了 $ℎ_2$ 和 $ℎ_5$， 因此输出的计算不再依赖于 $ℎ_2$ 或 $ℎ_5$，并且它们各自的梯度在执行反向传播时也会消失。
      这样，输出层的计算不能过度依赖于 $ℎ_1,…,ℎ_5$ 的任何一个元素
- 通常，我们==在测试（预测）时不用暂退法==（测试的时候更关注整个模型的泛化度（泛化误差））
    - 给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。
    - 然而也有一些例外：一些研究人员在测试时使用暂退法， 用于估计神经网络预测的“不确定性”：
      如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定

## 丢弃法总结

- 丢弃发将一些输出项随机置零来==控制模型复杂度==（防止过拟合）
- 常作用在多层感知机的隐藏层输出上
- 丢弃概率是控制模型复杂度的超参数
- 丢弃法是引入一定的噪声，增加模型对输入数据的扰动鲁棒，从而增强泛化；权重衰减在于约束模型参数防止过拟合

## 从零开始实现

### 定义丢弃函数

- 要实现单层的暂退法函数， 我们从均匀分布 $𝑈[0,1]$ 中抽取样本，样本数与这层神经网络的维度一致。
  然后我们保留那些对应样本大于 $𝑝$ 的节点，把剩下的丢弃

```python
import torch
from torch import nn
from d2l import torch as d2l


def dropout_layer(X, dropout):
    """  
    丢弃法激活函数  
    :param X: 输入数据  
    :param dropout: 丢弃率  
    """
    assert 0 <= dropout <= 1
    # 在本情况中，所有元素都被丢弃  
    if dropout == 1:
        return torch.zeros_like(X)
        # 在本情况中，所有元素都被保留  
    if dropout == 0:
        return X
    mask = (torch.rand(X.shape) > dropout).float()  # 随机生成一个0-1的mask  
    return mask * X / (1.0 - dropout)  # 乘以mask，将mask为0的元素置为0，将mask为1的元素乘以1/(1-dropout)
```

- 以`dropout`的概率丢弃张量输入`X`中的元素， 如上所述重新缩放剩余部分：将剩余部分除以`1.0-dropout`
- 通过下面几个例子来测试`dropout_layer`函数。 我们将输入`X`通过暂退法操作，暂退概率分别为0、0.5和1

```python
X = torch.arange(16, dtype=torch.float32).reshape((2, 8))
print(X)
# tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],  
#         [ 8.,  9., 10., 11., 12., 13., 14., 15.]])  
print(dropout_layer(X, 0.))
# tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],  
#         [ 8.,  9., 10., 11., 12., 13., 14., 15.]])  
print(dropout_layer(X, 0.5))
# tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.],  
#         [16.,  0.,  0., 22., 24.,  0.,  0., 30.]])  
print(dropout_layer(X, 1.))
# tensor([[0., 0., 0., 0., 0., 0., 0., 0.],  
#         [0., 0., 0., 0., 0., 0., 0., 0.]])
```

### 初始化模型参数

- 使用 Fashion-MNIST数据集。 我们定义具有两个隐藏层的多层感知机，每个隐藏层包含256个单元

```python
num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256
```

### 定义模型

- 将暂退法应用于每个隐藏层的输出（在激活函数之后），并且可以为每一层分别设置暂退概率：常见的技巧是在靠近输入层的地方设置较低的暂退概率。下面的模型将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5，并且暂退法只在训练期间有效

```python
class Net(nn.Module):
    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,
                 is_training=True):
        super(Net, self).__init__()
        self.num_inputs = num_inputs
        self.training = is_training
        self.lin1 = nn.Linear(num_inputs, num_hiddens1)
        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)
        self.lin3 = nn.Linear(num_hiddens2, num_outputs)
        self.relu = nn.ReLU()

    def forward(self, X):
        # 第一层隐藏层  
        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))
        # 只有在训练模型时才使用dropout  
        if self.training == True:
            # 在第一个全连接层之后添加一个dropout层  
            H1 = dropout_layer(H1, dropout1)
            # 第二层隐藏层  
        H2 = self.relu(self.lin2(H1))
        if self.training == True:
            # 在第二个全连接层之后添加一个dropout层  
            H2 = dropout_layer(H2, dropout2)
            # 输出层 未经过softmax函数  
        out = self.lin3(H2)
        return out
```

### 训练和测试

```python
num_epochs, lr, batch_size = 10, 0.5, 256
dropout1, dropout2 = 0.2, 0.5
net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)
loss = nn.CrossEntropyLoss(reduction='none')  # 交叉熵损失函数 用于多类分类  
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)  # 加载数据集  
trainer = torch.optim.SGD(net.parameters(), lr=lr)  # 梯度下降优化器  
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)  # 训练模型
```

- ![[00 Attachments/Pasted image 20240525153658.png|400]]

## 简洁实现

- 对于深度学习框架的高级API，我们只需在每个全连接层之后添加一个`Dropout`层， 将暂退概率作为唯一的参数传递给它的构造函数。
  在训练时，`Dropout`层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。 在测试时，`Dropout`层仅传递数据

```python
net = nn.Sequential(nn.Flatten(),  # 展平输入  
                    nn.Linear(784, 256),  # 第一层隐藏层  
                    nn.ReLU(),
                    # 在第一层隐藏层之后添加一个dropout层  
                    nn.Dropout(dropout1),
                    nn.Linear(256, 256),  # 第二层隐藏层  
                    nn.ReLU(),
                    # 在第二层隐藏层之后添加一个dropout层  
                    nn.Dropout(dropout2),
                    nn.Linear(256, 10))  # 输出层  


def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight)


net.apply(init_weights)  # 初始化权重  

trainer = torch.optim.SGD(net.parameters(), lr=lr)  # 梯度下降优化器  
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)  # 训练模型
```

# 4.3、数值稳定性、模型初始化、激活函数

## 数值稳定性

- 当神经网络很深的时候，数值很容易变得非常不稳定
- ![[00 Attachments/Pasted image 20240525181949.png|400]]
    - 进行了太多的矩阵乘法，可能导致梯度消失或梯度爆炸![[00 Attachments/Pasted image 20240525182225.png|400]]

### 梯度消失和梯度爆炸

- 梯度爆炸（gradient exploding）问题： 参数更新过大，破坏了模型的稳定收敛
- 梯度消失（gradient vanishing）问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习
- 产生原因![[00 Attachments/Pasted image 20240525183208.png|400]]
    - ![[00 Attachments/Pasted image 20240508141109.png|400]]$f(x_1)$ 只为 $x_1$ 的导数，所以关于 $x_2$ 的导数为
      0![[00 Attachments/Pasted image 20240525215020.png|400]]

#### 梯度爆炸

- 当W元素值大于1时，层数很深时，连乘会导致梯度爆炸![[00 Attachments/Pasted image 20240525220611.png|400]]
- ![[00 Attachments/Pasted image 20240525221031.png|400]]

#### 梯度消失

- ![[00 Attachments/Pasted image 20240525221741.png|400]]
-

当激活函数的输入稍微大一点时，它的导数就变为接近0，连续n个接近0的数相乘，最后的梯度就接近0，梯度就消失了![[00 Attachments/Pasted image 20240525221817.png|400]]

- ![[00 Attachments/Pasted image 20240525221958.png|300]]

### 总结

- 当梯度数值过大或者过小时会导致数值问题
- 常发生再深度模型中，因为其会对 n 个数累乘

## 模型初始化和激活函数

- 避免梯度过大或者过小
    - 就需要给梯度一个固定的期望和方差

### 让训练更加稳定

- ![[00 Attachments/Pasted image 20240525222501.png|400]]
- ![[00 Attachments/Pasted image 20240525222747.png|400]]

### 权重初始化

- 普通的初始化无法满足稳定性![[00 Attachments/Pasted image 20240525223207.png|400]]
    - 若权重初始化使损失位于陡峭处，会导致梯度大，进而导致更新的权重大，从而梯度累乘变大
    - 若权重初始化使损失位于平缓处，会导致梯度小，进而导致权重更新缓慢
- 为了让每层的方差都是个常数
- 假设没有激活函数![[00 Attachments/Pasted image 20240525223645.png|400]]
    - 假设当前层的权重与当前层的输入是独立同分布的
- ![[00 Attachments/Pasted image 20240525224120.png|400]]
    - 独立同分布的随机变量的乘积的期望，等于各自期望的乘积，这里各自期望等于0
    - 第 t 层的权重个数等于第 t-1 层的输出，所以累加后为 $n_{t-1}$
    - ==希望输入的方差与输出的方差一样==
- ![[00 Attachments/Pasted image 20240525230501.png|400]]

#### Xavier 初始化

- 保证梯度的稳定性
    - 第一个条件是使得每次前向输出的方差是一致的
    - 第二个条件是使得梯度是一样的
    - 除非输入等于输出，否则无法同时满足这两个条件
- Xavier 初始化通过给定当前层的输入和输出的大小，就能确定权重所要满足的方差的大小
- Xavier是常用的权重初始化方法：权重初始化的时候的方差是根据输入和输出维度来定的
- $n_{t-1}$（第 t-1 层神经元个数） 和 $n_t$（第 t
  层神经元个数）不一定相等![[00 Attachments/Pasted image 20240525231547.png|400]]
    - $$\begin{aligned}
      \frac{1}{2} (n_\mathrm{in} + n_\mathrm{out}) \sigma^2 = 1 \text{ 或等价于 }
      \sigma = \sqrt{\frac{2}{n_\mathrm{in} + n_\mathrm{out}}}
      \end{aligned}$$
    - 正太分布的第二项应该使标准差的平方，而不是标准差
    - $X服从U(a,b)，D(X) = (a-b)^2/12$

### 激活函数

-

假设有一个线性激活函数（通常不会使用，因为不会产生非线性性，这里是为了方便）![[00 Attachments/Pasted image 20240525232722.png|400]]![[00 Attachments/Pasted image 20240525234418.png|400]]
- 激活函数的输入和输出的方差有 $a^2$ 倍的关系，激活函数如果将值放大 $a$ 倍的话，它的方差会被放大 $a^2$ 倍
- 如果要==使激活函数不改变输入输出的方差==，则 $a=1$：
- 为了使得前向输出的均值和方差都是均值为 0，方差为固定的话，激活函数只能是β=0，a=1，即激活函数必须是等于本身

- ![[00 Attachments/Pasted image 20240526000217.png|400]]
    - 在 0 点附近的时候 tanh 和 relu 基本满足 $f(x)=x$ 的关系（趋近于 0 时，等价），sigmoid 需要做出调整
    - 图中蓝色曲线是经过调整之后的 sigmoid 激活函数，调整之后就能够解决 sigmoid 激活函数本身所存在的一些问题

### 总结

- 合理的权重初始值和激活函数的选取可以提升数值稳定性
    - 使得每一层的输出以及梯度都是一个期望为 0，方差为固定数的一个随即变量
    - 权重初始化选用
