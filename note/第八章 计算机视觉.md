```toc
```

## 图像增广（Image Augmentation）

- 大型数据集是成功应用神经网路的先决条件
- 图像增广
    - 对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模
    - 随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力
        - 例如以不用的方式裁剪图像，是感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖
        - 还可以调整亮度、颜色等因素来降低模型对颜色的敏感度
- ![[00 Attachments/Pasted image 20240622153039.png|400]]
    - 在实际中可能只拍摄了猫的一部分，但也要识别为猫

```python
import torch
import torchvision
from torch import nn
from d2l import torch as d2l

d2l.set_figsize()
img = d2l.Image.open('cat1.jpg')  # 读取图片  
d2l.plt.imshow(img)  # 显示图片  
d2l.plt.show()
```

- ![[00 Attachments/Pasted image 20240622155812.png|400]]
- 在输入图像`img`上多次运行图像增广方法`aug`并显示所有结果

```python
def apply(img, aug, num_rows=2, num_cols=4, scale=1.5):
    """  
    应用图像增广，并展示结果  
    """
    Y = [aug(img) for _ in range(num_rows * num_cols)]
    d2l.show_images(Y, num_rows, num_cols, scale=scale)
    d2l.plt.show()
```

### 翻转和裁剪

```python
apply(img, torchvision.transforms.RandomHorizontalFlip())  # 水平翻转  
apply(img, torchvision.transforms.RandomVerticalFlip())  # 垂直翻转
```

- 水平翻转![[00 Attachments/Pasted image 20240622160826.png|400]]
- 垂直翻转![[00 Attachments/Pasted image 20240622160832.png|400]]
- 在我们使用的示例图像中，猫位于图像的中间，但并非所有图像都是这样。
    - 在[6.5节](https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/pooling.html#sec-pooling)
      中，解释了汇聚层可以降低卷积层对目标位置的敏感性
    - 另外，可以通过对图像进行随机裁剪，使物体以不同的比例出现在图像的不同位置。这也可以降低模型对目标位置的敏感性
- 图像裁剪
    - 下面的代码将随机裁剪一个面积为原始面积10%到100%的区域，该区域的宽高比从0.5～2之间随机取值。 然后，区域的宽度和高度都被缩放到200像素

```python
shape_aug = torchvision.transforms.RandomResizedCrop(  # 裁剪并调整大小  
    (200, 200), scale=(0.1, 1), ratio=(0.5, 2))  # 裁剪尺寸为200x200, 缩放范围为(0.1, 1), 长宽比范围为(0.5, 2)  
apply(img, shape_aug)
```

- 裁剪![[00 Attachments/Pasted image 20240622160934.png|400]]

### 改变颜色

- 可以改变图像颜色的四个方面：亮度、对比度、饱和度和色调

```python
apply(img, torchvision.transforms.ColorJitter(
    brightness=0.5, contrast=0, saturation=0, hue=0))  # 亮度变化, 随机值为原始图像的50%到150%之间  
apply(img, torchvision.transforms.ColorJitter(
    brightness=0, contrast=0, saturation=0, hue=0.5))  # 色调变化, 50%的概率  
color_aug = torchvision.transforms.ColorJitter(
    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)  # 同时设置亮度、对比度、饱和度、色调变化的概率  
apply(img, color_aug)
```

- 亮度变化![[00 Attachments/Pasted image 20240622161948.png|400]]
- 色调变化![[00 Attachments/Pasted image 20240622162006.png|400]]
- 同时设置亮度、对比度、饱和度、色调变化![[00 Attachments/Pasted image 20240622162030.png|400]]

### 结合多种图像增广方法

- 通过使用一个`Compose`实例来综合上面定义的不同的图像增广方法，并将它们应用到每个图像

```python
augs = torchvision.transforms.Compose([  # Compose将多个增广组合起来  
    torchvision.transforms.RandomHorizontalFlip(), color_aug, shape_aug])
apply(img, augs)
```

- ![[00 Attachments/Pasted image 20240622162045.png|400]]

### 使用图像增广进行训练

- 切换使用数据集

```python
all_images = torchvision.datasets.CIFAR10(train=True, root="../data", download=False)
d2l.show_images([all_images[i][0] for i in range(32)], 4, 8, scale=0.8);  # 显示前32张训练集图片  
d2l.plt.show()
```

![[00 Attachments/Pasted image 20240622163031.png|400]]

```python
train_augs = torchvision.transforms.Compose([  # 训练集增广  
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ToTensor()])

test_augs = torchvision.transforms.Compose([  # 测试集增广  
    torchvision.transforms.ToTensor()])
```

- 定义一个辅助函数，以便于读取图像和应用图像增广

```python
def load_cifar10(is_train, augs, batch_size):
    """  
    加载CIFAR-10数据集, 并应用图像增广  
    """
    dataset = torchvision.datasets.CIFAR10(root="../data", train=is_train,
                                           transform=augs, download=False)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                             shuffle=is_train, num_workers=d2l.get_dataloader_workers())
    return dataloader
```

- 使用多GPU对模型进行训练和评估

```python
def train_batch_ch13(net, X, y, loss, trainer, devices):
    """用多GPU进行小批量训练"""
    if isinstance(X, list):  # isinstance()函数来判断对象是否是一个已知的类型  
        # 微调BERT中所需  
        X = [x.to(devices[0]) for x in X]
    else:
        X = X.to(devices[0])
    y = y.to(devices[0])
    net.train()  # 训练模式  
    trainer.zero_grad()
    pred = net(X)
    l = loss(pred, y)
    l.sum().backward()
    trainer.step()  # step()函数用来更新权重  
    train_loss_sum = l.sum()
    train_acc_sum = d2l.accuracy(pred, y)
    return train_loss_sum, train_acc_sum


# @save  
def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
               devices=d2l.try_all_gpus()):
    """用多GPU进行模型训练"""
    timer, num_batches = d2l.Timer(), len(train_iter)
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],
                            legend=['train loss', 'train acc', 'test acc'])
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])  # DataParallel用于并行计算  
    for epoch in range(num_epochs):
        # 4个维度：储存训练损失，训练准确度，实例数，特点数  
        metric = d2l.Accumulator(4)
        for i, (features, labels) in enumerate(train_iter):  # 遍历训练集 features是图像, labels是标签 小批量  
            timer.start()
            l, acc = train_batch_ch13(  # 训练一个小批量  
                net, features, labels, loss, trainer, devices)
            metric.add(l, acc, labels.shape[0], labels.numel())
            timer.stop()
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:  # 每5个batch输出一次信息  
                animator.add(epoch + (i + 1) / num_batches,
                             (metric[0] / metric[2], metric[1] / metric[3],
                              None))
        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)  # 计算测试集准确度  
        animator.add(epoch + 1, (None, None, test_acc))
        print(f'loss {metric[0] / metric[2]:.3f}, train acc '
              f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '          f'{str(devices)}')
```

- 现在，可以定义`train_with_data_aug`函数，使用图像增广来训练模型
    - 该函数获取所有的GPU，并使用Adam作为训练的优化算法，将图像增广应用于训练集，最后调用刚刚定义的用于训练和评估模型的
      `train_ch13`函数

```python
batch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10, 3)


def init_weights(m):
    """  
    初始化模型权重  
    """
    if type(m) in [nn.Linear, nn.Conv2d]:  # 全连接层和卷积层的权重初始化  
        nn.init.xavier_uniform_(m.weight)  # 使用Xavier初始化方法  


net.apply(init_weights)


def train_with_data_aug(train_augs, test_augs, net, lr=0.001):
    """  
    训练模型，使用图像增广  
    """
    train_iter = load_cifar10(True, train_augs, batch_size)
    test_iter = load_cifar10(False, test_augs, batch_size)
    loss = nn.CrossEntropyLoss(reduction="none")  # 交叉熵损失函数  
    trainer = torch.optim.Adam(net.parameters(), lr=lr)  # Adam优化器算是一个比较平滑的SGD，它对学习率调参不是很敏感  
    train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)
```

- 使用基于随机左右翻转的图像增广来训练模型

```python
train_with_data_aug(train_augs, test_augs, net)
# loss 0.162, train acc 0.944, test acc 0.807  
# 9049.0 examples/sec on [device(type='cuda', index=0)]  
# 总耗时：128.05秒
```

- ![[00 Attachments/Figure_16.png|400]]

## 微调 （fine-tuning）

- 对于一个中等规模的数据集
    - 复杂的模型可能会导致过拟合
    - 此外，由于训练样本数量有限，训练模型的准确性可能无法满足实际要求
- 为解决这种问题
    - 收集更多的数据
        - 但是耗费大量时间和金钱
    - 应用迁移学习（transfer learning）（技巧之一：微调）将从源数据学到的知识迁移到目标数据集
        - 例如，尽管ImageNet数据集中的大多数图像与椅子无关，但在此数据集上训练的模型==可能会提取更通用的图像特征，这有助于识别边缘、纹理、形状和对象组合==。
          这些类似的特征也可能有效地识别椅子

### 微调步骤

1. 在源数据集（例如ImageNet数据集）上预训练神经网络模型，即源模型
2. 创建一个新的神经网络模型，即目标模型
    - 这将复制源模型上的所有模型设计及其参数（输出层除外）
    - 我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集
    - 我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层
3. 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。
4. 在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。
   ![[00 Attachments/Pasted image 20240622230802.png|400]]

### 微调

- ![[00 Attachments/Pasted image 20240622223139.png|400]]
- ![[00 Attachments/Pasted image 20240622223149.png|400]]
- 微调步骤![[00 Attachments/Pasted image 20240622223159.png|400]]
    - 在自己的数据集上使用源数据集一样的的架构
    - 模型的参数不再是随机初始化了，而是从已经学习的地方复制过来
    - 最后一层由于标号不同，可以自由发挥
- ![[00 Attachments/Pasted image 20240622223811.png|400]]
    - 由于微调的使用，已经比较接近最优解，就不需要太大的学习率
    - 原来需要 迭代 10 次的，现在可能只要迭代 1、2 次
    - 源数据的类别数、图片数量、样本个数通常要大于目标数据集一百倍以上
- ![[00 Attachments/Pasted image 20240622225750.png|400]]
    - 源数据集中存在与目标数据集一致的标号，则最后一层相关标量的权重可以保留
- ![[00 Attachments/Pasted image 20240622225758.png|400]]
    - 可以说越深层，跟标号越相关
    - 固定了一些参数
        - 模型的复杂度更低了，可以认为是一种更强的正则
- ![[00 Attachments/Pasted image 20240622231241.png|400]]
    - 微调模型往往较随机初始化的模型效果更好，因为==微调的初始参数更有效==
- ==优势==
    1. **预训练的优势**
        - 预训练模型通常是在大规模数据集（如ImageNet）上进行训练的。这样的模型已经学会了许多有用的特征表示，这些特征可以很好地泛化到其他任务。
        - 通过微调预训练模型，可以利用这些已学到的特征，而不是从零开始训练模型。
    2. **特征泛化能力**
        - 预训练模型的初始层学到的是低级特征（如边缘、纹理等），这些特征在很多视觉任务中都是通用的。
        - 通过微调，仅需调整模型的高层次特征，使其适应新的任务和数据集。这种方法利用了模型的泛化能力，加速了训练过程。
    3. **减少过拟合风险**
        - 从头开始训练一个深度学习模型，需要大量的训练数据。如果训练数据不足，模型容易过拟合。
        - 而微调预训练模型，由于预训练模型已经学习了丰富的特征表示，可以在相对较少的数据上微调，从而减少过拟合风险。
    4. **加速训练过程**
        - 从零开始训练一个深度学习模型，需要花费大量的计算资源和时间。
        - 而微调预训练模型，只需在新的数据集上进行少量的训练即可，大大减少了训练时间和计算资源。
    5. **更高的性能**
        - 由于预训练模型已经在大量数据上进行过训练，其参数已经调整到一个比较好的状态（初始的参数更有效）
        - 在此基础上进行微调，可以比从头开始训练更快地收敛到一个更好的结果。实验证明，微调预训练模型通常能达到比从零开始训练更高的性能。

### 热狗识别

#### 获取数据集

- 我们将在一个小型数据集上微调ResNet模型
    - 该模型已在 ImageNet 数据集上进行了预训练
    - 这个小型数据集包含数千张包含热狗和不包含热狗的图像，我们将使用微调模型来识别图像中是否包含热狗

```python
import os
import torch
import torchvision
from torch import nn
from d2l import torch as d2l

d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip',
                          'fba480ffa8aa7e0febbb511d181409f899b9baa5')  # 下载数据集  
data_dir = d2l.download_extract('hotdog')  # 下载解压数据集  

train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))  # 读取训练集图片  
test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'))  # 读取测试集图片  

hotdogs = [train_imgs[i][0] for i in range(8)]  # 前8张热狗图片  
not_hotdogs = [train_imgs[-i - 1][0] for i in range(8)]  # 后8张非热狗图片  
d2l.show_images(hotdogs + not_hotdogs, 2, 8, scale=1.4)  # 显示热狗和非热狗图片
```

- ![[00 Attachments/Pasted image 20240623170327.png|400]]
- 在训练期间，我们首先从图像中裁切随机大小和随机长宽比的区域，然后将该区域缩放为224×224输入图像
- 在测试过程中，我们将图像的高度和宽度都缩放到256像素，然后裁剪中央224×224区域作为输入。
- 此外，对于RGB（红、绿和蓝）颜色通道，我们分别标准化每个通道
    - 具体而言，该通道的每个值减去该通道的平均值，然后将结果除以该通道的标准差

```python
# 使用RGB通道的均值和标准差，以标准化每个通道  
normalize = torchvision.transforms.Normalize(
    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # imagenet数据集的均值和标准差  

# 数据增广  
train_augs = torchvision.transforms.Compose([
    torchvision.transforms.RandomResizedCrop(224),  # 随机裁剪为源图像的大小  
    torchvision.transforms.RandomHorizontalFlip(),  # 随机水平翻转  
    torchvision.transforms.ToTensor(),  # 转换为Tensor  
    normalize])  # 标准化  

test_augs = torchvision.transforms.Compose([
    torchvision.transforms.Resize([256, 256]),  # 调整图像大小为256×256  
    torchvision.transforms.CenterCrop(224),  # 裁剪图像中央的224×224  
    torchvision.transforms.ToTensor(),
    normalize])
```

#### 定义和初始化模型

- 我们使用在ImageNet数据集上预训练的ResNet-18作为源模型
    - 在这里，我们指定`pretrained=True`以自动下载预训练的模型参数
    - 如果首次使用此模型，则需要连接互联网才能下载

```python
pretrained_net = torchvision.models.resnet18(pretrained=True)  # 预训练的ResNet-18模型  pretrained: 是否加载预训练模型的参数  
print(pretrained_net)  # 打印模型结构  
# ResNet(  
#   (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)  
#   (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#   (relu): ReLU(inplace=True)  
#   (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)  
#   (layer1): Sequential(  
#     (0): BasicBlock(  
#       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       (relu): ReLU(inplace=True)  
#       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#     )  
#     (1): BasicBlock(  
#       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       (relu): ReLU(inplace=True)  
#       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#     )  
#   )  
#   (layer2): Sequential(  
#     (0): BasicBlock(  
#       (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)  
#       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       (relu): ReLU(inplace=True)  
#       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#       (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       (downsample): Sequential(  
#         (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)  
#         (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       )  
#     )  
#     (1): BasicBlock(  
#       (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       (relu): ReLU(inplace=True)  
#       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#       (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#     )  
#   )  
#   (layer3): Sequential(  
#     (0): BasicBlock(  
#       (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)  
#       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       (relu): ReLU(inplace=True)  
#       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       (downsample): Sequential(  
#         (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)  
#         (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       )  
#     )  
#     (1): BasicBlock(  
#       (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#       (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       (relu): ReLU(inplace=True)  
#       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#     )  
#   )  
#   (layer4): Sequential(  
#     (0): BasicBlock(  
#       (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)  
#       (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       (relu): ReLU(inplace=True)  
#       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#       (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       (downsample): Sequential(  
#         (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)  
#         (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       )  
#     )  
#     (1): BasicBlock(  
#       (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#       (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#       (relu): ReLU(inplace=True)  
#       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#       (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#     )  
#   )  
#   (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))  
#   (fc): Linear(in_features=512, out_features=1000, bias=True)  
# )  
print(pretrained_net.fc)  # 打印输出层  
# Linear(in_features=512, out_features=1000, bias=True)
```

- 在ResNet的全局平均汇聚层后，全连接层转换为ImageNet数据集的 1000 个类输出
- 之后，我们构建一个新的神经网络作为目标模型
    - 它的定义方式与预训练源模型的定义方式相同，只是最终层中的输出数量被设置为目标数据集中的类数（而不是1000个）
- 在下面的代码中，目标模型`finetune_net`中成员变量`features`的参数被初始化为源模型相应层的模型参数
    - 由于模型参数是在ImageNet数据集上预训练的，并且足够好，因此通常只需要较小的学习率即可微调这些参数
- 成员变量`output`的参数是随机初始化的，通常需要更高的学习率才能从头开始训练。 假设`Trainer`实例中的学习率为𝜂，我们将成员变量
  `output`中参数的学习率设置为10𝜂。

```python
finetune_net = torchvision.models.resnet18(pretrained=True)  # 微调的ResNet-18模型  
finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)  # 重新定义输出层 2个类别  
nn.init.xavier_uniform_(finetune_net.fc.weight)  # 使用Xavier初始化输出层(最后一层)
```

#### 微调模型

```python
# 如果param_group=True，输出层中的模型参数将使用十倍的学习率  
def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5,
                      param_group=True):
    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'train'), transform=train_augs),
        batch_size=batch_size, shuffle=True)
    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(
        os.path.join(data_dir, 'test'), transform=test_augs),
        batch_size=batch_size)
    devices = d2l.try_all_gpus()
    loss = nn.CrossEntropyLoss(reduction="none")
    if param_group:
        """  
        除了最后一层的learning rate外，用的是默认的learning rate  
        最后一层的learning rate用的是十倍的learning rate  
        """
        params_1x = [param for name, param in net.named_parameters()  # named_parameters: 网络中的参数  
                     if name not in ["fc.weight", "fc.bias"]]  # 不包括最后一层的模型参数  
        trainer = torch.optim.SGD([{'params': params_1x},
                                   {'params': net.fc.parameters(), 'lr': learning_rate * 10}],  # params: 要优化的参数  
                                  # 因为最后一层是随机初始化的，希望它学习的更快，所以用十倍的学习率  
                                  lr=learning_rate, weight_decay=0.001)  # weight_decay: 正则化参数  
    else:  # 不使用源模型的参数组  
        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,
                                  weight_decay=0.001)
    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
```

- 训练微调模型

```python
train_fine_tuning(finetune_net, 5e-5)  # 学习率很小  
# loss 0.223, train acc 0.924, test acc 0.934  
# 1879.0 examples/sec on [device(type='cuda', index=0)]
```

- ![[00 Attachments/Pasted image 20240623170415.png]]
    - 可以看到，模型精确度一开始就很高（很快就训练完成，第二个周期时就差不多了）
- 从头开始训练模型
- 参数都是随机的

```python
scratch_net = torchvision.models.resnet18()
scratch_net.fc = nn.Linear(scratch_net.fc.in_features, 2)  # 重新定义输出层 2个类别  
train_fine_tuning(scratch_net, 5e-4, param_group=False)  # 学习率较大  
# loss 0.366, train acc 0.839, test acc 0.804  
# 2086.2 examples/sec on [device(type='cuda', index=0)]
```

- ![[00 Attachments/Pasted image 20240623170429.png]]
    - 增长缓慢

## 物体检测和数据集

### 物体检测

- 在图像分类任务中，我们假设图像中只有一个主要物体对象，我们只关注如何识别其类别
- 然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的具体位置
    - 在计算机视觉里，我们将这类任务称为目标检测（object detection）或目标识别（object recognition）
- 图片分类是目标检测的基础![[00 Attachments/Pasted image 20240623213640.png|400]]

### 边界框实现（bounding box）

- 在目标检测中，我们通常使用边界框（bounding box）来描述对象的空间位置
- 边界框是矩形的，由矩形左上角的以及右下角的𝑥和𝑦坐标决定
    - 另一种常用的边界框表示方法是边界框中心的(𝑥,𝑦)轴坐标以及框的宽度和高度
- ![[00 Attachments/Pasted image 20240623213713.png|400]]

```python
import torch
from d2l import torch as d2l

d2l.set_figsize()
img = d2l.plt.imread('catdog.jpg')
d2l.plt.imshow(img)
```

- ![[00 Attachments/Pasted image 20240623214838.png|400]]
- 定义在这两种表示法之间进行转换的函数：`box_corner_to_center`从两角表示法转换为中心宽度表示法，而`box_center_to_corner`
  反之亦然。 输入参数`boxes`可以是长度为4的张量，也可以是形状为（𝑛，4）的二维张量，其中𝑛是边界框的数量

```python
# @save  
def box_corner_to_center(boxes):
    """从（左上，右下）转换到（中间，宽度，高度）"""
    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    cx = (x1 + x2) / 2
    cy = (y1 + y2) / 2
    w = x2 - x1
    h = y2 - y1
    boxes = torch.stack((cx, cy, w, h), axis=-1)
    return boxes


# @save  
def box_center_to_corner(boxes):
    """从（中间，宽度，高度）转换到（左上，右下）"""
    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    x1 = cx - 0.5 * w
    y1 = cy - 0.5 * h
    x2 = cx + 0.5 * w
    y2 = cy + 0.5 * h
    boxes = torch.stack((x1, y1, x2, y2), axis=-1)
    return boxes
```

- 将根据坐标信息定义图像中狗和猫的边界框
- 图像中坐标的原点是图像的左上角，向右的方向为𝑥轴的正方向，向下的方向为𝑦轴的正方向

```python
# bbox是边界框的英文缩写
dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]
```

- 通过转换两次来验证边界框转换函数的正确性

```python
boxes = torch.tensor((dog_bbox, cat_bbox))  # 将数据转换为张量  
print(box_center_to_corner(box_corner_to_center(boxes)) == boxes)  # 转换为中心坐标后再转换回来  
# tensor([[True, True, True, True],  
#         [True, True, True, True]])
```

- 将边界框在图中画出，以检查其是否准确
- 画之前，定义一个辅助函数`bbox_to_rect`
    - 它将边界框表示成`matplotlib`的边界框格式
- ![[00 Attachments/Pasted image 20240623221245.png|400]]

### 目标检测数据集

- 包含所有图像和CSV标签文件的香蕉检测数据集可以直接从互联网下载

```python
import os
import pandas as pd
import torch
import torchvision
from d2l import torch as d2l

# @save  
d2l.DATA_HUB['banana-detection'] = (  # 下载数据集  
    d2l.DATA_URL + 'banana-detection.zip',
    '5de26c8fce5ccdea9f91267273464dc968d20d72')
```

- 通过`read_data_bananas`函数，读取香蕉检测数据集
    - 该数据集包括一个的CSV文件，内含目标类别标签和位于左上角和右下角的真实边界框坐标

```python
def read_data_bananas(is_train=True):
    """读取香蕉检测数据集中的图像和标签"""
    data_dir = d2l.download_extract('banana-detection')  # 下载解压数据集  
    csv_fname = os.path.join(data_dir, 'bananas_train' if is_train else 'bananas_val', 'label.csv')
    csv_data = pd.read_csv(csv_fname)
    csv_data = csv_data.set_index('img_name')
    images, targets = [], []
    # 把图片、标号全部读到内存里面
    for img_name, target in csv_data.iterrows():
        images.append(torchvision.io.read_image(
            os.path.join(data_dir, 'bananas_train' if is_train else
            'bananas_val', 'images', f'{img_name}')))
        # 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y），  
        # 其中所有图像都具有相同的香蕉类（索引为0）  
        targets.append(list(target))
    return images, torch.tensor(targets).unsqueeze(1) / 256
```

- 通过使用`read_data_bananas`函数读取图像和标签
- 以下`BananasDataset`类别将允许我们创建一个自定义`Dataset`实例来加载香蕉检测数据集

```python
class BananasDataset(torch.utils.data.Dataset):
    """一个用于加载香蕉检测数据集的自定义数据集"""

    def __init__(self, is_train):
        self.features, self.labels = read_data_bananas(is_train)
        print('read ' + str(len(self.features))
              + (f' training examples' if is_train else f' validation examples'))


def __getitem__(self, idx):
    return (self.features[idx].float(), self.labels[idx])


def __len__(self):
    return len(self.features)  # 标签值归一化到0-1之间
```

- 定义`load_data_bananas`函数，来为训练集和测试集返回两个数据加载器实例
    - 对于测试集，无须按随机顺序读取它

```python
def load_data_bananas(batch_size):  # 定义一个函数来小批量加载数据集  
    """加载香蕉检测数据集"""
    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True),
                                             batch_size, shuffle=True)
    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False),
                                           batch_size)
    return train_iter, val_iter
```

- 读取一个小批量，并打印其中的图像和标签的形状
    - 图像的小批量的形状为（批量大小、通道数、高度、宽度）
        - 看起来很眼熟：它与我们之前图像分类任务中的相同。
    - 标签的小批量的形状为（批量大小，𝑚，5）
        - 其中𝑚是数据集的任何图像中边界框可能出现的最大数量。
- 小批量计算虽然高效，但它要求每张图像含有相同数量的边界框，以便放在同一个批量中
- 通常来说，图像可能拥有不同数量个边界框
    - 因此，在达到 𝑚 之前，边界框少于 𝑚 的图像将被非法边界框填充。这样，每个边界框的标签将被长度为5的数组表示
    - 数组中的第一个元素是边界框中对象的类别，其中 -1 表示用于填充的非法边界框
    - 数组的其余四个元素是边界框左上角和右下角的（𝑥，𝑦）坐标值（值域在0～1之间）
    - 对于香蕉数据集而言，由于每张图像上只有一个边界框，因此 𝑚=1

```python
batch_size, edge_size = 32, 256
train_iter, _ = load_data_bananas(batch_size)
batch = next(iter(train_iter))
print(batch[0].shape, batch[1].shape)
# torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 5])
```

- 演示
- pytorch 里 permute 是改变参数维度的函数，
    - Dataset里读的img维度是[batch_size, RGB, h, w]，
    - 但是plt画图的时候要求是[h, w, RGB]，所以要调整一下
- 做图片的时候，一般是会用一个ToTensor()将图片归一化到【0, 1】，这样收敛更快

```python
imgs = (batch[0][0:10].permute(0, 2, 3, 1)) / 255  # # permute 将通道维移到最后面 并将像素值归一化到0-1之间
axes = d2l.show_images(imgs, 2, 5, scale=2)
for ax, label in zip(axes, batch[1][0:10]):
    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])
```

- ![[00 Attachments/Pasted image 20240623232536.png|400]]

## 锚框（anchor box）

- 目标检测算法通常会==在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界从而更准确地预测目标的真实边界框==（ground-truth
  bounding box）
- 不同的模型使用的区域采样方法可能不同
- 这里我们介绍其中的一种方法：
    - 以每个像素为中心，生成多个缩放比和宽高比（aspect ratio）不同的边界框
    - 这些边界框被称为锚框（anchor box）
- 在卷积神经网络中，最后用 softmax 完成分类，但==随着的加入，使得问题变得更加复杂==
- 锚框就是算法预测的边框，边缘框（bounding
  box）就是真实标签所处区域的边框![[00 Attachments/Pasted image 20240624072310.png|400]]

### 生成多个锚框

- ==当我们不清楚物体会出现在什么地方时，那就对每个像素中心生成大小不一的锚框，希望其中有一些能尽可能的覆盖样本（实际边缘框）==
    - 但反过来，生成的锚框越多，计算量就越大，所以说==锚框的质量非常关键==
    - 如何生成高质量的锚框是研究的关键
- 假设输入图像的高度为 $ℎ$，宽度为 $𝑤$。我们以图像的每个像素为中心生成不同形状的锚框：缩放比为 $𝑠∈(0,1]$，宽高比为 $𝑟>0$
  ，那么锚框的宽度和高度分别是 $hs\sqrt{r}$ 和 $ℎ𝑠/\sqrt{𝑟}$
    - 请注意，当中心位置给定时，已知宽和高的锚框是确定的
- 要生成多个不同形状的锚框，需要设置许多缩放比（scale）取值 $𝑠_1,…,𝑠_𝑛$ 和许多宽高比（aspect ratio）取值 $𝑟_1,…,𝑟_𝑚$
    - 当使用这些比例和长宽比的所有组合以每个像素为中心时，输入图像将总共有 $𝑤ℎ𝑛𝑚$ 个锚框（$wh$ 为像素个数）。
      尽管这些锚框可能会覆盖所有真实边界框，但计算复杂性很容易过高
    - 在实践中，我们只考虑包含 $𝑠_1$ 或 $𝑟_1$ 的组合（固定缩放比为 $s_1$
      或者高宽比为 $r_1$）：$$(s_1, r_1), (s_1, r_2), \ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \ldots, (s_n, r_1).$$
    - 也就是说，以同一像素为中心的锚框的数量是 $𝑛+𝑚−1$（固定缩放比为 $s_1$ 的组合有 $m$ 个，固定高宽比为 $r_1$ 的组合有 $n$
      个）
    - 对于整个输入图像，将共生成 $𝑤ℎ(𝑛+𝑚−1)$ 个锚框

#### 代码实现

- 上述生成锚框的方法在下面的`multibox_prior`函数中实现
    - 我们指定输入图像、尺寸列表和宽高比列表，然后此函数将返回所有的锚框

```python
import torch
from d2l import torch as d2l

torch.set_printoptions(2)  # 精简输出精度  


# @save  
def multibox_prior(data, sizes, ratios):
    """生成以每个像素为中心具有不同形状的锚框  
    :param data: 输入图像  
    :param sizes: 锚框的缩放比  
    :param ratios: 锚框的长宽比  
    :return: 锚框的中心点坐标(x,y)和长宽(h,w)  
    """
    in_height, in_width = data.shape[-2:]
    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)  # num_ 为缩放比和长宽比的个数  
    boxes_per_pixel = (num_sizes + num_ratios - 1)  # 每个像素生成的锚框数  
    size_tensor = torch.tensor(sizes, device=device)  # 将锚框缩放比列表转为张量并将其移动到指定设备  
    ratio_tensor = torch.tensor(ratios, device=device)

    # 为了将锚点移动到像素的中心，需要设置偏移量。  
    # 因为一个像素的高为1且宽为1，我们选择偏移我们的中心0.5  
    offset_h, offset_w = 0.5, 0.5
    steps_h = 1.0 / in_height  # 计算高度方向上的步长  
    steps_w = 1.0 / in_width  # 计算宽度方向上的步长  

    # torch.arange(in_height, device=device)获得每一行像素  
    # (torch.arange(in_height, device=device) + offset_h) 获得每一行像素的中心  
    # (torch.arange(in_height, device=device) + offset_h) * steps_h 对每一行像素的中心坐标作归一化处理  

    # 生成归一化的高度和宽度方向上的像素点中心坐标  
    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h
    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w
    # 生成坐标网络  
    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')
    # 将坐标网络平铺为一维张量  
    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)

    # 计算每个锚框的宽度和高度 （高宽一一对应）  
    # cat 函数将两个张量拼接在一起
    # 进行了归一化处理
    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),  # 固定高宽比，下一行固定缩放比例  
                   sizes[0] * torch.sqrt(ratio_tensor[1:])))
    *in_height / in_width  # 处理矩形输入  


h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),
               sizes[0] / torch.sqrt(ratio_tensor[1:])))

# # 除以2来获得半高和半宽（相对于锚框中心的偏移量），用来计算锚框的左上角和右下角坐标  
anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(in_height * in_width, 1) / 2

# 计算所有锚框的中心坐标，每个像素对应boxes_per_pixel个锚框  
out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],
                       dim=1).repeat_interleave(boxes_per_pixel, dim=0)

# 将锚框的中心坐标与锚框的半宽高相加得到锚框的左上角和右下角坐标  
output = out_grid + anchor_manipulations

return output.unsqueeze(0)  # 增加一个维度作为批量维度
```

- 在计算时 w 时，相较于 h 额外进行了 `* in_height / in_width`
    - 因为什么？？？？？？？？
- 可以看到返回的锚框变量`Y`的形状是（批量大小，锚框的数量，4）

```python
# 返回锚框变量Y的形状  
img = d2l.plt.imread('catdog.jpg')
print("img.shape：", img.shape)  # 高561，宽72，3通道  
# img.shape： (561, 728, 3)h, w = img.shape[:2]  
print(h, w)
# 561 728  

X = torch.rand(size=(1, 3, h, w))
Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])
print(Y.shape)
# torch.Size([1, 2042040, 4])  # 1是批量维度，2042040是锚框个数，4是单个锚框的左上角坐标和右下角坐标(x1, y1, x2, y2)
```

- 将锚框变量`Y`的形状更改为(图像高度,图像宽度,以同一像素为中心的锚框的数量,4)后，可以获得以指定像素的位置为中心的所有锚框
- 在接下来的内容中，我们访问以（250,250）为中心的第一个锚框
    - 它有四个元素：锚框左上角的(𝑥,𝑦)轴坐标和右下角的(𝑥,𝑦)轴坐标
    - 输出中两个轴的坐标各分别除以了图像的宽度和高度

```python
# 访问以(250,250)为中心的第一个锚框  
boxes = Y.reshape(h, w, 5, 4)  # 上面的sizes×sizes=3×3，3+3-1=5，故每个像素为中心生成五个锚框  
print(boxes[250, 250, 0, :])  # 以250×250为中心的第一个锚框的坐标  
# tensor([0.06, 0.07, 0.63, 0.82])
```

- 为了显示以图像中以某个像素为中心的所有锚框，定义下面的`show_bboxes`函数来在图像上绘制多个边界框

```python
# 显示以图像中一个像素为中心的所有锚框  
def show_bboxes(axes, bboxes, labels=None, colors=None):
    """  
    显示所有边界框  
    :param axes: 图像坐标轴  
    :param bboxes: 边界框坐标，(x1, y1, x2, y2)  
    :param labels: 边界框标签  
    :param colors: 边界框颜色  
    """

    def _make_list(obj, default_values=None):
        # 如果obj为None，使用默认值；如果obj不是列表或元组，将其转换为列表  
        if obj is None:
            obj = default_values
        elif not isinstance(obj, (list, tuple)):
            obj = [obj]
        return obj

        # 处理labels，确保其为列表形式  

    labels = _make_list(labels)
    # 处理colors，确保其为列表形式  
    colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])
    # 遍历所有边界框  
    for i, bbox in enumerate(bboxes):
        # 选择颜色  
        color = colors[i % len(colors)]
        # 使用边界框和颜色生成矩形框  
        rect = d2l.bbox_to_rect(bbox.detach().numpy(), color)
        # 在图像上添加矩形框  
        axes.add_patch(rect)
        # 如果存在标签  
        if labels and len(labels) > i:
            # 根据边界框的颜色选择标签的颜色  
            text_color = 'k' if color == 'w' else 'w'
            # 在边界框上添加标签  
            axes.text(rect.xy[0], rect.xy[1], labels[i], va='center',
                      ha='center', fontsize=9, color=text_color,
                      bbox=dict(facecolor=color, lw=0))
```

- 正如从上面代码中所看到的，变量`boxes`中𝑥轴和𝑦轴的坐标值已分别除以图像的宽度和高度（?）
- 绘制锚框时，我们需要恢复它们原始的坐标值。 因此，在下面定义了变量`bbox_scale`。现在可以绘制出图像中所有以(250,250)为中心的锚框了

```python
# 设置图像大小  
d2l.set_figsize()
# 创建一个张量来缩放边界框的尺寸  
bbox_scale = torch.tensor((w, h, w, h))
# 在图像上显示图像  
fig = d2l.plt.imshow(img)
print("fig.axes：", fig.axes)
# fig.axes： Axes(0.125,0.11;0.775x0.77)# 在生成锚框的时候是0-1的值，进行缩放的话就可以省点乘法运算，因为最后输出并不需要显示所有锚框，所以可能会更快一点  
print("boxes[250,250,:,:]：", boxes[250, 250, :, :])
# boxes[250,250,:,:]： tensor([[ 0.06,  0.07,  0.63,  0.82],#         [ 0.15,  0.20,  0.54,  0.70],  
#         [ 0.25,  0.32,  0.44,  0.57],  
#         [-0.06,  0.18,  0.75,  0.71],  
#         [ 0.14, -0.08,  0.55,  0.98]])  
print("bbox_scale：", bbox_scale)
# bbox_scale： tensor([728, 561, 728, 561])print("boxes[250,250,:,:] * bbox_scale：", boxes[250, 250, :, :] * bbox_scale)  
# boxes[250,250,:,:] * bbox_scale： tensor([[ 40.13,  40.12, 460.88, 460.87],#         [110.25, 110.25, 390.75, 390.75],  
#         [180.38, 180.38, 320.62, 320.62],  
show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,
            ['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75,r=2', 's=0.75,r=0.5'])  # 画出以250×250像素为中心的不同高宽比的五个锚框
```

-

如下所示，缩放比为0.75且宽高比为1的蓝色锚框很好地围绕着图像中的狗![[00 Attachments/Pasted image 20240627140026.png|400]]

### 交并比（intersection over union，IoU）

- 比较锚框与真实框的相似度![[00 Attachments/Pasted image 20240627141807.png|400]]
- 如果已知目标的真实边界框，那么这里的“好”该如何如何量化呢？
    - 直观地说，可以衡量锚框和真实边界框之间的相似性
    - $$J(\mathcal{A},\mathcal{B}) = \frac{\left|\mathcal{A} \cap \mathcal{B}\right|}{\left| \mathcal{A} \cup \mathcal{B}\right|}$$
    - 交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框完全重合
    - 交并比越大越好

#### 代码实现

- 接下来部分将使用交并比来衡量锚框和真实边界框之间、以及不同锚框之间的相似度
- 给定两个锚框或边界框的列表，以下`box_iou`函数将在这两个列表中计算它们成对的交并比

```python
# 交并比(IoU)  
def box_iou(boxes1, boxes2):
    """计算两个锚框或边界框列表中成对的交并比"""
    # 定义一个lambda函数，计算一个锚框或边界框的面积  
    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *
                              (boxes[:, 3] - boxes[:, 1]))
    # 计算boxes1中每个框的面积  
    areas1 = box_area(boxes1)
    # 计算boxes2中每个框的面积  
    areas2 = box_area(boxes2)
    # 计算交集区域的左上角坐标（对于每对框，取其左上角坐标的最大值）  
    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])
    # 计算交集区域的右下角坐标（对于每对框，取其右下角坐标的最小值）  
    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])
    # 计算交集区域的宽和高（如果交集不存在，宽和高为0）  
    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)
    # 计算交集区域的面积  
    inter_areas = inters[:, :, 0] * inters[:, :, 1]
    # 计算并集区域的面积（boxes1的面积 + boxes2的面积 - 交集的面积）  
    union_areas = areas1[:, None] + areas2 - inter_areas
    # 返回交并比（交集的面积除以并集的面积）  
    return inter_areas / union_areas
```

### 在训练数据中标注锚框

- ![[00 Attachments/Pasted image 20240627143434.png|400]]
    - 大量锚框包括的是背景，正负样本不均
- 在训练集中，我们==将每个锚框视为一个训练样本==
- 为了训练目标检测模型，我们需要==每个锚框的类别（class）和偏移量（offset）标签==
    - 其中前者是与锚框相关的对象的类别
    - 后者是真实边界框相对于锚框的偏移量
- 在预测时，我们为每个图像生成多个锚框，预测所有锚框的类别和偏移量，==根据预测的偏移量调整它们的位置以获得预测的边界框==，最后只输出符合特定条件的预测边界框
- 目标检测训练集带有真实边界框的位置及其包围物体类别的标签，要标记任何生成的锚框，我们可以参考分配到的最接近此锚框的真实边界框的位置和类别标签。
  下文将介绍一个算法，它能够把最接近的真实边界框分配给锚框

#### 将真实边界框分配给锚框

-

并不是所有的锚框都是有用的（有些锚框为背景），需要提取出与边缘框相似度较大的锚框![[00 Attachments/Pasted image 20240627143839.png|400]]
- 矩阵中的元素为该锚框与边缘框的 IoU

- 赋予标号过程如下
    - 在矩阵 $\mathbf{X}$ 中找到最大的元素，假设它的行索引和列索引分别表示为 $𝑖_1$ 和 $𝑗_1$
    - 然后将真实边界框 $B_{j_1}$ 分配给锚框 $𝐴_{𝑖_1}$。这很直观，因为 $𝐴_{𝑖_1}$ 和 $B_{j_1}$ 是所有锚框和真实边界框配对中最相近的
    - 在第一个分配完成后，丢弃矩阵中 ${i_1}^\mathrm{th}$ 行和 ${j_1}^\mathrm{th}$ 列中的所有元素
    - 重复上述直至所有边缘框已被分配
    - 之后，只需要遍历剩余的锚框，然后根据阈值（与实际边缘框的相似度的最小值）确定是否为它们分配真实边界框

```python
# 将真实边界框分配给锚框  
def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):
    """将最接近的真实边界框分配给锚框"""

    # 获取锚框和真实边界框的数量  
    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]

    # 计算所有的锚框和真实边缘框的IOU  
    jaccard = box_iou(anchors, ground_truth)  # jaccard[i, j]表示第i个锚框和第j个真实边界框的IOU  

    # 创建一个长度为num_anchors的张量，用-1填充，存储每个锚框分配的真实边界框的索引，默认-1 表示背景
    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long, device=device)

    # 对于每个锚框，找到与其IoU最大的真实边界框  
    max_ious, indices = torch.max(jaccard, dim=1)

    # 找到IoU大于等于阈值（如0.5）的锚框，将这些锚框分配给对应的真实边界框  
    anc_i = torch.nonzero(max_ious >= 0.5).reshape(-1)  # 找到IoU大于等于阈值的锚框的索引  
    box_j = indices[max_ious >= 0.5]  # 找到与这些锚框对应的真实边界框的索引  
    anchors_bbox_map[anc_i] = box_j  # 将这些锚框分配给对应的真实边界框  

    # 初始化用于删除行和列的张量  
    col_discard = torch.full((num_anchors,), -1)
    row_discard = torch.full((num_gt_boxes,), -1)

    # 通过迭代找到IoU最大的锚框，并将其分配给对应的真实边界框  
    for _ in range(num_gt_boxes):
        max_idx = torch.argmax(jaccard)  # 找IOU最大的锚框  
        box_idx = (max_idx % num_gt_boxes).long()  # 通过取余数操作，得到该元素对应的真实边界框的索引  
        anc_idx = (max_idx / num_gt_boxes).long()  # 通过整除操作，得到该元素对应的锚框的索引  

        # 更新锚框到真实边界框的映射  
        anchors_bbox_map[anc_idx] = box_idx

        # 在jaccard矩阵中删除已分配的锚框所在的行和列，以避免重复分配  
        jaccard[:, box_idx] = col_discard  # 把最大Iou对应的锚框在 锚框-类别 矩阵中的一列删掉  
        jaccard[anc_idx, :] = row_discard  # 把最大Iou对应的锚框在 锚框-类别 矩阵中的一行删掉  

    # 函数返回一个张量anchors_bbox_map，它的长度与锚框的数量相同。  
    # 这个张量用于存储每个锚框分配到的真实边界框的索引。  
    # 如果某个锚框没有分配到真实边界框，那么在这个张量中对应的位置就会是-1。  
    # 如果某个锚框分配到了真实边界框，那么在这个张量中对应的位置就会是分配到的真实边界框的索引。  
    # 例如，如果我们有5个锚框和3个真实边界框，那么anchors_bbox_map可能会是这样的：[0, -1, 1, 2, -1]。这表示第1个锚框被分配到了第1个真实边界框，第2个锚框没有被分配到真实边界框，第3个锚框被分配到了第2个真实边界框，第4个锚框被分配到了第3个真实边界框，第5个锚框没有被分配到真实边界框。  
    return anchors_bbox_map
```

- `anchors_bbox_map` 在代码中进行两次分配，原因在于处理不同的场景和确保正确分配所有锚框：
    - **第一次分配**：
        - 代码片段：`max_ious, indices = torch.max(jaccard, dim=1)` 和 `anchors_bbox_map[anc_i] = box_j`
        - 解释：首先，计算每个锚框与所有真实边界框的 IoU，找出每个锚框的最大 IoU 及其对应的真实边界框索引。然后，将那些最大
          IoU 大于或等于阈值的锚框分配给对应的真实边界框。这一步确保了大部分锚框能够根据阈值进行合理分配
        - 此时一个真实边缘框可能会被多个锚框关联（极端情况下，所有锚框都关联了一个边缘框），导致其他边缘框没有被关联
    - **第二次分配**：
        - 代码片段：
          ```python
          for _ in range(num_gt_boxes):
              max_idx = torch.argmax(jaccard)
              box_idx = (max_idx % num_gt_boxes).long()
              anc_idx = (max_idx / num_gt_boxes).long()
              anchors_bbox_map[anc_idx] = box_idx
              jaccard[:, box_idx] = col_discard
              jaccard[anc_idx, :] = row_discard
          ```
        - 解释：第二次分配是为了==确保所有真实边界框至少被一个锚框分配==。在第一次分配之后，有些真实边界框可能还没有被任何锚框分配。因此，使用一个循环来确保每个真实边界框至少分配给一个锚框。具体步骤是找到剩余的最大
          IoU 值及其对应的锚框和真实边界框索引，并进行分配，然后将已分配的锚框和真实边界框的对应行列从 `jaccard`
          矩阵中丢弃，防止重复分配。
    - 总结
        - **第一次分配**：确保大部分锚框根据 IoU 阈值进行合理分配，主要关注锚框的最大 IoU 是否满足阈值条件
        - **第二次分配**：确保所有真实边界框至少被一个锚框分配，防止有未被分配的真实边界框，从而提高检测模型的准确性

#### 标记类别和偏移量

- 现在我们可以为每个锚框标记类别和偏移量了
- 假设一个锚框 $𝐴$ 被分配了一个真实边界框 $𝐵$
    - 一方面，锚框 $𝐴$ 的类别将被标记为与 $𝐵$ 相同
    - 另一方面，锚框 $𝐴$ 的偏移量将根据 $𝐵$ 和 $𝐴$ 中心坐标的相对位置以及这两个框的相对大小进行标记
- 鉴于数据集内不同的框的位置和大小不同，我们可以对那些相对位置和大小应用变换，使其获得分布更均匀且易于拟合的偏移量
    - 这里介绍一种常见的变换。给定框 $𝐴$ 和 $𝐵$，中心坐标分别为 $(𝑥_𝑎,𝑦_𝑎)$ 和 $(𝑥_𝑏,𝑦_𝑏)$，宽度分别为 $𝑤_𝑎$ 和 $𝑤_𝑏$
      ，高度分别为 $ℎ_𝑎$ 和 $ℎ_𝑏$，可以将 $𝐴$ 的偏移量标记为$$\left( \frac{ \frac{x_b - x_a}{w_a} - \mu_x }{\sigma_x},
      \frac{ \frac{y_b - y_a}{h_a} - \mu_y }{\sigma_y},
      \frac{ \log \frac{w_b}{w_a} - \mu_w }{\sigma_w},
      \frac{ \log \frac{h_b}{h_a} - \mu_h }{\sigma_h}\right)$$
    - 其中常量的默认值为$\mu_x = \mu_y = \mu_w = \mu_h = 0, \sigma_x=\sigma_y=0.1, \sigma_w=\sigma_h=0.2$，

```python
def offset_boxes(anchors, assigned_bb, eps=1e-6):
    """  
    对锚框偏移量的转换  
    :param anchors: 锚框，形状为(锚框数, 4)  
    :param assigned_bb: 被分配的真实边界框，形状为(真实边界框数, 4)  
    :param eps: 防止log(0)的常数  
    :return: 锚框偏移量，形状为(锚框数, 4)  
    """  # 将锚框从(左上角, 右下角)的形式转换为(中心点, 宽度, 高度)的形式  
    c_anc = d2l.box_corner_to_center(anchors)
    # 将被分配的真实边界框从(左上角, 右下角)的形式转换为(中心点, 宽度, 高度)的形式  
    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)
    # 计算中心点的偏移量，并进行缩放  
    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2] / c_anc[:, 2:])
    # 计算宽度和高度的偏移量，并进行缩放
    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])  # 这里的log函数底为eps，避免出现log(0)的情况
    # 将中心点和宽高的偏移量合并在一起  
    offset = torch.cat([offset_xy, offset_wh], axis=1)
    # 返回计算得到的偏移量  
    return offset
```

- 如果一个锚框没有被分配真实边界框（IoU 没有超过阈值），我们只需将锚框的类别标记为背景（background）背景类别的锚框通常被称为负类锚框，其余的被称为正类锚框
- 我们使用真实边界框（`labels`参数）实现以下`multibox_target`函数，来标记锚框的类别和偏移量（`anchors`参数）。
  此函数将背景类别的索引设置为零，然后将新类别的整数索引递增一

```python
# 标记锚框的类和偏移量  
def multibox_target(anchors, labels):
    """使用真实边界框标记锚框"""
    # 获取批量大小和锚框  
    batch_size, anchors = labels.shape[0], anchors.squeeze(0)
    # 初始化偏移量、掩码和类别标签列表  
    batch_offset, batch_mask, batch_class_labels = [], [], []
    # 获取设备和锚框数量  
    device, num_anchors = anchors.device, anchors.shape[0]
    # 对于每个样本  
    for i in range(batch_size):
        # 获取该样本的标签  
        label = labels[i, :, :]
        # 将最接近的真实边界框分配给锚框  
        anchors_bbox_map = assign_anchor_to_bbox(label[:, 1:], anchors, device)
        # 生成锚框掩码，用于标记哪些锚框包含目标  
        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(1, 4)
        # 初始化类别标签  
        class_labels = torch.zeros(num_anchors, dtype=torch.long, device=device)
        # 初始化被分配的边界框  
        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32, device=device)
        # 获取包含目标的锚框的索引  
        indices_true = torch.nonzero(anchors_bbox_map >= 0)
        # 获取对应的真实边界框的索引  
        bb_idx = anchors_bbox_map[indices_true]
        # 设置包含目标的锚框的类别标签  
        class_labels[indices_true] = label[bb_idx, 0].long() + 1
        # 设置被分配的边界框  
        assigned_bb[indices_true] = label[bb_idx, 1:]
        # 计算锚框的偏移量，并通过掩码进行过滤  
        offset = offset_boxes(anchors, assigned_bb) * bbox_mask
        # 将偏移量添加到列表中  
        batch_offset.append(offset.reshape(-1))
        # 将掩码添加到列表中  
        batch_mask.append(bbox_mask.reshape(-1))
        # 将类别标签添加到列表中  
        batch_class_labels.append(class_labels)
        # 将所有偏移量堆叠在一起  
    bbox_offset = torch.stack(batch_offset)
    # 将所有掩码堆叠在一起  
    bbox_mask = torch.stack(batch_mask)
    # 将所有类别标签堆叠在一起  
    class_labels = torch.stack(batch_class_labels)
    # 返回每一个锚框到真实标注框的offset偏移  
    # bbox_mask为0表示背景锚框，就不用了，为1表示对应真实的物体  
    # class_labels为锚框对应类的编号  
    # 返回偏移量、掩码和类别标签  
    return (bbox_offset, bbox_mask, class_labels)
```

#### 举例

- 已经为加载图像中的狗和猫定义了真实边界框，其中第一个元素是类别（0代表狗，1代表猫），其余四个元素是左上角和右下角的(𝑥,𝑦)
  轴坐标（范围介于0和1之间）

```python
# 在图像中绘制边界框和锚框  

# 两个真实边缘框的位置信息，类别 + 左上角和右下角的坐标  
ground_truth = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],
                             [1, 0.55, 0.2, 0.9, 0.88]])  # 真实标注框的信息，包括类别标签（0代表狗，1代表猫）和位置信息（归一化的坐标）  

# 五个锚框的位置信息，左上角和右下角的坐标  
anchors = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],
                        [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],
                        [0.57, 0.3, 0.92, 0.9]])  # 锚框的位置信息（归一化的坐标）  

# 新建一个窗口  
d2l.plt.figure()
fig = d2l.plt.imshow(img)
# 在图像上画出真实的边界框，其中'k'代表黑色  
show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')
# 在图像上画出锚框，标注出锚框的索引号  
show_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4'])
```

- ![[00 Attachments/Pasted image 20240627161610.png|300]]
- 使用上面定义的`multibox_target`函数，我们可以根据狗和猫的真实边界框，标注这些锚框的分类和偏移量
- 在这个例子中，背景、狗和猫的类索引分别为0、1和2。 下面我们为锚框和真实边界框样本添加一个维度

```python
# 根据狗和猫的真实边界框，标注这些锚框的分类和偏移量  

# anchors.unsqueeze(dim=0)在0号位置加了一个批量维度，该批量维度大小为1  
labels = multibox_target(anchors.unsqueeze(dim=0), ground_truth.unsqueeze(dim=0))
# labels 对应 multibox_target 函数返回的  (bbox_offset, bbox_mask, class_labels)print(len(labels))  
# 3  
# 共有五个锚框 labels[2]中0表示背景、1表示狗、2表示猫 这里3号框被表示为背景是因为被2号框和四号框非极大值抑制了  
print(labels[2])
# tensor([[0, 1, 2, 0, 2]])  
print(labels[1])  # 锚框是不是对应是真实物体，每一个锚框有四个值，0表示不需要预测  
# tensor([[0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,  
#          1., 1.]])  
print(labels[0])  # 偏移，每个锚框有四个值  
# tensor([[-0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00, -7.90e+00, -1.00e+01,  
#           2.59e+00,  7.18e+00, -2.30e+01, -1.38e-01,  1.68e+00, -1.57e+00,  
#          -0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00, -1.40e+01, -4.60e+00,  
#           4.17e-06,  6.26e-01]])
```

- 返回的第二个元素是掩码（mask）变量，形状为（批量大小，锚框数的四倍）
- 掩码变量中的元素与每个锚框的4个偏移量一一对应。 由于我们不关心对背景的检测，负类的偏移量不应影响目标函数。通过元素乘法，掩码变量中的零将在计算目标函数之前过滤掉负类偏移量
- 返回的第一个元素包含了为每个锚框标记的四个偏移值。 请注意，负类锚框的偏移量被标记为零

### 使用非极大值预测边界框（non-maximum suppression，NMS）

- ==在预测时==，为图像生成多个锚框，再为这些锚框一一预测类别和偏移量
- 一个*预测好的边界框*则==根据其中某个带有预测偏移量（非背景）的锚框而生成==
- 下面实现了`offset_inverse`函数，该函数将锚框和偏移量预测作为输入，并应用逆偏移变换来返回预测的边界框坐标

```python
def offset_boxes(anchors, assigned_bb, eps=1e-6):
    """对锚框偏移量的转换"""
    # 将锚框从(左上角, 右下角)的形式转换为(中心点, 宽度, 高度)的形式  
    c_anc = d2l.box_corner_to_center(anchors)
    # 将被分配的真实边界框从(左上角, 右下角)的形式转换为(中心点, 宽度, 高度)的形式  
    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)
    # 计算中心点的偏移量，并进行缩放  
    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2] / c_anc[:, 2:])
    # 计算宽度和高度的偏移量，并进行缩放  
    offset_wh = 5 * torch.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])
    # 将中心点和宽高的偏移量合并在一起  
    offset = torch.cat([offset_xy, offset_wh], axis=1)
    # 返回计算得到的偏移量  
    return offset
```

- ==可能会输出许多具有明显重叠的预测边界框，且这些框都围绕着同一目标（真实边缘框）==。了简化输出，我们可以使用非极大值抑制（non-maximum
  suppression，NMS）合并属于同一目标的类似的预测边界框![[00 Attachments/Pasted image 20240627171119.png|400]]
    - 选中非背景类中具有最大预测值（概率）的预测边界框
    - 去除所有其它和它 IoU 值大于 $\epsilon$ 的预测边缘框（去掉相似度过高的）
    - 选中剩下的具有第二大的预测值（概率）的预测边界框（即剩下的未被使用的预测边界框中的最大概率的预测边界框）
    - 重复上述过程，直至 $𝐿$ 中任意一对预测边界框的 IoU 都小于阈值 $𝜖$；此时，没有一对边界框过于相似

```python
def nms(boxes, scores, iou_threshold):
    """  
    对预测边界框的置信度进行排序  
    :param boxes: 预测边界框，形状为(预测边界框数, 4)  
    :param scores: 预测边界框的置信度，形状为(预测边界框数,)  
    :param iou_threshold: float, 用于指定IoU的阈值  
    :return: 保留下来的边界框的索引，形状为(保留下来的边界框数,)  
    """
    # 按照得分降序排列预测边界框的索引  
    B = torch.argsort(scores, dim=-1, descending=True)
    # 创建一个空列表，用于存储保留下来的边界框索引  
    keep = []
    # 当B中还有元素时，进行循环  
    while B.numel() > 0:  # 直到把所有框都访问过了，再退出循环  
        # 取B中得分最高的边界框索引  
        i = B[0]  # B中的最大值，已经排好序了  
        # 将这个边界框索引添加到保留列表中  
        keep.append(i)
        # 如果B中只有一个元素，那么结束循环  
        if B.numel() == 1: break
        # 计算剩余的边界框与当前得分最高的边界框的IoU（交并比）  
        iou = box_iou(boxes[i, :].reshape(-1, 4),
                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)
        # 找到所有与当前得分最高的边界框的IoU不大于阈值的边界框的索引  
        inds = torch.nonzero(iou <= iou_threshold).reshape(-1)
        # 保留那些与当前得分最高的边界框的IoU不大于阈值的边界框  
        B = B[inds + 1]
        # 返回保留下来的边界框索引  
    return torch.tensor(keep, device=boxes.device)
```

- 定义以下`multibox_detection`函数来将非极大值抑制应用于预测边界框
    - 小批量读取，对每一个样本，将预测值取出，做一次非极大值抑制

```python
# 将非极大值抑制应用于预测边界框  
def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,
                       pos_threshold=0.009999999):
    """  
    使用非极大值抑制来预测边界框  
    :param cls_probs: 预测的类别概率，形状为(批量大小, 类别数+1, 锚框数)  
    :param offset_preds: 预测的偏移量，形状为(批量大小, 锚框数, 4)  
    :param anchors: 锚框，形状为(锚框数, 4)  
    :param nms_threshold: float, 用于指定IoU的阈值  
    :param pos_threshold: float, 用于指定置信度的阈值  
    :return: 预测的边界框信息，形状为(批量大小, 锚框数, 6)  
    """
    # 获取设备类型和批次大小  
    device, batch_size = cls_probs.device, cls_probs.shape[0]
    # 将锚框数据压缩到二维  
    anchors = anchors.squeeze(0)
    # 获取类别数量和锚框数量  
    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]
    # 创建一个空列表，用于存储每个批次的预测结果  
    out = []
    # 对每个批次进行循环  
    for i in range(batch_size):
        # 获取类别概率和预测的偏移量  
        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)
        # 获取最大类别概率和对应的类别id  
        conf, class_id = torch.max(cls_prob[1:], 0)
        # 根据预测的偏移量和锚框得到预测的边界框  
        predicted_bb = offset_inverse(anchors, offset_pred)  # 把预测框拿出来  
        # 对预测的边界框进行非极大值抑制，获取保留下来的边界框索引  
        keep = nms(predicted_bb, conf, nms_threshold)
        # 获取所有的边界框索引  
        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)
        # 将保留下来的边界框索引和所有的边界框索引拼接在一起  
        combined = torch.cat((keep, all_idx))
        # 获取唯一的索引和对应的计数  
        uniques, counts = combined.unique(return_counts=True)
        # 获取被丢弃的边界框索引  
        non_keep = uniques[counts == 1]
        # 将保留下来的边界框索引和被丢弃的边界框索引按顺序拼接在一起  
        all_id_sorted = torch.cat((keep, non_keep))
        # 将被丢弃的边界框的类别id设为-1  
        class_id[non_keep] = -1
        class_id = class_id[all_id_sorted]
        # 根据索引获取对应的类别概率和预测的边界框  
        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]
        # 找到类别概率低于阈值的边界框索引  
        below_min_idx = (conf < pos_threshold)
        # 将类别概率低于阈值的边界框的类别id设为-1  
        class_id[below_min_idx] = -1
        # 将类别概率低于阈值的边界框的类别概率设为1减去原来的值  
        conf[below_min_idx] = 1 - conf[below_min_idx]
        # 将类别id，类别概率和预测的边界框拼接在一起，作为预测信息  
        pred_info = torch.cat((class_id.unsqueeze(1), conf.unsqueeze(1), predicted_bb), dim=1)
        # 将每个批次的预测信息添加到结果列表中  
        out.append(pred_info)

        # 将结果列表转为张量返回  
    return torch.stack(out)
```

- 将上述算法应用到一个带有四个锚框的具体示例中
    - 为简单起见，我们假设预测的偏移量都是零，这意味着预测的边界框即是锚框
    - 对于背景、狗和猫其中的每个类，我们还定义了它的预测概率

```python
# 四个锚框的坐标  
anchors = torch.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],
                        [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])
# 偏移预测值，这里假设预测值全为0，即没有预测偏移  
offset_preds = torch.tensor([0] * anchors.numel())
# 类别概率，每一列对应一个锚框，每一行对应一个类别，这里有三个类别：背景、猫、狗  
cls_probs = torch.tensor([[0] * 4,  # 背景类别概率  
                          [0.9, 0.8, 0.7, 0.1],  # 猫类别概率  
                          [0.1, 0.2, 0.3, 0.9]])  # 狗类别概率
```

- 在图像上绘制这些预测边界框和置信度

```python
fig = d2l.plt.imshow(img)
# 在图像上显示锚框，其中锚框的尺度需要进行转换以适应图像的尺度  
show_bboxes(fig.axes, anchors * bbox_scale,  # 没有做NMS时，把四个锚框画出来  
            ['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])
```

- ![[00 Attachments/Pasted image 20240627193935.png|250]]
- 调用`multibox_detection`函数来执行非极大值抑制，其中阈值设置为0.5
    - 请注意，我们在示例的张量输入中添加了维度

```python
# 使用 multibox_detection 函数，输入类别预测概率、预测偏移量以及锚框，同时设置非极大值抑制的阈值为0.5  
output = multibox_detection(cls_probs.unsqueeze(dim=0),
                            offset_preds.unsqueeze(dim=0),
                            anchors.unsqueeze(dim=0), nms_threshold=0.5)
print("output：", output)
# output[0]为批量大小中的第一个图片  
# output： tensor([[[ 0.00,  0.90,  0.10,  0.08,  0.52,  0.92],
#          [ 1.00,  0.90,  0.55,  0.20,  0.90,  0.88],  
#          [-1.00,  0.80,  0.08,  0.20,  0.56,  0.95],  
#          [-1.00,  0.70,  0.15,  0.30,  0.62,  0.91]]])
```

- 我们可以看到返回结果的形状是（批量大小，锚框的数量，6）。最内层维度中的六个元素提供了同一预测边界框的输出信息。第一个元素是预测的类索引，从0开始（0代表狗，1代表猫），值
  -1 表示背景或在非极大值抑制中被移除了。 第二个元素是预测的边界框的置信度。 其余四个元素分别是预测边界框左上角和右下角的(
  𝑥,𝑦)轴坐标（范围介于0和1之间）
- 删除-1类别（背景）的预测边界框后，我们可以输出由非极大值抑制保存的最终预测边界框

```python
# 遍历预测结果  
for i in output[0].detach().numpy():
    if i[0] == -1:  # 值-1表示背景或在非极大值抑制中被移除了  
        continue
    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])  # 取('dog=', 'cat=')元组的第int(i[0]位置与str(i[1])进行拼接  
    show_bboxes(fig.axes, [torch.tensor(i[2:]) * bbox_scale], label)
```

- ![[00 Attachments/Pasted image 20240627202237.png|250]]

### 总结

- ![[00 Attachments/Pasted image 20240627173101.png|400]]

## 区域卷积神经网络 R-CNN

### R-CNN（区域卷积）

- R-CNN（Region-based Convolutional Neural Networks）是一种用于物体检测的深度学习模型，它通过将图像分割成多个候选区域（region
  proposals），然后对每个区域进行卷积特征提取和分类。R-CNN 首先使用选择性搜索算法生成候选区域，接着利用预训练的卷积神经网络（CNN）提取特征，最后通过支持向量机（SVM）进行分类。
- ![[00 Attachments/Pasted image 20240628004755.png|400]]
    - 使用启发式搜索（深度学习之前的办法）进行锚框选择
    - 将每个锚框当作一个图像，使用一个预训练好的模型来对每个锚框进行特征提取（微调？）
    - 然后使用支持向量机（SVM）进行类别分类
    - 使用线性回归模型来预测边缘框的偏移

#### 兴趣区域（RoI）池化层

- 锚框的生成大小不是统一的，因此需要进行统一为相同的形状，方便变为一个批量进行训练
- ![[00 Attachments/Pasted image 20240628011455.png|400]]
    - 使用 RoI-Pooling：给定一个锚框，将他均匀切成 $n * m$ 块，然后输出每一块中的最大值
    - 这样不管锚框多大，总是输出 nm 个值
    - 这样就可以将不同锚框进行批处理了

#### R-CNN 的缺点

- 测试速度慢
- 训练速度慢（过程复杂，cnn、svm、回归网络都要训练）
    - 一张图如果抽一千张锚框
- 训练所需空间大

### Fast RCNN

Fast R-CNN 是对 R-CNN 的一种改进，旨在提高物体检测的速度和效率。它通过在整个图像上一次性提取卷积特征，然后将这些特征应用于所有候选区域，避免了
R-CNN 中对每个区域重复提取特征的高计算开销。Fast R-CNN 使用 ROI（Region of
Interest）池化层来从卷积特征图中提取每个候选区域的特征，并通过一个单一的网络实现分类和边界框回归，从而显著加快了检测速度，同时保持了较高的检测精度。

- ![[00 Attachments/Pasted image 20240628012228.png|400]]
    - 在 R-CNN 上做块，首先对整张图片用 CNN 抽取特征，抽完之后，再用 Selective serch 搜索锚框
    - 搜到锚框之后，将其映射到 CNN 的输出上
        - 将该锚框在 CNN 的输出上按照比例找出来
    - 然后使用 RoI 池化层对锚框进行特征抽取
    - 之后在通过全连接层进行类别和边缘框预测
- 这个之所以比 R-CNN 要快是因为 CNN 不是对每个锚框进行特征抽取，而是对整张图片进行特征抽取

### Faster RCNN

- Faster R-CNN
  是一种基于区域卷积神经网络（R-CNN）的物体检测算法，旨在提高检测速度和准确性。它通过引入区域建议网络（RPN），实现了从输入图像中快速生成候选区域，而不是依赖于外部选择的候选区域生成算法。这一过程与特征提取模块共享卷积特征，从而显著提高了效率。Faster
  R-CNN 结合了精确的目标检测和快速的处理速度，广泛应用于各种计算机视觉任务中。
- 使用一个神经网络来替代启发式搜索算法（Selective
  serch）来获得更好的锚框![[00 Attachments/Pasted image 20240628013726.png|400]]
    - 首先将图片通过一个 CNN 中，然后将结果给 RPN，RPN 的输出就是一堆高质量的锚框
    - RPN 其实是在做一个模糊的目标检测
        - RPN 拿到了 CNN 输出后再做一次卷积
        - 然后提取一系列锚框并作二分类问题
            - 预测分类，锚框是否圈中了感兴趣的物体
            - 预测偏移，锚框能否更加接近实际边界框
        - 将锚框进行非极大值抑制（NMS）（将类似的锚框消去）
    - 将预测出来的好的锚框进行 RoI Pooling，再通过全连接层预测

### Mask R-CNN

- ![[00 Attachments/Pasted image 20240628015708.png|400]]
    - 如果拥有像素级别的标号，那么可以对每个像素做预测
    - RoI 输出后通过 FCN （进行像素预测标号）
    - RoI pooling 变为 RoI align
        - 在 RoI pooling 中，如果锚框的高宽不能被整除 （3\*3），分割时边框会发生偏移
            - 对于目标检测来说，这些偏移无伤大雅
            - 但是对于像素级别的标号，这些偏移就不能忽视
        - RoI align 对像素进行切割（赋权重）（3\*3 切割时，直接对中间的像素点进行划分（切割））

## 单发多框检测 SSD

- 单发多框检测（single shot multibox detector，SSD）
- 锚框抽取![[00 Attachments/Pasted image 20240628021331.png|400]]

## 物体检测算法 YOLO

## 语义分割和数据集

### 语义分割

- ![[00 Attachments/Pasted image 20240710165306.png|400]]
    - 图片分类：是给一张图片，将这张图片的主体的物体识别出来
    - 目标分类：在一张图中有多个感兴趣的物体，将这些物体识别出来并告诉位置（通过锚框实现）
    - 语义分割：在目标分类的基础上精细化，告诉每个像素属于哪一个类别（对每个像素生成一个标号）
    - 实例分割：更精细的分类，将像素精确到每个类的实例（像素属于哪只狗？）
        - ![[00 Attachments/Pasted image 20240710170448.png|400]]
- 应用：
    - 背景虚化
    - 路面分割

### 数据集

#### 下载并读取 Pascal VOC2012 语义分割数据集

```python
import os
import torch
import torchvision
from d2l import torch as d2l

# @save  
d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',
                           '4e443f8a2eca6b1dac8a6c57641b67dd40621a49')

voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')  # voc_dir是数据集的路径
```

- 进入路径`../data/VOCdevkit/VOC2012`之后，我们可以看到数据集的不同组件
    - `ImageSets/Segmentation`路径包含用于训练和测试样本的文本文件
    - `JPEGImages`和`SegmentationClass`路径分别存储着每个示例的输入图像和标签
        - 此处的标签也采用图像格式，其尺寸和它所标注的输入图像的尺寸相同
        - 此外，标签中颜色相同的像素属于同一个语义类别
- 下面将`read_voc_images`函数定义为将所有输入的图像和标签读入内存

```python
def read_voc_images(voc_dir, is_train=True):
    """读取所有VOC图像并标注"""
    # 定义txt_fname，这是我们将读取的文件的路径  
    # 这些文件中包含了我们需要读取的图像的文件名  
    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',
                             'train.txt' if is_train else 'val.txt')
    # 设定图像的读取模式为RGB  
    mode = torchvision.io.image.ImageReadMode.RGB
    # 读取txt_fname文件，并将文件中的内容分割成一个个的文件名，然后存储在images列表中  
    with open(txt_fname, 'r') as f:
        images = f.read().split()
        # 创建两个空列表，分别用于存储特征和标签  
    features, labels = [], []
    # 对于images中的每个文件名  
    for i, fname in enumerate(images):
        # 使用torchvision.io.read_image读取对应的图像文件，然后添加到features列表中  
        features.append(torchvision.io.read_image(os.path.join(voc_dir, 'JPEGImages', f'{fname}.jpg')))
        # 使用torchvision.io.read_image读取对应的标签图像文件，然后添加到labels列表中  
        labels.append(torchvision.io.read_image(os.path.join(voc_dir, 'SegmentationClass', f'{fname}.png'), mode))
        # 返回特征和标签列表  
    return features, labels


# 调用read_voc_images函数，读取训练数据集的图像和标签，然后存储在train_features和train_labels中  
train_features, train_labels = read_voc_images(voc_dir, True)
```

- 下面我们绘制前 5 个输入图像及其标签
    - 在标签图像中，白色和黑色分别表示边框和背景，而其他颜色则对应不同的类别

```python
n = 5
imgs = train_features[0:n] + train_labels[0:n]
# 将列表中的每个图像的通道维度从第一维（0开始计数）移到最后一维  
# 因为matplotlib等绘图库需要将通道维度放在最后才能正常显示图像  
imgs = [img.permute(1, 2, 0) for img in imgs]  # permute函数交换维度 (1,2,0) -> (2,0,1)
d2l.show_images(imgs, 2, n)
# 第一行显示的是图片，第二行显示的是标号
```

- ![[00 Attachments/Pasted image 20240711111520.png|500]]

#### 将 labels 中的颜色映射到类别

- 列举RGB颜色值和类名

```python
# 每种类别在标签图像中由一种特定的颜色表示，颜色的RGB值就是这里定义的  
VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],
                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],
                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],
                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],
                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],
                [0, 64, 128]]

# 列表中的类别名顺序与VOC_COLORMAP中的颜色值顺序相对应。  
VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',
               'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',
               'person', 'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']
```

- 通过上面定义的两个常量，我们可以方便地查找标签中每个像素的类索引
- `voc_colormap2label`函数来构建从上述RGB颜色值到类别索引的映射
- `voc_label_indices`函数将RGB值映射到在 Pascal VOC2012 数据集中的类别索引

```python
# 定义函数voc_colormap2label，这个函数用于建立一个映射，将RGB颜色值映射为对应的类别索引  
def voc_colormap2label():
    """构建从RGB到VOC类别索引的映射"""
    # 创建一个全零的张量，大小为256的三次方，因为RGB颜色的每个通道有256种可能的值，所以总共有256^3种可能的颜色组合。数据类型设为long  
    colormap2label = torch.zeros(256 ** 3, dtype=torch.long)
    # 对于VOC_COLORMAP中的每个颜色值（colormap）  
    for i, colormap in enumerate(VOC_COLORMAP):
        # 计算颜色值的一维索引，并将这个索引对应的位置设为i。这样，给定一个颜色值，我们就可以通过这个映射找到对应的类别索引  
        colormap2label[(colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i
        # 返回映射  
    return colormap2label


# 定义函数voc_label_indices，这个函数用于将一张标签图像中的每个像素的颜色值映射为对应的类别索引  
def voc_label_indices(colormap, colormap2label):
    """将VOC标签中的RGB值映射到它们的类别索引"""
    # 将输入的colormap的通道维度移到最后一维，并将其转换为numpy数组，然后转换为int32类型。这是因为我们需要使用numpy的高级索引功能  
    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')
    # 计算colormap中每个像素的颜色值对应的一维索引。这里的索引计算方式和上一个函数中的是一致的  
    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256 + colormap[:, :, 2])
    # 使用colormap2label这个映射将索引映射为对应的类别索引，并返回  
    return colormap2label[idx]
```

- 例如，在第一张样本图像中，飞机头部区域的类别索引为1，而背景索引为0

```python
# 调用上面定义的两个函数，将训练数据集中的第一个标签图像的RGB颜色值转换为对应的类别索引，并将结果保存在变量y中  
y = voc_label_indices(train_labels[0], voc_colormap2label())
# 打印变量y中的一小部分，即第105行到115行，第130列到140列的部分。这里是为了查看转换后的类别索引是否正确  
print(y[105:115, 130:140])
# 打印VOC_CLASSES列表中的第二个类别名（索引为1）。这里是为了查看第二个类别名是什么  
print(VOC_CLASSES[1])  # 标签类别  
# tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],  
#         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],  
#         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],  
#         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],  
#         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],  
#         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],  
#         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],  
#         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],  
#         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],  
#         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]])  
# aeroplane
```

#### 预处理数据

- 在之前的实验，通过再缩放图像使其符合模型的输入形状
- 然而在语义分割中，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像
    - ==这样的映射可能不够精确==，尤其在不同语义的分割区域
    - 为了避免这个问题，将图像裁剪为固定尺寸，而不是再缩放
        - 具体来说，我们使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域。

```python
# 使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域  
# 定义函数voc_rand_crop，用于对输入的特征图像和标签图像进行随机裁剪。height和width是裁剪的高度和宽度  
def voc_rand_crop(feature, label, height, width):
    """随即裁剪特征和标签图像"""
    # 调用RandomCrop的get_params方法，随机生成一个裁剪框。裁剪框的大小是(height, width)  
    # rect拿到特征的框  
    rect = torchvision.transforms.RandomCrop.get_params(feature, (height, width))
    # 根据生成的裁剪框，对特征图像进行裁剪  
    # 拿到框中的特征和标号  
    feature = torchvision.transforms.functional.crop(feature, *rect)
    # 根据生成的裁剪框，对标签图像进行裁剪。注意，我们是在同一个裁剪框下裁剪特征图像和标签图像，以保证它们对应的位置仍然是对齐的  
    label = torchvision.transforms.functional.crop(label, *rect)
    # 返回裁剪后的特征图像和标签图像  
    return feature, label


imgs = []
# 对于每个数字i，在0到n-1的范围内  
for _ in range(n):
    # 调用上面定义的voc_rand_crop函数，对训练集中的第一个特征图像和标签图像进行随机裁剪，然后将裁剪后的图像添加到imgs列表中  
    imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)
# 将imgs中的每个图像的通道维度移到最后一维，以方便绘图  
imgs = [img.permute(1, 2, 0) for img in imgs]
# 使用d2l库的show_images函数展示图像。这里首先将imgs列表中的图像分为两部分，一部分是特征图像，一部分是标签图像。然后将这两部分拼接在一起，使得特征图像和标签图像能够按对应的顺序展示  
d2l.show_images(imgs[::2] + imgs[1::2], 2, n)
```

- ![[00 Attachments/Pasted image 20240711112711.png|500]]

#### 自定义语义分割数据集类

- 通过继承高级API提供的`Dataset`类，自定义了一个语义分割数据集类`VOCSegDataset`
    - `__getitem__`函数，可以任意访问数据集中索引为`idx`的输入图像及其每个像素的类别索引
    - `filter`函数，可以移除尺寸小于随机裁剪所指定的输出尺寸的图像
    - `normalize_image`函数，对输入图像的RGB三个通道的值分别做标准化

```python
# 自定义语义分割数据集类  
# 定义一个自定义的数据集类，用于加载VOC数据集。这个类继承了torch.utils.data.Dataset  
class VOCSegDataset(torch.utils.data.Dataset):
    """一个用于加载VOC数据集的自定义数据集"""

    def __init__(self, is_train, crop_size, voc_dir):
        """  
        初始化  
        :param is_train: 是否是训练集  
        :param crop_size: 裁剪后的图像尺寸  
        :param voc_dir: VOC数据集的路径  
        """
        # 定义一个归一化变换，用于对输入图像进行归一化。这里使用了ImageNet数据集的均值和标准差  
        self.transform = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        self.crop_size = crop_size
        features, labels = read_voc_images(voc_dir, is_train=is_train)
        # 对读取到的特征图像进行筛选和归一化处理  
        self.features = [self.normalize_image(feature) for feature in self.filter(features)]
        # 对读取到的标签图像进行筛选处理  
        self.labels = self.filter(labels)
        # 创建一个从颜色映射到类别索引的映射表  
        self.colormap2label = voc_colormap2label()
        # 打印出读取到的图像数量  
        print('read ' + str(len(self.features)) + ' examples')

        # 定义一个方法，用于对输入图像进行归一化处理  

    def normalize_image(self, img):
        """  
        将输入图像转换为浮点数类型，并调用归一化变换对其进行处理，最后返回处理后的图像  
        """
        return self.transform(img.float())

        # 定义一个方法，  

    def filter(self, imgs):
        """  
        用于筛选符合要求的图像。筛选条件是图像的高度和宽度都大于等于裁剪尺寸  
        """
        return [img
                for img in imgs
                if (img.shape[1] >= self.crop_size[0] and img.shape[2] >= self.crop_size[1])]

    def __getitem__(self, idx):
        """  
        定义一个方法，用于根据索引获取一个样本。这是一个实现Dataset接口所必须的方法  
        每一次返回的样本做一次rand_crop  
        """  # 调用之前定义的voc_rand_crop函数，对指定索引的特征图像和标签图像进行随机裁剪  
        feature, label = voc_rand_crop(self.features[idx], self.labels[idx], *self.crop_size)
        # 调用voc_label_indices函数，将裁剪后的标签图像转换为类别索引，并返回裁剪和转换后的特征图像和标签图像  
        return (feature, voc_label_indices(label, self.colormap2label))

    def __len__(self):
        """  
        定义一个方法，用于获取数据集的长度。这是一个实现Dataset接口所必须的方法  
        """
        return len(self.features)
```

#### 读取数据集

```python
# 读取数据集  
crop_size = (320, 480)
voc_train = VOCSegDataset(True, crop_size, '../data/VOCdevkit/VOC2012')
voc_test = VOCSegDataset(False, crop_size, '../data/VOCdevkit/VOC2012')

batch_size = 64
# 创建一个数据加载器实例  
# shuffle=True表示在每个迭代周期中随机打乱数据，drop_last=True表示如果最后一个批次的样本数量小于batch_size，则丢弃该批次，num_workers=0表示数据加载不使用额外的进程或线程  
train_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=True, drop_last=True,
                                         num_workers=0)
# 对数据加载器进行迭代，每次迭代都返回一个批次的数据，其中X是特征，Y是标签  
for X, Y in train_iter:
    # 打印特征的形状，它应该是一个四维张量，其形状为(batch_size, channels, height, width)  
    print(X.shape)
    # 打印标签的形状，它应该是一个三维张量，其形状为(batch_size, height, width)，其中每个元素是一个像素的类别索引  
    print(Y.shape)  # Y也为每一个像素对应的值  
    # 只迭代一次，所以立即跳出循环  
    break
# torch.Size([64, 3, 320, 480])  
# torch.Size([64, 320, 480])
```

#### 整合所有组件

```python
def load_data_voc(batch_size, crop_size):
    """加载VOC语义分割数据集"""
    # 下载并提取VOC2012数据集，获取数据集的路径  
    voc_dir = d2l.download_extract('voc2012', os.path.join('VOCdevkit', 'VOC2012'))
    # 获取数据加载器的工作进程数量  
    num_workers = d2l.get_dataloader_workers()
    train_iter = torch.utils.data.DataLoader(VOCSegDataset(True, crop_size, voc_dir), batch_size, shuffle=True,
                                             drop_last=True, num_workers=num_workers)
    test_iter = torch.utils.data.DataLoader(VOCSegDataset(False, crop_size, voc_dir), batch_size, drop_last=True,
                                            num_workers=num_workers)
    # 返回训练数据加载器和测试数据加载器  
    return train_iter, test_iter
```

## 转置卷积（Transpose Convolution）

- 卷积会将输入的图像变小
- 如果输入和输出图像和空间维度相同，在以像素级语义分割终将会很方便
    - 例如，输出像素所处的通道维可以保有输入像素在同一位置上的分类结果
- 在图象被卷积层缩小后，可以通过转置卷积将图像的形状还原（增大输入高宽）

### 基本操作

- ![[00 Attachments/Pasted image 20240711165817.png]]

```python
# 定义转置卷积运算  
def trans_conv(X, K):
    h, w = K.shape  # 卷积核的宽、高  
    # 创建一个新的张量Y，其尺寸为输入X的尺寸加上卷积核K的尺寸减去1  
    # 在常规卷积中，输出尺寸通常是输入尺寸减去卷积核尺寸加1  
    Y = torch.zeros(
        (X.shape[0] + h - 1, X.shape[1] + w - 1))  # 正常的卷积后尺寸为(X.shape[0] - h + 1, X.shape[1] - w + 1)  
    for i in range(X.shape[0]):
        for j in range(X.shape[1]):
            # 对于输入X的每一个元素，我们将其与卷积核K进行元素级别的乘法，然后将结果加到输出张量Y的相应位置上  
            Y[i:i + h, j:j + w] += X[i, j] * K  # 按元素乘法，加回到自己矩阵  
    # 返回转置卷积的结果  
    return Y


X = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
print(trans_conv(X, K))
# tensor([[ 0.,  0.,  1.],  
#         [ 0.,  4.,  6.],  
#         [ 4., 12.,  9.]])
```

- 调用 API 实现转置卷积层的运算

```python
# 使用高级API获得相同的结果  
# 将输入张量X和卷积核K进行形状变换，原来是2x2的二维张量，现在变成了1x1x2x2的四维张量  
# 第一个1表示批量大小，第二个1表示通道数，2x2是卷积核的高和宽  
X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)
# 创建一个转置卷积层对象tconv，其中输入通道数为1，输出通道数为1，卷积核的大小为2，没有偏置项  
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)
# 将创建的转置卷积层对象tconv的权重设置为我们的卷积核K  
tconv.weight.data = K
print(tconv(X))
# tensor([[[[ 0.,  0.,  1.],  
#           [ 0.,  4.,  6.],  
#           [ 4., 12.,  9.]]]], grad_fn=<ConvolutionBackward0>)
```

### 填充、步幅和多通道

- 回顾一下常规卷积![[00 Attachments/Pasted image 20240530232346.png|400]]
    - n 为输入，k 为卷积核、p 为上下填充之和（代码里不是）
- 转置卷积的输出： $$(n_hs_h + k_h - p_h - s_h) * (n_ws_w + k_w - p_h - s_h)$$
    - 如果想让高宽成倍增加，那么 $k = 2p + s$

#### 填充

- 与常规卷积不同，在转置卷积中，==填充==被应用于的输出（常规卷积将填充应用于输入）
    - 例如，当将高和宽两侧的填充数指定为1时，转置卷积的输出中将删除第一和最后的行与列

```python
# 填充大小为1就相当于将输出矩阵最外面的一圈当作填充并剔除  
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)
tconv.weight.data = K
print(tconv(X))
# tensor([[[[4.]]]], grad_fn=<ConvolutionBackward0>)
```

#### 步幅

- 在转置卷积中，==步幅==被指定为中间结果（输出），而不是输入
    - 使用相同输入和卷积核张量，将步幅从1更改为2会增加中间张量的高和权重
- ![[00 Attachments/Pasted image 20240711172333.png|400]]

```python
# 步幅为2表示在进行卷积时，每次移动2个单位，相较于步幅为1，这样会使得输出尺寸增大  
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)
tconv.weight.data = K
print(tconv(X))
# tensor([[[[0., 0., 0., 1.],  
#           [0., 0., 2., 3.],  
#           [0., 2., 0., 3.],  
#           [4., 6., 6., 9.]]]], grad_fn=<ConvolutionBackward0>)
```

#### 多通道

- ==多通道==的情况与卷积的相同
- 同样，如果我们将 $𝑋$ 代入卷积层 $𝑓$ 来输出 $𝑌=𝑓(𝑋)$，并创建一个与 $𝑓$ 具有相同的超参数（填充、步长）、但输出通道数量是 $𝑋$
  中通道数的转置卷积层 $𝑔$，那么 $𝑔(𝑌)$ 的形状将与 $𝑋$ 相同
- 首先对输入张量X进行卷积操作，然后再对卷积的结果进行转置卷积操作，然后检查这个结果的形状是否和原始输入张量X的形状相同

```python
# 创建一个四维张量X，批量大小为1，通道数为10，高和宽都为16  
X = torch.rand(size=(1, 10, 16, 16))
# 卷积层 将 10 个输入通道转换为 20 个输出通道  
conv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)
# 转置卷积层 将 20 个输入通道转换为 10 个输出通道  
tconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)
# 检查转置卷积后的输出尺寸是否与输入尺寸相同  
print(tconv(conv(X)).shape == X.shape)
# True
```

### 与矩阵变换的联系

- ![[00 Attachments/Pasted image 20240711190732.png|400]]
- 卷积的操作可以用矩阵相乘的方式表示![[00 Attachments/Pasted image 20240711190741.png|400]]
- 卷积

```python
X = torch.arange(9.0).reshape(3, 3)
K = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
Y = d2l.corr2d(X, K)  # 卷积  
print(Y)
# tensor([[27., 37.],  
#         [57., 67.]])
```

- 生成稀疏权重矩阵

```python
def kernel2matrix(K):
    """  
    用于将给定的卷积核K转换为一个稀疏矩阵W  
    """
    k, W = torch.zeros(5), torch.zeros((4, 9))
    print(k)
    print(W)
    print(K)
    # 将卷积核K的元素填充到向量k中的适当位置，形成一个稀疏向量  
    k[:2], k[3:5] = K[0, :], K[1, :]
    # 打印填充后的向量k  
    print(k)
    # 将稀疏向量k填充到矩阵W中的适当位置，形成一个稀疏矩阵  
    W[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k
    return W


# 使用kernel2matrix函数将卷积核K转换为一个稀疏矩阵W  
# 这个矩阵的每一行表示在一个特定位置进行的卷积操作，其中的0表示卷积核没有覆盖的区域  
# 如果输入是一个3x3的图像，并被拉平为一个1x9的向量  
# 而卷积核是2x2的，那么输出图像的大小为2x2，拉平后变为一个4x1的向量  
# kernel2matrix函数实际上就是在构建这种转换关系  
W = kernel2matrix(K)
print(W)
# tensor([0., 0., 0., 0., 0.])  
# tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],  
#         [0., 0., 0., 0., 0., 0., 0., 0., 0.],  
#         [0., 0., 0., 0., 0., 0., 0., 0., 0.],  
#         [0., 0., 0., 0., 0., 0., 0., 0., 0.]])  
# tensor([[1., 2.],  
#         [3., 4.]])  
# tensor([1., 2., 0., 3., 4.])  
# tensor([[1., 2., 0., 3., 4., 0., 0., 0., 0.],  
#         [0., 1., 2., 0., 3., 4., 0., 0., 0.],  
#         [0., 0., 0., 1., 2., 0., 3., 4., 0.],  
#         [0., 0., 0., 0., 1., 2., 0., 3., 4.]])
```

- 将 $X$  化为一维向量，然后，`W`的矩阵乘法和向量化的`X`给出了一个长度为4的向量
    - 重塑它之后（化为 $2 * 2$），可以获得与上面的原始卷积操作所得相同的结果`Y`
        - 使用矩阵乘法实现了卷积

```python
print(X)
# tensor([[0., 1., 2.],  
#         [3., 4., 5.],  
#         [6., 7., 8.]])  
print(X.reshape(-1))  # 拉平输入张量X  
# tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])  

# 判断卷积操作的结果Y是否等于稀疏矩阵W与拉平的输入张量X的矩阵乘法的结果，并将结果重塑为2x2的形状  
print(Y == torch.matmul(W, X.reshape(-1)).reshape(2, 2))
# tensor([[True, True],  
#         [True, True]])
```

- 同样，可以使用矩阵乘法来实现转置卷积
- 在下面的示例中，我们将上面的常规卷积2×2的输出`Y`作为转置卷积的输入
    - 想要通过矩阵相乘来实现它，我们只需要将权重矩阵`W`的形状转置为(9,4)
- ==转置矩阵不能视作矩阵的逆运算==
    - W 的转置不一定等于 W 的逆

```python
Z = trans_conv(Y, K)
# 判断转置卷积操作的结果Z是否等于稀疏矩阵W的转置与拉平的卷积结果Y的矩阵乘法的结果，并将结果重塑为3x3的形状  
# 注意这里得到的结果并不是原图像 X ，尽管它们的尺寸是一样的  
print(Z == torch.matmul(W.T, Y.reshape(-1)).reshape(3, 3))  # 由卷积后的图像乘以转置卷积后，得到的并不是原图像，而是尺寸一样  
# tensor([[True, True, True],  
#         [True, True, True],  
#         [True, True, True]])
```

### 结论

- 与通过卷积核减少输入元素的常规卷积相反，转置卷积通过卷积核广播输入元素，从而==产生形状大于输入的输出==
- 可以使用矩阵乘法来实现卷积。==转置卷积层能够交换卷积层的正向传播函数和反向传播函数==
- ？？？？？？？？？？？？？？？？？？
- 抽象来看，给定输入向量 $\mathbf{x}$ 和权重矩阵 $\mathbf{W}$
  ，卷积的前向传播函数可以通过将其输入与权重矩阵相乘并输出向量 $\mathbf{y}=\mathbf{W}\mathbf{x}$ 来实现
- 由于反向传播遵循链式法则和 $\nabla_{\mathbf{x}}\mathbf{y}=\mathbf{W}^\top$（关于 x
  求导），==卷积的反向传播函数（损失函数对输入的梯度）可以通过将其输入（Y）与转置的权重矩阵 $\mathbf{W}^\top$
  相乘来实现（？？？？？？？？）（$\frac{\partial L}{\partial \mathbf{X}} = \frac{\partial L}{\partial \mathbf{Y}} * \mathbf{W}^\top$）==。
  因此，转置卷积层能够交换卷积层的正向传播函数和反向传播函数：它的正向传播和反向传播函数将输入向量分别与 $\mathbf{W}^\top$
  和  $\mathbf{W}$ 相乘

### 补充转置卷积也是一种卷积

- ![[00 Attachments/Pasted image 20240712010406.png|400]]
- ![[00 Attachments/Pasted image 20240712010416.png|400]]
- ![[00 Attachments/Pasted image 20240712010515.png|400]]
- ![[00 Attachments/Pasted image 20240712010607.png|400]]

## 全连接卷积神经网络（FCN）

### FCN

- 在图像分类任务中 CNN 最后==采用全连接层或者全局平均池化层==
- 但对语义分割来说，并不适用
- 实现了从图像像素到像素类别的变换![[00 Attachments/Pasted image 20240712010737.png|400]]
    - 1 * 1 卷积层用来降低通道维度
    - 转置卷积层将图片变为原来的尺寸（输出 k * 224 * 224）
        - CNN 将图片缩小为原来的 1/32，转置卷积层则将图片放大 32倍
        - k 为类别数，将每个类别的信息存于各个通道中
- 因此，输出的类别预测与输入图像在像素级别上具有一一对应关系
    - 通道维的输出即该位置对应像素的类别预测

### 代码实现

#### 构造模型

- 使用在 ImageNet 数据集上预训练的 ResNet-18 模型来提取图像特征，并将该网络记为`pretrained_net`
- ResNet-18模型的最后几层包括全局平均汇聚层和全连接层，然而全卷积网络中不需要它们

```python
import torch
import torchvision
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

pretrained_net = torchvision.models.resnet18(pretrained=True)
print(list(pretrained_net.children())[-3:])  # 展示预训练模型的最后三层  
# [Sequential(  
#   (0): BasicBlock(  
#     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)  
#     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#     (relu): ReLU(inplace=True)  
#     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#     (downsample): Sequential(  
#       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)  
#       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#     )  
#   )  
#   (1): BasicBlock(  
#     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#     (relu): ReLU(inplace=True)  
#     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)  
#     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  
#   )  
# ), AdaptiveAvgPool2d(output_size=(1, 1)), Linear(in_features=512, out_features=1000, bias=True)]
```

- 在 ResNet 的基础上构造全卷积网络，去掉ResNet18最后两层

```python
# 创建一个全卷积网络实例net  
net = nn.Sequential(*list(pretrained_net.children())[:-2])  # 去掉ResNet18最后两层  
X = torch.rand(size=(1, 3, 320, 480))
# 给定高度为320和宽度为480的输入，net的前向传播将输入的高和宽减小至原来的，即10和15  
print(net(X).shape)
# torch.Size([1, 512, 10, 15])
```

- 由于 $(320−64+16×2+32)/32=10$ 且 $(480−64+16×2+32)/32=15$，我们构造一个步幅为 32 的转置卷积层，并将卷积核的高和宽设为
  64，填充为 16
    - 我们可以看到如果步幅为 $𝑠$，填充为 $𝑠/2$（假设 $𝑠/2$ 是整数）且卷积核的高和宽为 $2𝑠$
      ，转置卷积核会将输入的高和宽分别放大 $𝑠$ 倍
    - 要想使转置卷积层能还原卷积层的输入形状，参数与卷积层的一样即可

```python
num_classes = 21
# 使用 1*1 卷积层将输出通道数转换为Pascal VOC2012数据集的类数（21类）  
net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))  # 减少计算量
# 使用转置卷积层将输出的高和宽恢复至输入的高和宽，并得到类别预测结果  
net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,
                                                    kernel_size=64, padding=16, stride=32))
```

#### 初始化转置卷积层

- 我们常使用==双线性插值法==（bilinear interpolation）将图像进行放大，也常用于初始化转置卷积层
    - 该初始化可以得到较好的初始参数，加速训练

    1. 将输出图像的坐标 $(𝑥,𝑦)$ 映射到输入图像的坐标 $(𝑥′,𝑦′)$ 上。 例如，根据输入与输出的尺寸之比来映射。
       请注意，映射后的 $𝑥′$ 和 $𝑦′$ 是实数
    2. 在输入图像上找到离坐标 $(𝑥′,𝑦′)$ 最近的4个像素
    3. 输出图像在坐标 $(𝑥,𝑦)$ 上的像素依据输入图像上这4个像素及其与 $(𝑥′,𝑦′)$ 的相对距离来计算

```python
# 初始化转置卷积层 双线性插值核的实现  
def bilinear_kernel(in_channels, out_channels, kernel_size):
    """  
    返回一个初始化的双线性插值核权重矩阵，可用于初始化转置卷积层  
    """  # 计算双线性插值核中心点位置  
    factor = (kernel_size + 1) // 2
    # 根据核的大小是奇数还是偶数，确定中心点的位置  
    if kernel_size % 2 == 1:
        center = factor - 1
    else:
        center = factor - 0.5
        # 创建一个矩阵，其元素的值等于其与中心点的距离  
    og = (torch.arange(kernel_size).reshape(-1, 1),
          torch.arange(kernel_size).reshape(1, -1))
    # 计算双线性插值核，其值由中心点出发，向外线性衰减  
    filt = (1 - torch.abs(og[0] - center) / factor) * (1 - torch.abs(og[1] - center) / factor)
    # 初始化一个权重矩阵，大小为 (输入通道数, 输出通道数, 核大小, 核大小)  
    weight = torch.zeros((in_channels, out_channels, kernel_size, kernel_size))
    # 将双线性插值核的值赋给对应位置的权重  
    weight[range(in_channels), range(out_channels), :, :] = filt
    return weight
```

- 使用 `bilinear_kernel` 初始化转置卷积层权重

```python
conv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,
                                bias=False)
conv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4))
```

- 举例

```python
img = torchvision.transforms.ToTensor()(d2l.Image.open('catdog.jpg'))
X = img.unsqueeze(0)  # 增加批量维度  
Y = conv_trans(X)
out_img = Y[0].permute(1, 2, 0).detach()  # 输出图像的维度是(H, W, C) Y[0] 是第一个样本

d2l.set_figsize()
print('input image shape:', img.permute(1, 2, 0).shape)
# input image shape: torch.Size([561, 728, 3])  
d2l.plt.imshow(img.permute(1, 2, 0))
print('output image shape:', out_img.shape)
# output image shape: torch.Size([1122, 1456, 3])  
d2l.plt.imshow(out_img)
```

- 全卷积网络用双线性插值的上采样初始化转置卷积层。对于1×1卷积层，使用Xavier初始化参数

```python
W = bilinear_kernel(num_classes, num_classes, 64)
net.transpose_conv.weight.data.copy_(W)
```

#### 读取数据集

- 指定随即裁剪的形状

```python
batch_size, crop_size = 32, (320, 480)
train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)
# read 1114 examples  
# read 1078 examples
```

#### 训练

```python
def loss(inputs, targets):
    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)


num_epochs, lr, wd, devices = 5, 0.001, 1e-3, d2l.try_all_gpus()
trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
# loss 0.446, train acc 0.861, test acc 0.854  
# 107.1 examples/sec on [device(type='cuda', index=0), device(type='cuda', index=1)]
```

- ![[00 Attachments/Figure_2 1.png|400]]

#### 预测

- 将输入图像在各个通道做标准化，并转成卷积神经网络所需要的四维输入格式

```python
def predict(img):
    # 首先，对图像进行归一化处理，并添加一个批量维度，以匹配模型的输入需求  
    # normalize_image函数会对图像的每个像素进行归一化处理，使其值在0到1之间  
    X = test_iter.dataset.normalize_image(img).unsqueeze(0)
    # 使用argmax(dim=1)找出预测结果中概率最大的类别，返回这个类别的索引  
    pred = net(X.to(devices[0])).argmax(dim=1)  # pred 是个标量，不是向量  
    # 最后，将预测结果的形状改变成与原始图像相同的形状  
    return pred.reshape(pred.shape[1], pred.shape[2])
```

- 可视化预测的类别给每个像素，我们将预测类别映射回它们在数据集中的标注颜色

```python
def label2image(pred):
    # 使用VOC_COLORMAP将类别转换为RGB颜色  
    colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[0])  # 把类别的RGB值做成一个tensor  
    # 将预测的结果转换为long型以对应颜色映射的索引  
    X = pred.long()  # 转换成long型，以对应colormap的索引  
    return colormap[X, :]  # 根据索引取出RGB值，并拼接成图像
```

- 裁剪图像用于预测

```python
voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')
test_images, test_labels = d2l.read_voc_images(voc_dir, False)
n, imgs = 4, []
for i in range(n):
    # 设置裁剪的区域  
    crop_rect = (0, 0, 320, 480)
    # 对图像进行裁剪  
    X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)
    # 预测裁剪后的图像，并将预测结果转换为图像  
    pred = label2image(predict(X))  # 预测转成图片  
    # 将原图，预测的图像和标签图像加入到imgs列表中  
    imgs += [X.permute(1, 2, 0), pred.cpu(),
             torchvision.transforms.functional.crop(test_labels[i], *crop_rect).permute(1, 2, 0)]
# 显示原图、预测的图像和标签图像  
d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2)  # 第二行为预测，第三行为真实标号
```

- ![[00 Attachments/Figure_3 1.png|400]]

## 样式迁移（style transfer）

- 自动将一个图像中的样式应用在另一图像上，即样式迁移
    - 需要两张输入图像：内容图像、样式图像
- ![[00 Attachments/Pasted image 20240715145610.png|400]]
- 让输入图片在某些 CNN 层与内容图片进行匹配，在另外的一些 CNN 层与样式图片进行匹配。从而使得最终输出的图片既有内容图片的内容，也具备样式图片的特征

### 方法

- 基于卷积神经网络的风格迁移方法
    - 初始化合成图像为内容图像
        - 该合成图像是风格迁移过程中==唯一需要更新的变量==（即训练的是合成图像）
    - 选择一个预训练的卷积神经网络来抽取图像特征
        - 该 ==CNN 参数无需更新==
        - 选择 CNN 的某些层的输出作为内容特征或风格特征
- 图示 CNN 为同一个 CNN，第二层抽取内容特征，第一、三层抽取样式特征![[00 Attachments/Pasted image 20240715152825.png|400]]
- 通过前向传播（实线）计算风格迁移的损失函数，并通过反向传播（虚线）迭代模型参数（更新合成图像）
- 损失函数的构成
    - 内容损失使合成图像与内容图像在内容特征上接近
    - 风格损失使合成图像与风格图像在风格特征上接近
    - 全变分损失则有助于减少合成图像中的噪点

### 内容和风格图像

- 内容图像和风格图像的尺寸不一样

```python
import torch
import torchvision
from torch import nn
from d2l import torch as d2l

# 内容和风格图像  
d2l.set_figsize()
content_img = d2l.Image.open('rainier.jpg')
d2l.plt.imshow(content_img)

# 新建画布  
d2l.plt.figure()
style_img = d2l.Image.open('autumn-oak.jpg')
d2l.plt.imshow(style_img)
```

- ![[00 Attachments/Figure_1 1.png|300]]
- ![[00 Attachments/Figure_2 2.png|300]]

### 预处理和后处理

- 预处理函数`preprocess`对输入图像在RGB三个通道分别做标准化，并将结果==变换成卷积神经网络接受的输入格式==
- 后处理函数`postprocess`则将输出图像中的像素值还原回标准化之前的值
- 由于图像打印函数要求每个像素的浮点数值在0～1之间，我们对小于0和大于1的值分别取0和1

```python
# 预设的RGB平均值和标准差，用于图像的标准化  
rgb_mean = torch.tensor([0.485, 0.456, 0.406])
rgb_std = torch.tensor([0.229, 0.224, 0.225])


def preprocess(img, image_shape):
    """将图片转化为适合模型训练的tensor"""
    # 定义图像预处理流程：调整大小、转换为tensor、标准化  
    transforms = torchvision.transforms.Compose([
        torchvision.transforms.Resize(image_shape),  # 调整图像大小到指定的image_shape  
        torchvision.transforms.ToTensor(),  # 将图像转换为tensor  
        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])  # 使用预设的均值和标准差对图像进行标准化  
    # 对输入的img应用上述的预处理流程，并在最前面添加一个新维度（用于batch size），然后返回结果  
    return transforms(img).unsqueeze(0)


def postprocess(img):
    """将模型输出的tensor转换为图片"""
    img = img[0].to(rgb_std.device)
    # 反标准化处理：对img的每个像素乘以标准差并加上均值，并确保结果在[0,1]范围内  
    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)
    # 将处理后的tensor转换为PIL图像，并返回  
    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))
```

### 抽取图像特征

- 使用基于 ImageNet 数据集预训练的 VGG-19 来抽取图像特征

```python
# 加载预训练的VGG19模型  
pretrained_net = torchvision.models.vgg19(pretrained=True)
```

- 为了抽取图像的内容特征和风格特征，我们可以选择VGG网络中某些层的输出
    - 一般来说，越靠近输入层，越容易抽取图像的细节信息；反之，则越容易抽取图像的全局信息
    - 为了避免合成图像过多保留内容图像的细节，我们选择VGG较靠近输出的层来输出图像的内容特征
    - 还从VGG中选择不同层的输出来匹配局部和全局的风格，这些图层也称为风格层

```python
# style_layers用于提取样式特征，content_layers用于提取内容特征  
# 这里选择的层数代表了不同级别的特征，越小的层数越接近输入，提取的特征越接近图像的局部信息  
style_layers, content_layers = [0, 5, 10, 19, 28], [25]
```

- 使用VGG层抽取特征时，只需要用到从输入层到最靠近输出层的内容层或风格层之间的所有层
- 下面构建一个新的网络`net`，它只保留需要用到的VGG的所有层

```python
net = nn.Sequential(*[pretrained_net.features[i]
                      for i in range(max(content_layers + style_layers) + 1)])
```

- 对输入的图像进行逐层计算，并保留内容层和风格层的输出

```python
def extract_features(X, content_layers, style_layers):
    """从指定层提取特征"""
    contents = []
    styles = []
    # 对于网络中的每一层  
    for i in range(len(net)):
        X = net[i](X)  # 在该层上运行输入X以提取特征  
        if i in style_layers:  # 如果这是一个样式层，将提取的特征添加到样式列表中  
            styles.append(X)
        if i in content_layers:  # 如果这是一个内容层，将提取的特征添加到内容列表中  
            contents.append(X)
    return contents, styles
```

- `get_contents`函数对内容图像抽取内容特征
- `get_styles`函数对风格图像抽取风格特征
- 因为在训练时无须改变预训练的 VGG 的模型参数，所以可以==在训练开始之前就提取出内容特征和风格特征==
- 由于合成图像是风格迁移所需迭代的模型参数，我们只能在训练过程中通过调用`extract_features`函数来抽取合成图像的内容特征和风格特征

```python
def get_contents(image_shape, device):
    """处理内容图像并提取内容特征"""
    # 预处理内容图像并移动到指定设备上  
    content_X = preprocess(content_img, image_shape).to(device)
    # 从内容图像中提取内容特征  
    content_Y, _ = extract_features(content_X, content_layers, style_layers)
    # 返回预处理后的内容图像（用于后续的图像合成）和从内容图像中提取的内容特征  
    return content_X, content_Y


def get_styles(image_shape, device):
    """处理样式图像并提取样式特征"""
    # 预处理样式图像并移动到指定设备上  
    style_X = preprocess(style_img, image_shape).to(device)
    # 从样式图像中提取样式特征  
    _, styles_Y = extract_features(style_X, content_layers, style_layers)
    # 返回预处理后的样式图像（用于后续的图像合成）和从样式图像中提取的样式特征  
    return style_X, styles_Y
```

### 定义损失函数

- 样式迁移的损失函数由三部分组成：内容损失、风格损失、全变分损失

#### 内容损失

- 使用平方误差衡量合成图像与内容图像在内容特征上的差异

```python
def content_loss(Y_hat, Y):
    """内容损失函数"""
    # 计算预测的内容特征（Y_hat）与实际的内容特征（Y）之间的均方误差  
    return torch.square(Y_hat - Y.detach()).mean()
```

#### 风格损失

- 也使用平方误差衡量合成图像与风格图像在风格特征上的差异
- 使用==格拉姆矩阵来表达风格层输出的风格==
    - 矩阵中的每一层代表了对应通道的风格特征（将该通道的特征图转化为一维线性向量）

```python
def gram(X):
    """Gram矩阵函数，计算输入矩阵（X）的Gram矩阵，用于表示样式特征"""
    # 计算通道数和特征数  
    num_channels, n = X.shape[1], X.numel() // X.shape[1]
    # 将输入矩阵reshape为(通道数, 特征数)的格式  
    X = X.reshape((num_channels, n))
    # 计算Gram矩阵，并进行规范化处理  
    return torch.matmul(X, X.T) / (num_channels * n)


# 样式损失函数  
def style_loss(Y_hat, gram_Y):
    """样式损失函数"""
    # 计算预测的样式特征（Y_hat）的Gram矩阵与实际的样式特征（gram_Y）的Gram矩阵之间的均方误差  
    return torch.square(gram(Y_hat) - gram_Y.detach()).mean()
```

#### 全变分损失

- 有时候，我们学到的合成图像里面有大量高频噪点，即有特别亮或者特别暗的颗粒像素
- 一种常见的去噪方法是全变分去噪（total variation denoising）：假设 $𝑥_{𝑖,𝑗}$ 表示坐标 $(𝑖,𝑗)$
  处的像素值，降低全变分损失 $$\sum_{i, j} \left|x_{i, j} - x_{i+1, j}\right| + \left|x_{i, j} - x_{i, j+1}\right|$$
  能够==尽可能使邻近的像素值相似==

```python
def tv_loss(Y_hat):
    """总变差损失函数，用于提高生成图像的空间连续性，以减少生成图像的高频噪声"""
    # 计算图像中相邻像素之间的差值的绝对值的平均值  
    # 分别获取张量在高度方向上错开的两个子张量，然后计算它们的差值  
    return 0.5 * (torch.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +
                  torch.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())
```

#### 总的损失函数

- 风格转移的损失函数是内容损失、风格损失和总变化损失的加权和
    - 通过调节这些权重超参数，我们可以权衡合成图像在保留内容、迁移风格以及去噪三方面的相对重要性

```python
# 定义内容损失、样式损失和总变差损失的权重  
content_weight, style_weight, tv_weight = 1, 1e3, 10


def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):
    """总的损失函数"""
    # 对于每一层的内容特征，计算预测的内容特征（contents_Y_hat）与实际的内容特征（contents_Y）之间的内容损失，  
    contents_l = [
        content_loss(Y_hat, Y) * content_weight
        for Y_hat, Y in zip(contents_Y_hat, contents_Y)]
    # 对于每一层的样式特征，计算预测的样式特征（styles_Y_hat）的Gram矩阵与实际的样式特征（styles_Y_gram）的Gram矩阵之间的样式损失，  
    styles_l = [
        style_loss(Y_hat, Y) * style_weight
        for Y_hat, Y in zip(styles_Y_hat, styles_Y_gram)]
    # 计算总变差损失  
    tv_l = tv_loss(X) * tv_weight
    # 计算总损失，它是所有层的内容损失、样式损失和总变差损失的加权和  
    l = sum(10 * styles_l + contents_l + [tv_l])
    return contents_l, styles_l, tv_l, l
```

### 初始化合成图像

- 在风格迁移中，合成的图像是训练期间唯一需要更新的变量
    - 因此，我们可以定义一个简单的模型`SynthesizedImage`，并将合成的图像视为模型参数
    - 模型的前向传播只需返回模型参数即可

```python
class SynthesizedImage(nn.Module):
    def __init__(self, img_shape, **kwargs):
        """调用父类nn.Module的构造函数"""
        super(SynthesizedImage, self).__init__(**kwargs)  # 调用父类构造函数  
        # 初始化图像的参数，图像的形状为img_shape，参数的初始值是随机生成的  
        self.weight = nn.Parameter(torch.rand(*img_shape))

    def forward(self):
        # 直接返回图像的权重  
        return self.weight
```

- `get_inits`函数创建了合成图像的模型实例，并将其初始化为图像`X`（内容图像）
    - 风格图像在各个风格层的格拉姆矩阵`styles_Y_gram`将在训练前预先计算好

```python
def get_inits(X, device, lr, styles_Y):
    """初始化合成图像、样式特征的Gram矩阵和优化器"""
    gen_img = SynthesizedImage(X.shape).to(device)
    gen_img.weight.data.copy_(X.data)  # 直接将内容图像的数据复制到gen_img的权重中  
    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)
    styles_Y_gram = [gram(Y) for Y in styles_Y]  # 计算样式特征的Gram矩阵  
    return gen_img(), styles_Y_gram, trainer
```

### 训练模型

- 训练时，不断抽取合成图像的内容和风格特征，然后计算损失函数

```python
def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):
    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)  # X 为合成图像  
    # 定义学习率调度器，每隔lr_decay_epoch个epoch，学习率乘以0.8  
    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, 0.8)
    # 创建一个动画展示器，用于展示每个epoch的内容损失、样式损失和TV损失  
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[10, num_epochs],
                            legend=['content', 'style', 'TV'], ncols=2, figsize=(7, 2.5))
    for epoch in range(num_epochs):
        trainer.zero_grad()  # 清零优化器的梯度  
        contents_Y_hat, styles_Y_hat = extract_features(X, content_layers, style_layers)
        contents_l, styles_l, tv_l, l = compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)
        l.backward()  # 计算梯度  
        trainer.step()  # 使用梯度更新模型参数  
        scheduler.step()  # 更新学习率  
        if (epoch + 1) % 10 == 0:
            # 在animator的第二个子图上显示当前生成的图像  
            # postprocess函数将生成的图像从张量转换回PIL图像  
            animator.axes[1].imshow(postprocess(X))
            animator.add(epoch + 1,
                         [float(sum(contents_l)),
                          float(sum(styles_l)),
                          float(tv_l)])
    return X


device, image_shape = d2l.try_gpu(), (300, 450)
net = net.to(device)
content_X, contents_Y = get_contents(image_shape, device)  # # 对内容图像进行预处理，并提取其内容特征  
_, styles_Y = get_styles(image_shape, device)  # 对样式图像进行预处理，并提取其样式特征  
output = train(content_X, contents_Y, styles_Y, device, 0.3, 500, 50)
```

- ![[00 Attachments/Figure_3 2.png|400]]
