```toc
```

## 门控循环单元（gated recurrent unit，GRU）

- LSTM 的简化变体

### RNN 的不足

- RNN 处理不了太长的序列，导致遗忘重要的早期信息
    - 因为训练时，将所有信息都存放于隐藏层中（隐变量表示）
    - 随着训练的进行（不断向前走），==积累了过多的信息，那么对于早期的信息（可能是很重要的信息）可能就不重要了（变得难以抽取，遗忘）==
- 在 RNN 中，==似乎每个观察值都是同等重要的==（都是做相同的处理），但是实际并不是
    - 例如图上，第一只猫以及老鼠是重要的（让人眼前一亮），但是之后重复出现的猫似乎不怎么重要
    - 对应到文本中就是一些关键字、关键词比较重要
    - 比如股票中重要的几个点
    - 在电影中，连续帧的画面似乎是不重要的（由于连续，画面都是相似的），但是突然的场景切换就显得重要了
- RNN 的内部状态一直是不变的
    - 但是书的章节切换，影片场景切换会导致状态的改变
    - 如果不改变（更新，重置）状态，那么之前的信息（上一章内容或场景）会影响现在的章节或场景
- 所以 GRU 通过一些额外的控制单元（门），==关注重要的观察值，遗忘不重要的观察值==
    - 之后的注意力机制会进一步刻画这一过程（重要的更重要，不重要的更不重要？）

### 门控隐状态（隐状态的门控）

- 模型有专门的机制来确定应该==何时更新隐状态， 以及应该何时重置隐状态==
    - 例如，如果第一个词元非常重要，模型将学会在第一次观测之后更新隐状态
    - 同样，模型也可以学会跳过不相关的临时观测
    - 最后，模型还将学会在需要的时候重置隐状态

#### 重置门和更新门

- 这些门控装置可以更好的捕捉==时间序列数据中的长期依赖关系==![[00 Attachments/Pasted image 20240729214204.png|400]]
    - 更新门：允许控制新状态中有多少个是旧状态的副本
        - 将重要信息更新到隐藏状态
        - ==有助于捕获序列中的长期依赖==（？？？？）
    - 重置（遗忘）门：允许控制“可能还想记住"的过去状态的数量
        - 遗忘（输入或者隐状态中）一些不重要的信息
        - ==有助于捕获序列中的短期依赖==（？？？？）
- 门的计算公式![[00 Attachments/Pasted image 20240730102714.png|400]]
    - 类似以 sigmoid 函数作为激活函数的全连接层，输出取值范围 \[0, 1]

#### 候选隐状态

- 用于生成真正的隐状态
- ==重置门==生成候选隐状态（H 的 权重）![[00 Attachments/Pasted image 20240730103142.png|400]]
    - 点圈为按元素乘法
    - 如果没有 $R_t$ 就是 RNN 计算隐状态的公式
    - $R_t$ 为 sigmoid 后的值（\[0, 1]），当 $R_t$ 接近 0 时，相乘就相当一遗忘一些信息（减少以往状态的影响）
        - 当 $R_t$ 全为 0，相当于将隐状态重置为初始状态（之前的信息全部抛弃）
        - 当 $R_t$ 全为 1，相当于将前面的全部信息做当前的更新（RNN 的隐藏层计算）
    - $R_t$ 表示了之前的信息对当前时刻的影响程度

#### 隐状态

- ==更新门==生成隐状态，确定新的隐状态在多大程度上来自旧状态 $H_{t-1}$ 和候选状态 $\tilde{\mathbf{H}}_t$
  （二者的加权和）![[00 Attachments/Pasted image 20240730104516.png|400]]
    - 点圈为按元素乘法
    - 当 $Z_t$ 接近 1 时，模型就会倾向于保留旧状态，此时来自 $X_t$ 的信息基本被忽略
    - 当 $Z_t$ 接近 0 时，新的隐状态就会接近候选隐状态
    - 这些设计可以帮助处理循环神经网络中的梯度消失问题（？？？）并更好地==捕获时间步距离很长的序列的依赖关系==
        - 例如，如果整个子序列的所有时间步的更新门都==接近于1，则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束==

#### 总结

- 使用门控评估各个状态的重要性
- ![[00 Attachments/Pasted image 20240730112147.png|400]]
    - 重置（遗忘）门：==要用到多少过去的隐藏状态的信息==
    - 更新门：==要用到多少关于 $X_t$（当前时刻）的信息==
    - 极端情况：
        - Z 全 0，R 全 1：即为之前的 RNN
        - Z 全 1：直接忽略当前 $X_t$
        - R 全 0：新（候选）状态只跟 $X_t$ 有关，不考虑之前的信息（只看当下）
        - R = 0，Z = 1 时不矛盾吗（？？？？？）

### 从零开始实现

#### 初始化模型参数

- 从标准差为 0.01 的高斯分布中提取权重，并将偏置项设为0，超参数`num_hiddens`定义隐藏单元的数量
- 实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)


# 初始化模型参数
def get_params(vocab_size, num_hiddens, device):
    # 设置输入和输出的维度为词汇表大小
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        """
        生成服从正态分布的随机张量，并乘以0.01进行缩放
        """

    return torch.randn(size=shape, device=device) * 0.01

    def three():
        """
        三组权重和偏置张量，用于不同的门控机制
        """

    return (normal((num_inputs, num_hiddens)),
            normal((num_hiddens, num_hiddens)),
            torch.zeros(num_hiddens, device=device))

    # 初始化GRU中的权重和偏置
    # 更新门
    W_xz, W_hz, b_z = three()  # GRU多了这两行
    # 重置门
    W_xr, W_hr, b_r = three()  # GRU多了这两行
    # 候选隐藏状态
    W_xh, W_hh, b_h = three()
    # 隐藏状态到输出的权重
    W_hq = normal((num_hiddens, num_outputs))
    # 输出的偏置
    b_q = torch.zeros(num_outputs, device=device)

    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]

    for param in params:
        param.requires_grad_(True)  # 进行梯度计算
    return params
```

#### 定义模型

- 隐状态初始化函数

```python
def init_gru_state(batch_size, num_hiddens, device):
    """返回一个形状为（批量大小，隐藏单元个数）的张量，张量的值全部为零"""


return (torch.zeros((batch_size, num_hiddens), device=device),)
```

- 门控循环单元模型
    - 模型的架构与基本的循环神经网络单元是相同的， 只是权重更新公式更为复杂

```python
def gru(inputs, state, params):
    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []  # 存储每个时间步的输出
    for X in inputs:
        # 更新门 Z
        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)
        # 重置门 R
        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)
        # 候选隐藏状态 H_tilda
        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)
        # 隐藏状态 H
        H = Z * H + (1 - Z) * H_tilda
        # 输出 Y
        Y = H @ W_hq + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)  # 将所有输出拼接在一起，并返回拼接后的结果和最终的隐藏状态
```

#### 训练与预测

```python
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()
num_epochs, lr = 500, 1
model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params,
                            init_gru_state, gru)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)

```

-

### 简洁实现

```python
num_inputs = vocab_size
gru_layer = nn.GRU(num_inputs, num_hiddens)
model = d2l.RNNModel(gru_layer, len(vocab))
model = model.to(device)
# 调用 D2L 库中的 train_ch8 函数进行训练，传入模型实例、训练数据迭代器、词汇表、学习率和训练的总轮数
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)

```

-

## 长短期记忆网络（long short-term memory，LSTM）

- [【LSTM长短期记忆网络】3D模型一目了然，带你领略算法背后的逻辑_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Z34y1k7mc/?spm_id_from=333.788&vd_source=6fde3ed6da8858e6f7b2f1cc620c6173)

### 门控记忆元

- ==记忆元==（单元）
    - 长短期记忆网络引入了记忆元
    - 记忆元是隐状态的一种特殊类型， 它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息（写日记）
- 为了控制记忆元，设计了三个门![[00 Attachments/Pasted image 20240730201409.png|400]]
    - LSTM通过引入三个门（输入门、遗忘门和输出门）来控制信息的流动和保留，从而解决了传统RNN中梯度消失或梯度爆炸的问题
    - LSTM中的三个门分别控制输入信息、遗忘信息和输出信息的流动，从而使得LSTM能够更好地处理长序列数据，并且能够保留长期的状态信息
        - 输出门：从单元中输出条目
        - 输入门：决定何时将数据读入单元
        - 遗忘门：重置单元内容

#### 输入门、遗忘门、输出门

- 采用 sigmoid 激活函数![[00 Attachments/Pasted image 20240730203218.png|400]]

#### 候选记忆元

- 类似 RNN 中的隐状态![[00 Attachments/Pasted image 20240730203906.png|400]]
    - 使用 tanh 作为激活函数，范围 \[-1, 1]

#### 记忆元

- 通过遗忘门、输入门更新记忆元![[00 Attachments/Pasted image 20240730204213.png|400]]
    - 可以看到模型中有两个状态 H, C
    - 当 F 为 0，即将 $C_{t-1}$ 遗忘
    - 当 I 为 1，即使用候选记忆元的信息；0 时，丢弃现在的记忆元（即 $X_t$ 的信息）
    - 相比于 GRU
      的隐状态更新： $\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1} + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t$
      ，LSTM ==将 $Z，1 - Z$ 相互独立出来==
        - GRU 中一方依赖得更多，则另一方就更少；LSTM 就比较随意

#### 隐状态

- 通过输出门更新隐状态![[00 Attachments/Pasted image 20240730210008.png|400]]
    - 由于 C 的取值范围为 \[-2, 2]，为保持 H 的取值范围为 \[-1, 1]，对 C 进行 tanh 处理
    - O 为 0 表示当前的信息以及之前的信息都不要，将 H 重置

#### 总结

- ![[00 Attachments/Pasted image 20240730210542.png|400]]
    - 相较于 GRU 多了 C

### 代码的从零开始实现

#### 初始化模型参数

- 按照标准差0.01的高斯分布初始化权重，并将偏置项设为0

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)


def get_lstm_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        """生成具有特定形状的正态分布随机数，并将其初始化为较小的值"""

    return torch.randn(size=shape, device=device) * 0.01

    def three():
        """生成三个参数：输入到隐藏状态的权重矩阵、隐藏状态到隐藏状态的权重矩阵和隐藏状态的偏置项"""

    return (normal((num_inputs, num_hiddens)),
            normal((num_hiddens, num_hiddens)),
            torch.zeros(num_hiddens, device=device))

    W_xi, W_hi, b_i = three()  # 输入门参数
    W_xf, W_hf, b_f = three()  # 遗忘门参数
    W_xo, W_ho, b_o = three()  # 输出门参数
    W_xc, W_hc, b_c = three()  # 候选记忆单元
    # 输出层参数
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)

    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q]

    for param in params:
        param.requires_grad_(True)  # 梯度计算
    return params
```

#### 定义模型

- 长短期记忆网络的隐状态需要返回一个额外的记忆元
    - 各个单元的值为0，形状为（批量大小，隐藏单元数）

```python
def init_lstm_state(batch_size, num_hiddens, device):
    """初始隐状态，初始记忆元"""
    return (torch.zeros((batch_size, num_hiddens), device=device),
            torch.zeros((batch_size, num_hiddens), device=device))
```

- 提供三个门和一个额外的记忆元
    - 只有隐状态才会传递到输出层， 而记忆元 $C_t$ 不直接参与输出计算

```python
def lstm(inputs, state, params):
    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params
    (H, C) = state  # 隐状态，记忆元
    outputs = []  # 存储每个时间步的输出
    for X in inputs:
        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)  # 输入门
        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)  # 遗忘门
        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)  # 输出门
        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)  # 候选记忆元
        C = F * C + I * C_tilda  # 更新记忆元
        H = O * torch.tanh(C)  # 更新隐藏状态
        Y = (H @ W_hq) + b_q  # 输出计算
        outputs.append(Y)

    return torch.cat(outputs, dim=0), (H, C)  # 输出结果和更新后的隐藏状态和记忆元
```

#### 训练

```python
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()
num_epochs, lr = 500, 1
# 传入词汇表大小、隐藏单元数量、设备、参数初始化函数、初始状态函数和 LSTM 模型函数
model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_lstm_params, init_lstm_state, lstm)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)

```

-

### 代码的简洁实现

```python
num_inputs = vocab_size
lstm_layer = nn.LSTM(num_inputs, num_hiddens)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)

```

-

## 深层循环神经网络

- 之前的都是单层的
- 序列变长不是深度，层数变多才是深度

### 函数依赖关系

- ![[00 Attachments/Pasted image 20240730213941.png|400]]
- 多加几个隐藏层，以获得更多的非线性![[00 Attachments/Pasted image 20240730214132.png|400]]
- ![[00 Attachments/Pasted image 20240730214511.png|400]]

### 代码的简洁实现

- 以 LSMC 为例，唯一的区别是指定了层数

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

# 设置隐藏层数为2
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
# 将输入的维度设置为词汇表的大小
num_inputs = vocab_size
device = d2l.try_gpu()
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)

# 训练
num_epochs, lr = 500, 2
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)

```

-

## 双向循环神经网络（bidirectional RNNs）

- 双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出
- 在序列学习中，我们以往假设的目标是： ==在给定观测的情况下 （例如，在时间序列的上下文中或在语言模型的上下文中），
  对下一个输出进行建模==
- ![[00 Attachments/Pasted image 20240801173036.png|400]]
    - 根据过去和未来的上下文，可以填很不一样的词
    - 目前为止 RNN 只看过去，但是在填空时，也需要未来的信息

### 双向模型

- ![[00 Attachments/Pasted image 20240801183417.png|400]]
    - 增加一个==“从最后一个词元开始从后向前运行”的循环神经网络==
    - 双向循环神经网络的隐藏层中有两个隐状态（==前向隐状态和反向隐状态==）
        - 通过添加反向传递信息的隐藏层来更灵活地处理反向传递的信息
        - 以输入 X1 为例，当输入 X1 进入到模型之后，当前的隐藏状态（右箭头，前向隐状态）放入下一个时间步的状态中去
        - X2 更新完隐藏状态之后，将更新后的隐藏状态传递给 X1 的隐藏状态（左箭头，反向隐状态）
        - 将两个隐藏状态（前向隐状态和反向隐状态）合并在一起，就得到了需要送入输出层的隐状态 Ht
          （在具有多个隐藏层的深度双向循环神经网络中，则==前向隐状态和反向隐状态这两个隐状态会作为输入继续传递到下一个双向层==（具有多个隐藏层的深度双向循环神经网络其实就是多个双向隐藏层的叠加））
        - 最后输出层计算得到输出 Ot
- ![[00 Attachments/Pasted image 20240802044850.png|400]]
    - 将输入复制一遍，一份用于做前向隐态，正常的隐藏层会得到一些输出
    - 另一份用于做反向隐态，反向的隐藏层会得到另外一些输出，然后进行反序
    - 将正向输出和颠倒顺序后的反向输出进行合并（concat），就能得到最终的输出了

### 计算代价及其应用

- 双向循环神经网络的一个关键特性是：==使用来自序列两端的信息来估计输出==
    - 也就是说使用来自过去和未来的观测信息来预测当前的观测
    - 因此并不适用于预测下一个词元的场景
        - 因为在预测下一个词元时，==终究无法知道下一个词元的下文是什么==， 所以将不会得到很好的精度
    - 如果使用双向循环神经网络预测下一个词元，尽管==在训练的时候能够利用过去和未来的数据（也就是所预测词元的上下文）来估计现在空缺的词==
        - 但是在测试的时候，只有过去的数据，因此精度将会很差
- 此外，双向循环神经网络的==计算速度非常慢==
    - 主要原因是网络的==前向传播需要在双向层中进行前向和后向递归==
    - 并且网络的==反向传播也以依赖于前向传播的结果==
    - 因此梯度求解将有一个非常长的链
- 双向层在实际中的时用的比较少，仅仅应用于部分场合：
    - 填充缺失的单词
    - 词元注释（如命名实体识别）
    - 作为序列处理流水线中的一个步骤对序列进行编码（如机器翻译）
    - 双向循环神经网络主要的作用是==对句子做特征提取==，比如在做翻译的时候，给定一个句子去翻译下一个句子，那么可以用双向循环神经网络来做已给定句子的特征提取；或者是做改写等能够看到整个句子的应用场景下，做整个句子的特征提取

### 错误应用

- 由于双向循环神经网络使用了过去的和未来的数据，所以不能盲目地将这一语言模型应用于任何预测任务
- 尽管模型训练得到的困惑度是合理的， 但是模型预测未来词元的能力却可能存在严重缺陷（上文提及）

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps, device = 32, 35, d2l.try_gpu()
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
# 创建一个双向LSTM层
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)  # bidirectional是双向参数
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
num_epochs, lr = 500, 1
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)

```

-
- 从结果上可以看出，虽然困惑度下降的很快，但是预测结果很不好

## 机器翻译与数据集

- 机器翻译（machine translation）指的是 将序列从一种语言自动翻译成另一种语言
- 与先前的语料库所用的单一语言的语言模型不同，奇迹翻译的数据集是由==源语言和目标语言的文本序列对==组成的

### 下载和预处理数据集

- 这里使用了英文-法语数据集

```python
import os
import torch
from d2l import torch as d2l

d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',
                           '94646ad1522d915e7b0f9296181140edcf86a4f5')


def read_data_nmt():
    """载入 “英语-法语” 数据集 """
    data_dir = d2l.download_extract('fra-eng')
    with open(os.path.join(data_dir, 'fra.txt'), 'r', encoding='utf-8') as f:
        return f.read()


raw_text = read_data_nmt()
print(raw_text[:75])  # 打印前75个字符  
# Go.   Va !  
# Hi.   Salut !  
# Run!  Cours !  
# Run!  Courez !  
# Who?  Qui ?  
# Wow!  Ça alors !
```

- 数据预处理
    - 空格代替不间断空格（non-breaking space）
    - 使用小写字母替换大写字母，并在单词和标点符号之间插入空格

```python
# 几个预处理步骤  
def preprocess_nmt(text):
    """预处理 “英语-法语” 数据集"""

    # 判断字符是否是特定标点符号并且前一个字符不是空格  
    def no_space(char, prev_char):
        return char in set(',.!?') and prev_char != ' '

        # 替换特殊字符为空格，转换为小写  

    text = text.replace('\u202f', ' ').replace('\xa0', ' ').lower()
    out = [
        ' ' + char if i > 0 and no_space(char, text[i - 1]) else char  # 在特定标点符号前添加空格  
        for i, char in enumerate(text)]  # 遍历文本中的每个字符  
    return ''.join(out)


text = preprocess_nmt(raw_text)
print(text[:80])  # 打印前80个字符  
# go .  va !  
# hi .  salut !  
# run ! cours !  
# run ! courez !  
# who ? qui ?  
# wow ! ça alors !
```

### 词元化

- 在机器翻译中，常用单词级词元化
- `tokenize_nmt`函数对前`num_examples`个文本序列对进行词元
    - 其中每个词元要么是一个词，要么是一个标点符号
    - 此函数返回两个词元列表：`source`和`target`：`source[i]`是源语言（这里是英语）第 i 个文本序列的词元列表，`target[i]`
      是目标语言（这里是法语）第 i 个文本序列的词元列表

```python
# 词元化  
def tokenize_nmt(text, num_examples=None):
    """  
    词元化 “英语-法语” 数据数据集  
    :param text: 预处理后的文本数据  
    :param num_examples: 用于限制数据集大小的示例数  
    :return: 词元化后的英语和法语文本序列  
    """
    source, target = [], []
    for i, line in enumerate(text.split('\n')):
        if num_examples and i > num_examples:
            break
            # 按制表符分割行  
        parts = line.split('\t')
        # 如果行中包含了两个部分  
        if len(parts) == 2:
            source.append(parts[0].split(' '))  # 英语  
            target.append(parts[1].split(' '))  # 法语  
    return source, target


source, target = tokenize_nmt(text)
source[:6], target[:6]
# ([['go', '.'],  
#   ['hi', '.'],  
#   ['run', '!'],  
#   ['run', '!'],  
#   ['who', '?'],  
#   ['wow', '!']],  
#  [['va', '!'],  
#   ['salut', '!'],  
#   ['cours', '!'],  
#   ['courez', '!'],  
#   ['qui', '?'],  
#   ['ça', 'alors', '!']])
```

- 绘制每个文本序列所包含的词元数量的直方图，即句子长度

```python
# 绘制每个文本序列所包含的标记数量的直方图，根据句子长度做的直方图  
d2l.set_figsize()
# 绘制每个文本序列所包含的标记数量的直方图  
_, _, patches = d2l.plt.hist([[len(l)
                               for l in source], [len(l) for l in target]],
                             label=['source', 'target'])  # 添加标签  
for patch in patches[1].patches:
    # 设置矩形的填充样式为斜线  
    patch.set_hatch('/')
# 添加图例，位于右上角  
d2l.plt.legend(loc='upper right')
```

- ![[00 Attachments/Pasted image 20240807201353.png|400]]

### 词汇表

- 创建源语言的词汇表对象
- bos 表示句子开始，eos表示句子结束，min_freq=2表示句子长度小于2个就不要了

```python
# 创建源语言的词汇表对象  
src_vocab = d2l.Vocab(source, min_freq=2,
                      reserved_tokens=['<pad>', '<bos>',
                                       '<eos>'])  # bos 表示句子开始，eos表示句子结束，min_freq=2表示句子长度小于2个就不要了  len(src_vocab)  
# 10012
```

### 加载数据集

- 语言模型中的序列样本都有一个固定的长度，无论这个样本是一个句子的一部分还是跨越了多个句子的一个片断。
- 在机器翻译中，每个样本都是由源和目标组成的文本序列对，其中的每个文本序列可能具有不同的长度
- 为了提高计算效率，仍然可以通过截断（truncation）和填充（padding）方式实现一次只处理一个小批量的文本序列假
    - 设同一个小批量中的每个序列都应该具有相同的长度`num_steps`
    - 如果文本序列的词元数目少于`num_steps`时， 继续在其末尾添加特定的 “\<pad>” 词元，直到其长度达到`num_steps`
    - 将截断文本序列时，只取其前`num_steps`个词元， 并且丢弃剩余的词元
    - 这样，每个文本序列将具有相同的长度， 以便以相同形状的小批量进行加载

```python
def truncate_pad(line, num_steps, padding_token):
    """  
    截断或填充文本序列  
    :param line: 文本序列  
    :param num_steps: 截断或填充后的序列长度  
    :param padding_token: 填充使用的标记  
    :return: 截断或填充后的文本序列  
    """
    if len(line) > num_steps:
        return line[:num_steps]  # 截断  
    return line + [padding_token] * (num_steps - len(line))  # 填充  


truncate_pad(src_vocab[source[0]], 10, src_vocab['<pad>'])  # 截断或填充第一个英语句子  
# [47, 4, 1, 1, 1, 1, 1, 1, 1, 1]
```

- `build_array_nmt`函数，可以将文本序列转换成小批量数据集用于训练
    - 将特定的 “\<eos>” 词元添加到所有序列的末尾，用于表示序列的结束，句子到头了
    - `valid_len` 表示填充后句子实际上有多长，即有多少是有效数据

```python
# 转换成小批量数据集用于训练  
def build_array_nmt(lines, vocab, num_steps):
    """  
    将机器翻译的文本序列转换成小批量  
    :param lines: 文本序列  
    :param vocab: 词汇表对象  
    :param num_steps: 序列长度  
    :return: 转换后的小批量数据集和实际长度  
    """
    # 将文本序列中的词元转换为词汇表中的索引  
    lines = [vocab[l] for l in lines]
    lines = [l + [vocab['<eos>']] for l in lines]  # 每个句子后面加了一个截止符  
    # 构建小批量数据集的张量表示  
    array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])
    # 计算原始句子的实际长度  
    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)
    return array, valid_len
```

### 训练模型

- 定义`load_data_nmt`函数来返回数据迭代器， 以及源语言和目标语言的两种词表

```python
def load_data_nmt(batch_size, num_steps, num_examples=600):
    """  
    返回翻译数据集的迭代器和词汇表  
    :param batch_size: 批量大小  
    :param num_steps: 序列长度  
    :param num_examples: 用于限制数据集大小的示例数  
    :return: 数据集的迭代器和源语言、目标语言的词汇表对象  
    """
    text = preprocess_nmt(read_data_nmt())  # 预处理文本数据  
    source, target = tokenize_nmt(text, num_examples)  # 词元化文本数据  

    src_vocab = d2l.Vocab(source, min_freq=2,
                          reserved_tokens=['<pad>', '<bos>', '<eos>'])  # 创建源语言词汇表对象  
    tgt_vocab = d2l.Vocab(target, min_freq=2,
                          reserved_tokens=['<pad>', '<bos>', '<eos>'])  # 创建目标语言词汇表对象  
    # 将源语言文本序列转换为小批量数据集的张量表示和实际长度  
    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)
    # 将目标语言文本序列转换为小批量数据集的张量表示和实际长度  
    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)

    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)

    data_iter = d2l.load_array(data_arrays, batch_size)  # 构建数据迭代器  

    return data_iter, src_vocab, tgt_vocab
```

- 读出“英语－法语”数据集中的第一个小批量数据

```python
# 读出 “英语-法语” 数据集中第一个小批量数据  
train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8)
for X, X_valid_len, Y, Y_valid_len in train_iter:
    print('X:', X.type(torch.int32))  # 打印源语言序列的张量表示（整数类型）  
    print('valid lengths for X:', X_valid_len)  # 打印源语言序列的有效长度  
    print('Y:', Y.type(torch.int32))  # 打印目标语言序列的张量表示（整数类型）  
    print('valid lengths for Y:', Y_valid_len)  # 打印目标语言序列的有效长度  
    break
# X: tensor([[  8, 159,   4,   3,   1,   1,   1,   1],  
#         [ 40,  27,   5,   3,   1,   1,   1,   1]], dtype=torch.int32)  
# valid lengths for X: tensor([4, 4])  
# Y: tensor([[12,  0,  4,  3,  1,  1,  1,  1],  
#         [52, 13,  0,  5,  3,  1,  1,  1]], dtype=torch.int32)  
# valid lengths for Y: tensor([4, 5])
```

## 编码器-解码器架构

### 编码器-解码器架构

#### CNN 和 RNN 中的编码器-解码器

- 回顾 CNN![[00 Attachments/Pasted image 20240807205949.png|400]]
    - 在 CNN 中，输入一张图片，经过多层的卷积层，最后到输出层判别图片中的物体的类别
        - CNN 中使用卷积层做特征提取，使用 Softmax 回归做预测
        - 从某种意义上来说，==特征提取可以看成是编码， Softmax 回归可以看成是解码==
            - 编码器：将输入编程成中间表达形式（特征）
            - 解码器：将中间表示解码成输出（标号）
- 回顾 RNN![[00 Attachments/Pasted image 20240807210243.png|400]]
    - 对于 RNN 来讲，输入一个句子，然后对其进行向量输出（参考 RNN 的简洁实现，rnn_layer 只包含隐藏的循环层，所以还需要创建一个单独的输出层）
        - 如果将最终 RNN 的输出（最后的输出 Y）当成中间表示的话（当作抽取的特征），这部分也可以当成是编码器
        - 最后通过全连接层得到最终的输出的话，这部分可以看成是解码器
            - 编码器：将文本表示成向量
            - 解码器：将向量表示成输出

#### 编码器-解码器架构

- 将模型抽象成更常见的形式![[00 Attachments/Pasted image 20240808013939.png|400]]
    - 编码器（encoder）处理输入：**接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态**
        - 编码器在拿到输入之后，将其表示成为中间状态或者中间表示（如**隐藏状态、特征图**）
    - 解码器（decoder）生成输出：**解码器将固定形状的编码状态映射到长度可变的序列**
        - 最简单的解码器能够直接将中间状态或者中间表示翻译成输出
        - 解码器也能够结合一些额外的输入得到输出

#### 总结

- 使用编码器-解码器架构的模型，编码器负责表示输入，解码器负责输出
- “编码器－解码器” 架构可以将长度可变的序列作为输入和输出，因此适用于**机器翻译**等序列转换问题
- 编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态
- 解码器将具有固定形状的编码状态映射为长度可变的序列

### 代码实现

## 序列到序列学习（seq2seq）

### seq2seq

#### 架构

- ![[00 Attachments/Pasted image 20240809013134.png|400]]
- 编码器是一个 RNN，==使用长度可变的序列作为输入，将其转换为固定形状的隐状态==
    - 由于机器翻译并不是用来生成句子（预测）（语言模型）的，所以编码器的 RNN 可以使用双向的（将输入的信息正着看一遍，又反着看一遍）
- 然后将最终的隐藏状态传给解码器，==隐藏状态包括了整个源句子（输入序列的信息）==
    - 解码器使用另外一个 RNN，基于输入序列的编码信息和输出序列（已经推理出的词元）来推理下一个词元
    - \<bos> 作为开始，\<eos> 作为停止（直到解码器生成 \<eos> 为止）
    - 这个架构实现了==可变长度到可变长度==的翻译

#### 隐藏状态的传播

- 将编码器的最后一个 state（H）传给解码器![[00 Attachments/Pasted image 20240809020106.png|400]]

#### 训练和推理的不同

- ![[00 Attachments/Pasted image 20240809020405.png|400]]
    - 训练时，解码器的 RNN 输出是每个目标句子的真正输入（而不是上一个时间步的输出做当前时间步的出入）
        - 所以就算是翻译错误了，也还是会有正确的输出
    - 推理时，没有正真正确的翻译，所以当前时间步的输入只能是上个时间步的输出

#### 衡量翻译的好坏

- ![[00 Attachments/Pasted image 20240809021546.png|400]]
    - 原则上说，对于预测序列中的任意 n 元语法（n-grams），BLEU 的评估都是这个 n 元语法是否出现在标签序列中
        - pn 表示 **n-grams 的精度**，它是两个数量的比值：第一个是**预测序列与标签序列中匹配的 n 元语法的数量**，第二个是*
          *预测序列中 n 元语法的数量的比率**
    - BLUE 由两个部分组成
        - 如果预测很短，实际上很长，那么 e 指数项会很小，作为惩罚（防止预测过短）
        - 后面的连乘项作为精度的判断，n 越大，权重越大
    - BLEU 的值越大越好，最大值为 1，越小的话效果越差

#### 总结

- 在 seq2seq 中，通过设计一个两个循环神经网络的编码器-解码器架构，用于序列到序列的学习
    - 编码器将长度可变的序列转换为固定形状的上下文变量（最后输出的隐状态）
    - 解码器根据生成的词元和上下文变量按词元生成输出（目标）序列词元

### 代码实现

#### 编码器的实现

- 使用的是一个单向循环神经网络来设计编码器， 其中隐状态只依赖于输入子序列， 这个子序列是==由输入序列的开始位置到隐状态所在的时间步的位置==
  （包括隐状态所在的时间步）组成
    - 也可以使用双向循环神经网络构造编码器， 其中隐状态依赖于两个输入子序列， 两个子序列是由隐状态所在的时间步的位置之前的序列和之后的序列
      （包括隐状态所在的时间步）， 因此隐状态对整个序列的信息都进行了编码
- 使用了嵌入层（embedding layer） 来获得输入序列中每个词元（已经用数字索引表示）的特征向量
    - 嵌入层的权重是一个矩阵，其行数等于输入词表的大小（`vocab_size`），其列数等于特征向量的维度（`embed_size`）
    - 对于任意输入词元的索引 i，嵌入层获取权重矩阵的第 i 行（从0开始）以返回其特征向量
    - 这里不使用独热编码是因为太消耗资源，且相似的词在向量空间上有更小的距离
    - Embedding 向量不仅仅是对物体进行简单编号或标识，而是通过特征抽象和编码，在尽量保持物体间相似性的前提下，将物体映射到一个高维特征空间中。
      **Embedding向量能够捕捉到物体之间的相似性和关系**，在映射到高维特征空间后，==相似的物体在空间中会聚集在一起，而不同的物体会被分隔开==

```python

```

- 实例化一个编码器

```python

```

- 使用一个两层门控循环单元编码器，其隐藏单元数为16，给定一小批量的输入序列`X`（批量大小为4，时间步为7）
    - 在完成所有时间步后， 最后一层的隐状态的输出是一个张量（`output`由编码器的循环层返回）， 其形状为（时间步数，批量大小，隐藏单元数）
    - 最后一个时间步的多层隐状态的形状是 （隐藏层的数量，批量大小，隐藏单元的数量）
        - 如果使用长短期记忆网络，`state`中还将包含记忆单元信息

```python

```

#### 解码器

- 使用编码器最后一个时间步的隐状态来初始化解码器的隐状态
    - 这就要求使用循环神经网络实现的编码器和解码器==具有相同数量的层和隐藏单元==
    - 为了进一步包含经过编码的输入序列的信息，上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）
        - 上下文：state 最后时刻最后一层的输出（包含了原句子的上下文信息），repeat 成 decoder 输入的长度（每个时刻）
        - 也就是说 ==decoder 中的每一个输入是：x 和 encoder 最后一个 state 的和==
    - 为了预测输出词元的概率分布，在循环神经网络解码器的最后一层使用全连接层来变换隐状态

```python

```

- 实例化解码器
    - 解码器的输出形状变为（批量大小，时间步数，词表大小）
        - 其中张量的最后一个维度==存储预测的词元分布==

```python

```

#### 损失函数

- 首先屏蔽无关项

```python

```

- 通过扩展 softmax 交叉熵损失函数来遮蔽不相关的预测
    - 最初，所有预测词元的掩码都设置为 1
    - 一旦给定了有效长度，与填充词元对应的掩码（无效部分）将被设置为 0
    - 最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测

```python

```

#### 训练

#### 预测

#### 预测序列的评估

## 束搜索
